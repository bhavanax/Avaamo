id,Title,CreationDate,question,answer,views,votes,AnswerCount,Tags
"57007007","OpenVINO: How to build OpenCV with Inference Engine to enable loading models from Model Optimizer","2019-07-12 12:26:19","<p>I installed OpenVINO and want to run the following code on windows 10. </p>

<pre><code>import numpy as np
import cv2
import sys

from get_face_id import face_id_getter
from check import check
from win10toast import ToastNotifier 

FP = 32

targetId = 0
toaster = None

if '-use_notifications' in sys.argv:
    toaster = ToastNotifier() 

if len(sys.argv) &gt; 1 and '-m' in sys.argv:
    FP =  16
    targetId = cv2.dnn.DNN_TARGET_MYRIAD



cap = cv2.VideoCapture(0)

getter = None

if '-get_face_id' in sys.argv:
    getter = face_id_getter()

weights = 'face-detection-adas-0001/FP{}/face-detection-adas-0001.bin'.format(FP)
config = 'face-detection-adas-0001/FP{}/face-detection-adas-0001.xml'.format(FP)

weights_emotions, config_emotions, emotions = None, None, None



if len(sys.argv) &gt; 1 and '-use_emotions' in sys.argv:
    weights_emotions = 'emotions-recognition-retail-0003/FP{}/emotions-recognition-retail-0003.bin'.format(FP)
    config_emotions = 'emotions-recognition-retail-0003/FP{}/emotions-recognition-retail-0003.xml'.format(FP)
framework = 'DLDT'

model = cv2.dnn.readNet(weights, config, framework)
model.setPreferableTarget(targetId=targetId)

if len(sys.argv) &gt; 1 and '-use_emotions' in sys.argv:
    emotions = cv2.dnn.readNet(weights_emotions, config_emotions, framework)
    emotions.setPreferableTarget(targetId=targetId)

emotions_decode = ('neutral', 'happy', 'sad', 'surprise', 'anger')

names = [""Plotnikov Egor"", ""Vainberg Roman"", ""Sataev Emil"", ""Unknown person""]

emotion_text = None

while(True):
    ret, frame = cap.read()

    blob = cv2.dnn.blobFromImage(frame, size=(672, 384), crop=False)

    have_nots = False

    model.setInput(blob)
    ans = model.forward()
    for i in range(0, 200):
        x_min, y_min, x_max, y_max = np.array(ans[0, 0, i, 3:7]) * np.array([640, 480, 640, 480])
        if ans[0, 0, i, 2] &gt; 0.5:
            cv2.rectangle(frame, (int(x_min), int(y_min)), (int(x_max), int(y_max)), ( 0, 255, 255))

            if len(sys.argv) &gt; 1 and '-use_emotions' in sys.argv:
                blob_emotions = cv2.dnn.blobFromImage(frame[int(y_min):int(y_max), int(x_min):int(x_max)], size=(64, 64), crop=False)
                emotions.setInput(blob_emotions)
                ans_emotions = emotions.forward()[0, : , 0 , 0]
                ans_emotions = list(map(lambda x: 1 if x &gt; 0.5 else 0, ans_emotions))
                _t = ''.join(list(map(str,ans_emotions))).find('1')
                if _t == -1:
                    _t = 0
                emotion_text = emotions_decode[_t]

            if '-get_face_id' in sys.argv:
                _ans = getter.get_answer(frame[int(y_min):int(y_max), int(x_min):int(x_max)])

                t = check('labels.txt', _ans)
                #print(names[t])
                font = cv2.FONT_HERSHEY_SIMPLEX 
                cv2.putText(frame,names[t],(int(x_min), int(y_min)), font, 1,(255,255,255),2,cv2.LINE_AA)
                if emotion_text != None:
                    cv2.putText(frame,emotion_text,(int(x_min), int(y_max)), font, 1,(255,255,255),2,cv2.LINE_AA)

            if '-use_notifications' in sys.argv and not have_nots:
                toaster.show_toast(""Welcome, "" + names[t],"""")
                have_nots = True

    cv2.imshow('frame',frame)

    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break


cap.release()
cv2.destroyAllWindows() 
</code></pre>

<p>I want to run a pre-trained OpenVINO model, but get the error:</p>

<pre><code>v\modules\dnn\src\dnn.cpp:2670: error: (-2:Unspecified error) Build OpenCV with Inference Engine to enable loading models from Model Optimizer. in function 'cv::dnn::dnn4_v20190122::Net::readFromModelOptimizer'
</code></pre>

<p>I need to build OpenCV with Inference Engine. I'm not experienced in programming and don't know what this means.</p>

<p>When I try this:
<a href=""https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend"" rel=""nofollow noreferrer"">https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend</a></p>

<p>and try</p>

<pre><code>cmake \
  -DWITH_INF_ENGINE=ON \
  -DENABLE_CXX11=ON \
  ...
</code></pre>

<p>in the C:\Program Files (x86)\IntelSWTools\openvino_2019.1.148\opencv\samples
it gives an error saying: </p>

<pre><code>CMake Error: The source directory ""C:/Program Files (x86)/IntelSWTools/openvino_2019.1.148/opencv/samples/..."" does not appear to contain CMakeLists.txt.
</code></pre>

<p>even though there is a CMakeLists.txt in that folder.</p>

<p>Can someone please help me?</p>
","<p>You can just use OpenCV from OpenVINO. It's already compiled with Intel's Inference Engine.</p>
","6440","2","3","<python><opencv><cmake><openvino>"
"57007007","OpenVINO: How to build OpenCV with Inference Engine to enable loading models from Model Optimizer","2019-07-12 12:26:19","<p>I installed OpenVINO and want to run the following code on windows 10. </p>

<pre><code>import numpy as np
import cv2
import sys

from get_face_id import face_id_getter
from check import check
from win10toast import ToastNotifier 

FP = 32

targetId = 0
toaster = None

if '-use_notifications' in sys.argv:
    toaster = ToastNotifier() 

if len(sys.argv) &gt; 1 and '-m' in sys.argv:
    FP =  16
    targetId = cv2.dnn.DNN_TARGET_MYRIAD



cap = cv2.VideoCapture(0)

getter = None

if '-get_face_id' in sys.argv:
    getter = face_id_getter()

weights = 'face-detection-adas-0001/FP{}/face-detection-adas-0001.bin'.format(FP)
config = 'face-detection-adas-0001/FP{}/face-detection-adas-0001.xml'.format(FP)

weights_emotions, config_emotions, emotions = None, None, None



if len(sys.argv) &gt; 1 and '-use_emotions' in sys.argv:
    weights_emotions = 'emotions-recognition-retail-0003/FP{}/emotions-recognition-retail-0003.bin'.format(FP)
    config_emotions = 'emotions-recognition-retail-0003/FP{}/emotions-recognition-retail-0003.xml'.format(FP)
framework = 'DLDT'

model = cv2.dnn.readNet(weights, config, framework)
model.setPreferableTarget(targetId=targetId)

if len(sys.argv) &gt; 1 and '-use_emotions' in sys.argv:
    emotions = cv2.dnn.readNet(weights_emotions, config_emotions, framework)
    emotions.setPreferableTarget(targetId=targetId)

emotions_decode = ('neutral', 'happy', 'sad', 'surprise', 'anger')

names = [""Plotnikov Egor"", ""Vainberg Roman"", ""Sataev Emil"", ""Unknown person""]

emotion_text = None

while(True):
    ret, frame = cap.read()

    blob = cv2.dnn.blobFromImage(frame, size=(672, 384), crop=False)

    have_nots = False

    model.setInput(blob)
    ans = model.forward()
    for i in range(0, 200):
        x_min, y_min, x_max, y_max = np.array(ans[0, 0, i, 3:7]) * np.array([640, 480, 640, 480])
        if ans[0, 0, i, 2] &gt; 0.5:
            cv2.rectangle(frame, (int(x_min), int(y_min)), (int(x_max), int(y_max)), ( 0, 255, 255))

            if len(sys.argv) &gt; 1 and '-use_emotions' in sys.argv:
                blob_emotions = cv2.dnn.blobFromImage(frame[int(y_min):int(y_max), int(x_min):int(x_max)], size=(64, 64), crop=False)
                emotions.setInput(blob_emotions)
                ans_emotions = emotions.forward()[0, : , 0 , 0]
                ans_emotions = list(map(lambda x: 1 if x &gt; 0.5 else 0, ans_emotions))
                _t = ''.join(list(map(str,ans_emotions))).find('1')
                if _t == -1:
                    _t = 0
                emotion_text = emotions_decode[_t]

            if '-get_face_id' in sys.argv:
                _ans = getter.get_answer(frame[int(y_min):int(y_max), int(x_min):int(x_max)])

                t = check('labels.txt', _ans)
                #print(names[t])
                font = cv2.FONT_HERSHEY_SIMPLEX 
                cv2.putText(frame,names[t],(int(x_min), int(y_min)), font, 1,(255,255,255),2,cv2.LINE_AA)
                if emotion_text != None:
                    cv2.putText(frame,emotion_text,(int(x_min), int(y_max)), font, 1,(255,255,255),2,cv2.LINE_AA)

            if '-use_notifications' in sys.argv and not have_nots:
                toaster.show_toast(""Welcome, "" + names[t],"""")
                have_nots = True

    cv2.imshow('frame',frame)

    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break


cap.release()
cv2.destroyAllWindows() 
</code></pre>

<p>I want to run a pre-trained OpenVINO model, but get the error:</p>

<pre><code>v\modules\dnn\src\dnn.cpp:2670: error: (-2:Unspecified error) Build OpenCV with Inference Engine to enable loading models from Model Optimizer. in function 'cv::dnn::dnn4_v20190122::Net::readFromModelOptimizer'
</code></pre>

<p>I need to build OpenCV with Inference Engine. I'm not experienced in programming and don't know what this means.</p>

<p>When I try this:
<a href=""https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend"" rel=""nofollow noreferrer"">https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend</a></p>

<p>and try</p>

<pre><code>cmake \
  -DWITH_INF_ENGINE=ON \
  -DENABLE_CXX11=ON \
  ...
</code></pre>

<p>in the C:\Program Files (x86)\IntelSWTools\openvino_2019.1.148\opencv\samples
it gives an error saying: </p>

<pre><code>CMake Error: The source directory ""C:/Program Files (x86)/IntelSWTools/openvino_2019.1.148/opencv/samples/..."" does not appear to contain CMakeLists.txt.
</code></pre>

<p>even though there is a CMakeLists.txt in that folder.</p>

<p>Can someone please help me?</p>
","<p>You should add OpenCV to your environment variables.</p>

<p>Open environment variables window: Type <code>environment variables</code> in search box. </p>

<p>Edit <code>Path</code> from <code>System Variables</code>, insert OpenCV path as your installation path. In my case, I added and able to work with it. </p>

<pre><code>C:\Program Files (x86)\IntelSWTools\openvino\opencv\bin
</code></pre>

<p>You also need to check for preferable backend and target to pick inference backend and target hardware.</p>

<p>see following API references: </p>

<ul>
<li><a href=""https://docs.opencv.org/master/d6/d0f/group__dnn.html"" rel=""nofollow noreferrer"">https://docs.opencv.org/master/d6/d0f/group__dnn.html</a> </li>
<li><a href=""https://docs.opencv.org/3.4/db/d30/classcv_1_1dnn_1_1Net.html#a7f767df11386d39374db49cd8df8f59e"" rel=""nofollow noreferrer"">https://docs.opencv.org/3.4/db/d30/classcv_1_1dnn_1_1Net.html#a7f767df11386d39374db49cd8df8f59e</a> </li>
<li><a href=""https://docs.opencv.org/3.4/db/d30/classcv_1_1dnn_1_1Net.html#a9dddbefbc7f3defbe3eeb5dc3d3483f4"" rel=""nofollow noreferrer"">https://docs.opencv.org/3.4/db/d30/classcv_1_1dnn_1_1Net.html#a9dddbefbc7f3defbe3eeb5dc3d3483f4</a></li>
</ul>
","6440","2","3","<python><opencv><cmake><openvino>"
"57007007","OpenVINO: How to build OpenCV with Inference Engine to enable loading models from Model Optimizer","2019-07-12 12:26:19","<p>I installed OpenVINO and want to run the following code on windows 10. </p>

<pre><code>import numpy as np
import cv2
import sys

from get_face_id import face_id_getter
from check import check
from win10toast import ToastNotifier 

FP = 32

targetId = 0
toaster = None

if '-use_notifications' in sys.argv:
    toaster = ToastNotifier() 

if len(sys.argv) &gt; 1 and '-m' in sys.argv:
    FP =  16
    targetId = cv2.dnn.DNN_TARGET_MYRIAD



cap = cv2.VideoCapture(0)

getter = None

if '-get_face_id' in sys.argv:
    getter = face_id_getter()

weights = 'face-detection-adas-0001/FP{}/face-detection-adas-0001.bin'.format(FP)
config = 'face-detection-adas-0001/FP{}/face-detection-adas-0001.xml'.format(FP)

weights_emotions, config_emotions, emotions = None, None, None



if len(sys.argv) &gt; 1 and '-use_emotions' in sys.argv:
    weights_emotions = 'emotions-recognition-retail-0003/FP{}/emotions-recognition-retail-0003.bin'.format(FP)
    config_emotions = 'emotions-recognition-retail-0003/FP{}/emotions-recognition-retail-0003.xml'.format(FP)
framework = 'DLDT'

model = cv2.dnn.readNet(weights, config, framework)
model.setPreferableTarget(targetId=targetId)

if len(sys.argv) &gt; 1 and '-use_emotions' in sys.argv:
    emotions = cv2.dnn.readNet(weights_emotions, config_emotions, framework)
    emotions.setPreferableTarget(targetId=targetId)

emotions_decode = ('neutral', 'happy', 'sad', 'surprise', 'anger')

names = [""Plotnikov Egor"", ""Vainberg Roman"", ""Sataev Emil"", ""Unknown person""]

emotion_text = None

while(True):
    ret, frame = cap.read()

    blob = cv2.dnn.blobFromImage(frame, size=(672, 384), crop=False)

    have_nots = False

    model.setInput(blob)
    ans = model.forward()
    for i in range(0, 200):
        x_min, y_min, x_max, y_max = np.array(ans[0, 0, i, 3:7]) * np.array([640, 480, 640, 480])
        if ans[0, 0, i, 2] &gt; 0.5:
            cv2.rectangle(frame, (int(x_min), int(y_min)), (int(x_max), int(y_max)), ( 0, 255, 255))

            if len(sys.argv) &gt; 1 and '-use_emotions' in sys.argv:
                blob_emotions = cv2.dnn.blobFromImage(frame[int(y_min):int(y_max), int(x_min):int(x_max)], size=(64, 64), crop=False)
                emotions.setInput(blob_emotions)
                ans_emotions = emotions.forward()[0, : , 0 , 0]
                ans_emotions = list(map(lambda x: 1 if x &gt; 0.5 else 0, ans_emotions))
                _t = ''.join(list(map(str,ans_emotions))).find('1')
                if _t == -1:
                    _t = 0
                emotion_text = emotions_decode[_t]

            if '-get_face_id' in sys.argv:
                _ans = getter.get_answer(frame[int(y_min):int(y_max), int(x_min):int(x_max)])

                t = check('labels.txt', _ans)
                #print(names[t])
                font = cv2.FONT_HERSHEY_SIMPLEX 
                cv2.putText(frame,names[t],(int(x_min), int(y_min)), font, 1,(255,255,255),2,cv2.LINE_AA)
                if emotion_text != None:
                    cv2.putText(frame,emotion_text,(int(x_min), int(y_max)), font, 1,(255,255,255),2,cv2.LINE_AA)

            if '-use_notifications' in sys.argv and not have_nots:
                toaster.show_toast(""Welcome, "" + names[t],"""")
                have_nots = True

    cv2.imshow('frame',frame)

    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break


cap.release()
cv2.destroyAllWindows() 
</code></pre>

<p>I want to run a pre-trained OpenVINO model, but get the error:</p>

<pre><code>v\modules\dnn\src\dnn.cpp:2670: error: (-2:Unspecified error) Build OpenCV with Inference Engine to enable loading models from Model Optimizer. in function 'cv::dnn::dnn4_v20190122::Net::readFromModelOptimizer'
</code></pre>

<p>I need to build OpenCV with Inference Engine. I'm not experienced in programming and don't know what this means.</p>

<p>When I try this:
<a href=""https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend"" rel=""nofollow noreferrer"">https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend</a></p>

<p>and try</p>

<pre><code>cmake \
  -DWITH_INF_ENGINE=ON \
  -DENABLE_CXX11=ON \
  ...
</code></pre>

<p>in the C:\Program Files (x86)\IntelSWTools\openvino_2019.1.148\opencv\samples
it gives an error saying: </p>

<pre><code>CMake Error: The source directory ""C:/Program Files (x86)/IntelSWTools/openvino_2019.1.148/opencv/samples/..."" does not appear to contain CMakeLists.txt.
</code></pre>

<p>even though there is a CMakeLists.txt in that folder.</p>

<p>Can someone please help me?</p>
","<p>For me, the solution was to remove the OpenCV Python libraries and install <em>opencv-python-inference-engine</em></p>

<pre><code>pip3 uninstall opencv-python
pip3 uninstall opencv-contrib-python
pip3 install opencv-python-inference-engine
</code></pre>
","6440","2","3","<python><opencv><cmake><openvino>"
"58264381","python ImportError Openvino by script and by shell","2019-10-07 06:23:26","<p>When I run python script by command <code>sudo python script.py</code> I get error in the line</p>

<pre><code>from openvino.inference_engine import IENetwork, IECore
</code></pre>

<p>The error is</p>

<pre><code>ImportError: No module named openvino.inference_engine
</code></pre>

<p>But When I open the python shell and run </p>

<pre><code>from openvino.inference_engine import IENetwork, IECore
</code></pre>

<p>I don't get this error.</p>

<p>What is the reason for the difference and how to fix this error?</p>
","<p>The issue you are facing is because the inference engine path is not found in the path variable. In openvino the path variables such as the path to openvino inference engine are setup for user by running the setupvars.sh shell script in the below path:</p>

<p>intel/openvino_2019.1.144/bin/setupvars.sh</p>

<p>The path variables are set specific to the user and are not present in the path variable for the sudo user. So when you run the python script using ""<code>sudo python script.py</code>"" you get the module not found error as the path variables for openvino are not rightly set for sudo.</p>

<p>If you open the setupvars.sh you can see all path variable are set without sudo like the below example</p>

<pre><code>export PATH=~/intel/openvino_2019.2.242/python/python3.7:$PATH
</code></pre>

<p>**</p>

<h2>Resolution</h2>

<p>**
To resolve your error you can use any of the two below alternatives:</p>

<p>1)You can run ""<code>python script.py</code>"" which could give you your expected result.</p>

<p>2)If you want to get this packages in ""<code>sudo python script.py</code>"" you must add openvino path to the sudo path. This can be done by editing the setupvars.sh file by changing the commands used to set paths as in the below example</p>

<p>eg:</p>

<pre><code>export PATH=~/intel/openvino_2019.2.242/python/python3.7:$PATH
</code></pre>

<p>should be replaced with</p>

<pre><code>sudo PATH=~/intel/openvino_2019.2.242/python/python3.7:$PATH
</code></pre>
","5571","4","1","<python><import><sudo><openvino>"
"56754574","Channels first vs Channels last - what do these mean?","2019-06-25 12:53:54","<p><a href=""https://software.intel.com/en-us/forums/computer-vision/topic/785538"" rel=""nofollow noreferrer"">https://software.intel.com/en-us/forums/computer-vision/topic/785538</a></p>

<p>""The problem has been resolved. It's because the model I use uses channels_first as default for GPU training, while OPENVINO requires channels_last for TF models.""</p>

<p>What do these mean?</p>

<p>How can I change them? </p>

<p>I cannot find any further references to this on the net.</p>
","<p>Channels first means that in a specific tensor (consider a photo), you would have <code>(Number_Of_Channels, Height , Width)</code>.</p>
<p>Channels last means channels are on the last position in a tensor(n-dimensional array).</p>
<p>Examples:</p>
<pre><code>    (3,360,720) --- Channels first

    (360,720,3) --- Channels last
</code></pre>
<p>where 3 comes from RGB (coloured image).</p>
<p>TensorFlow has by default channels last setting in the configuration.</p>
<p>The issue comes from the fact that some obsolete now frameworks(such as <code>Theano</code>) had a channels-first approach; porting was a problem particularly for newbies.</p>
<p>The solution to your problem would be to re-train your model in &quot;Channels_Last&quot; format.</p>
","5242","2","3","<tensorflow><neural-network><deep-learning><conv-neural-network><openvino>"
"56754574","Channels first vs Channels last - what do these mean?","2019-06-25 12:53:54","<p><a href=""https://software.intel.com/en-us/forums/computer-vision/topic/785538"" rel=""nofollow noreferrer"">https://software.intel.com/en-us/forums/computer-vision/topic/785538</a></p>

<p>""The problem has been resolved. It's because the model I use uses channels_first as default for GPU training, while OPENVINO requires channels_last for TF models.""</p>

<p>What do these mean?</p>

<p>How can I change them? </p>

<p>I cannot find any further references to this on the net.</p>
","<p>You can convert TF model with NCHW layout to IR by using --disable_nhwc_to_nchw with Model Optimizer.  </p>
","5242","2","3","<tensorflow><neural-network><deep-learning><conv-neural-network><openvino>"
"56754574","Channels first vs Channels last - what do these mean?","2019-06-25 12:53:54","<p><a href=""https://software.intel.com/en-us/forums/computer-vision/topic/785538"" rel=""nofollow noreferrer"">https://software.intel.com/en-us/forums/computer-vision/topic/785538</a></p>

<p>""The problem has been resolved. It's because the model I use uses channels_first as default for GPU training, while OPENVINO requires channels_last for TF models.""</p>

<p>What do these mean?</p>

<p>How can I change them? </p>

<p>I cannot find any further references to this on the net.</p>
","<p>NCHW - channel first<br>
NHWC - channel last</p>

<p>N:batch_size, C:no.of.channels, H:input_img_height, W:input_img_width  </p>

<p>by default MKLDNN-plugin uses NCHW data layout. </p>
","5242","2","3","<tensorflow><neural-network><deep-learning><conv-neural-network><openvino>"
"57423838","Whats the right way of using openCV with openVINO?","2019-08-09 05:18:48","<p>Dislcaimer: I have never used openCV or openVINO or for the fact anything even close to ML before. However I've been slamming my head studying neural-networks(reading material online) because I've to work with intel's openVINO on an edge device.
Here's what the official documentation says about using openCV with openVINO(using openVINO's inference engine with openCV). </p>

<p>->Optimize the pretrained model with openVINO's model optimizer(creating the IR file pair)
use these IR files with</p>

<pre><code> openCV's dnn.readnet() //this is where the inference engine gets set? 
</code></pre>

<p><a href=""https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_raspbian.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_raspbian.html</a></p>

<p>Tried digging more and found a third party reference. Here a difference approach is taken. </p>

<p>->Intermediatte files (bin/xml are not created. Instead caffe model file is used)</p>

<p>->the inference engine is  defined explicitly with the following line </p>

<pre><code>net.setPreferableBackend(cv2.dnn.DNN_BACKEND_INFERENCE_ENGINE)
</code></pre>

<p><a href=""https://www.learnopencv.com/using-openvino-with-opencv/"" rel=""nofollow noreferrer"">https://www.learnopencv.com/using-openvino-with-opencv/</a></p>

<p>Now I know to utilize openCV we have to use it's inference engine with pretrained models. I want to know which of the two approach is the correct(or preferred) one, and if rather I'm missing out no something. </p>
","<p>You can get started using OpenVino from: <a href=""https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_windows.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_windows.html</a></p>

<p>You would require a set of pre-requsites to run your sample. OpenCV is your Computer Vision package which can used for Image processing.</p>

<p>Openvino inference requires you to convert any of your trained models(.caffemodel,.pb,etc.) to Intermediate representations(.xml,.bin) files.</p>

<p>For a better understanding and sample demos on OpenVino, watch the videos/subscribe to the OpenVino Youtube channel: <a href=""https://www.youtube.com/channel/UCkN8KINLvP1rMkL4trkNgTg"" rel=""nofollow noreferrer"">https://www.youtube.com/channel/UCkN8KINLvP1rMkL4trkNgTg</a></p>
","3956","0","3","<opencv><neural-network><conv-neural-network><openvino>"
"57423838","Whats the right way of using openCV with openVINO?","2019-08-09 05:18:48","<p>Dislcaimer: I have never used openCV or openVINO or for the fact anything even close to ML before. However I've been slamming my head studying neural-networks(reading material online) because I've to work with intel's openVINO on an edge device.
Here's what the official documentation says about using openCV with openVINO(using openVINO's inference engine with openCV). </p>

<p>->Optimize the pretrained model with openVINO's model optimizer(creating the IR file pair)
use these IR files with</p>

<pre><code> openCV's dnn.readnet() //this is where the inference engine gets set? 
</code></pre>

<p><a href=""https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_raspbian.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_raspbian.html</a></p>

<p>Tried digging more and found a third party reference. Here a difference approach is taken. </p>

<p>->Intermediatte files (bin/xml are not created. Instead caffe model file is used)</p>

<p>->the inference engine is  defined explicitly with the following line </p>

<pre><code>net.setPreferableBackend(cv2.dnn.DNN_BACKEND_INFERENCE_ENGINE)
</code></pre>

<p><a href=""https://www.learnopencv.com/using-openvino-with-opencv/"" rel=""nofollow noreferrer"">https://www.learnopencv.com/using-openvino-with-opencv/</a></p>

<p>Now I know to utilize openCV we have to use it's inference engine with pretrained models. I want to know which of the two approach is the correct(or preferred) one, and if rather I'm missing out no something. </p>
","<p>If the topology that you are using is supported by OpenVino,the best way to use is the opencv that comes with openvino. For that you need to </p>

<p>1.Initialize the openvino environment by running the setupvars.bat in your openvino path(C:\Program Files (x86)\IntelSWTools\openvino\bin)</p>

<p>2.Generate the IR file (xml&amp;bin)for your model using model optimizer.</p>

<p>3.Run using inference engine samples in the path /inference_engine_samples_build/</p>

<p>If the topology is not supported, then you can go for the other procedure that you mentioned.</p>
","3956","0","3","<opencv><neural-network><conv-neural-network><openvino>"
"57423838","Whats the right way of using openCV with openVINO?","2019-08-09 05:18:48","<p>Dislcaimer: I have never used openCV or openVINO or for the fact anything even close to ML before. However I've been slamming my head studying neural-networks(reading material online) because I've to work with intel's openVINO on an edge device.
Here's what the official documentation says about using openCV with openVINO(using openVINO's inference engine with openCV). </p>

<p>->Optimize the pretrained model with openVINO's model optimizer(creating the IR file pair)
use these IR files with</p>

<pre><code> openCV's dnn.readnet() //this is where the inference engine gets set? 
</code></pre>

<p><a href=""https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_raspbian.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_raspbian.html</a></p>

<p>Tried digging more and found a third party reference. Here a difference approach is taken. </p>

<p>->Intermediatte files (bin/xml are not created. Instead caffe model file is used)</p>

<p>->the inference engine is  defined explicitly with the following line </p>

<pre><code>net.setPreferableBackend(cv2.dnn.DNN_BACKEND_INFERENCE_ENGINE)
</code></pre>

<p><a href=""https://www.learnopencv.com/using-openvino-with-opencv/"" rel=""nofollow noreferrer"">https://www.learnopencv.com/using-openvino-with-opencv/</a></p>

<p>Now I know to utilize openCV we have to use it's inference engine with pretrained models. I want to know which of the two approach is the correct(or preferred) one, and if rather I'm missing out no something. </p>
","<p>The most common issues I ran into:</p>
<ul>
<li>setupvars.bat must be run within the same terminal, or use os.environ[&quot;varname&quot;] = varvalue</li>
<li>OpenCV needs to be built with support for the inference engines (ie DLDT).  There are pre-built binaries here: <a href=""https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend"" rel=""nofollow noreferrer"">https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend</a></li>
<li>Target inference engine: net.setPreferableBackend(cv2.dnn.DNN_BACKEND_INFERENCE_ENGINE)</li>
<li>Target NCS2: net.setPreferableTarget(cv2.dnn.DNN_TARGET_MYRIAD)</li>
</ul>
<p>The OpenCV pre-built binary located in the OpenVino directory already has IE support and is also an option.</p>
<p>Note that the Neural Compute Stick 2 AKA NCS2 (OpenVino IE/VPU/MYRIAD) requires FP16 model formats (float16).  Also try to keep you image in this format to avoid conversion penalties.  You can input images as any of these formats though: FP32, FP16, U8</p>
<p>I found this guide helpful: <a href=""https://learnopencv.com/using-openvino-with-opencv/"" rel=""nofollow noreferrer"">https://learnopencv.com/using-openvino-with-opencv/</a></p>
<p>Here's an example targetting the NCS2 from <a href=""https://medium.com/sclable/intel-openvino-with-opencv-f5ad03363a38"" rel=""nofollow noreferrer"">https://medium.com/sclable/intel-openvino-with-opencv-f5ad03363a38</a>:</p>
<pre><code># Load the model.
net = cv2.dnn.readNet(ARCH_FPATH, MODEL_FPATH)

# Specify target device.
net.setPreferableBackend(cv2.dnn.DNN_BACKEND_INFERENCE_ENGINE)
net.setPreferableTarget(cv2.dnn.DNN_TARGET_MYRIAD) # NCS 2

# Read an image.
print(&quot;Processing input image...&quot;)
img = cv2.imread(IMG_FPATH)
if img is None:
    raise Exception(f'Image not found here: {IMG_FPATH}')

# Prepare input blob and perform inference
blob = cv2.dnn.blobFromImage(img, size=(672, 384), ddepth=cv2.CV_8U)
net.setInput(blob)
out = net.forward()

# Draw detected faces
for detect in out.reshape(-1, 7):
    conf = float(detect[2])
    xmin = int(detect[3] * frame.shape[1])
    ymin = int(detect[4] * frame.shape[0])
    xmax = int(detect[5] * frame.shape[1])
    ymax = int(detect[6] * frame.shape[0])
    
    if conf &gt; CONF_THRESH:
        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color=(0, 255, 0))
</code></pre>
<p>There are more samples here (jupyter notebook/python): <a href=""https://github.com/sclable/openvino_opencv"" rel=""nofollow noreferrer"">https://github.com/sclable/openvino_opencv</a></p>
","3956","0","3","<opencv><neural-network><conv-neural-network><openvino>"
"54772173","Using Openvino's OpenCV build on Anaconda environment","2019-02-19 17:50:25","<p>I have recently installed the latest OpenVINO release (2018 R5 0.1) for Windows 10 which, if I understood correctly, comes with a fully built OpenCV. Many tutorials show the use of that OpenCV but I failed to make it work on my Anaconda environment (with Python 3.6).</p>
<hr />
<p>Running the environement setup <code>C:\Intel\cvsdk\bin\setupvars.bat</code> script I get the following output:</p>
<blockquote>
<p>Commande ECHO désactivée.</p>
<p>PYTHONPATH=C:\Intel\computer_vision_sdk_2018.5.456\python\python3.6;C:\Program Files\Python36;</p>
<p>[setupvars.bat] OpenVINO environment initialized</p>
</blockquote>
<p>In my conda env, if I have no opencv package installed, I get the error:</p>
<p><code>ModuleNotFoundError: No module named cv2</code></p>
<p>And if I install one with <code>conda install py-opencv</code> (or <code>opencv</code>), I get this:</p>
<p><code>cv2.error: OpenCV(3.4.2) [...] Build OpenCV with Inference Engine to enable loading models from Model Optimizer</code></p>
<p>Installing with pip (<code>pip install opencv-python</code>) while on the anaconda environment also doesn't work:</p>
<p><code>cv2.error: OpenCV(4.0.0) [...] Build OpenCV with Inference Engine to enable loading models from Model Optimizer</code></p>
<hr />
<p>For clarification, I have successfully built the opencv examples with CMake and can run the executables. Here is the output of one of their sample programs:</p>
<pre><code>(OpenVino) C:\Intel\computer_vision_sdk_2018.5.456\opencv\build\Debug&gt;openvino_sample_opencv_version.exe
</code></pre>
<blockquote>
<p>Welcome to OpenCV 4.0.1-openvino</p>
</blockquote>
<p>Clearly, that OpenCV is usable somehow, I just can't find how to use it in my conda environment from a python script.</p>
","<p>Append OpenVINO python path in the beginning of your python code as shown below:</p>

<pre><code>import sys
sys.path.append(""C:\Intel\computer_vision_sdk_&lt;version_number&gt;\python\python3.6"") 
</code></pre>

<p>For eg:</p>

<pre><code>sys.path.append(""C:\Intel\computer_vision_sdk_2018.5.456\python\python3.6"") 
</code></pre>
","3099","1","4","<python><opencv><anaconda><openvino>"
"54772173","Using Openvino's OpenCV build on Anaconda environment","2019-02-19 17:50:25","<p>I have recently installed the latest OpenVINO release (2018 R5 0.1) for Windows 10 which, if I understood correctly, comes with a fully built OpenCV. Many tutorials show the use of that OpenCV but I failed to make it work on my Anaconda environment (with Python 3.6).</p>
<hr />
<p>Running the environement setup <code>C:\Intel\cvsdk\bin\setupvars.bat</code> script I get the following output:</p>
<blockquote>
<p>Commande ECHO désactivée.</p>
<p>PYTHONPATH=C:\Intel\computer_vision_sdk_2018.5.456\python\python3.6;C:\Program Files\Python36;</p>
<p>[setupvars.bat] OpenVINO environment initialized</p>
</blockquote>
<p>In my conda env, if I have no opencv package installed, I get the error:</p>
<p><code>ModuleNotFoundError: No module named cv2</code></p>
<p>And if I install one with <code>conda install py-opencv</code> (or <code>opencv</code>), I get this:</p>
<p><code>cv2.error: OpenCV(3.4.2) [...] Build OpenCV with Inference Engine to enable loading models from Model Optimizer</code></p>
<p>Installing with pip (<code>pip install opencv-python</code>) while on the anaconda environment also doesn't work:</p>
<p><code>cv2.error: OpenCV(4.0.0) [...] Build OpenCV with Inference Engine to enable loading models from Model Optimizer</code></p>
<hr />
<p>For clarification, I have successfully built the opencv examples with CMake and can run the executables. Here is the output of one of their sample programs:</p>
<pre><code>(OpenVino) C:\Intel\computer_vision_sdk_2018.5.456\opencv\build\Debug&gt;openvino_sample_opencv_version.exe
</code></pre>
<blockquote>
<p>Welcome to OpenCV 4.0.1-openvino</p>
</blockquote>
<p>Clearly, that OpenCV is usable somehow, I just can't find how to use it in my conda environment from a python script.</p>
","<p>you need to run</p>

<pre><code>C:\Intel\cvsdk\bin\setupvars.bat
</code></pre>

<p>every time you activate the enviroment</p>
","3099","1","4","<python><opencv><anaconda><openvino>"
"54772173","Using Openvino's OpenCV build on Anaconda environment","2019-02-19 17:50:25","<p>I have recently installed the latest OpenVINO release (2018 R5 0.1) for Windows 10 which, if I understood correctly, comes with a fully built OpenCV. Many tutorials show the use of that OpenCV but I failed to make it work on my Anaconda environment (with Python 3.6).</p>
<hr />
<p>Running the environement setup <code>C:\Intel\cvsdk\bin\setupvars.bat</code> script I get the following output:</p>
<blockquote>
<p>Commande ECHO désactivée.</p>
<p>PYTHONPATH=C:\Intel\computer_vision_sdk_2018.5.456\python\python3.6;C:\Program Files\Python36;</p>
<p>[setupvars.bat] OpenVINO environment initialized</p>
</blockquote>
<p>In my conda env, if I have no opencv package installed, I get the error:</p>
<p><code>ModuleNotFoundError: No module named cv2</code></p>
<p>And if I install one with <code>conda install py-opencv</code> (or <code>opencv</code>), I get this:</p>
<p><code>cv2.error: OpenCV(3.4.2) [...] Build OpenCV with Inference Engine to enable loading models from Model Optimizer</code></p>
<p>Installing with pip (<code>pip install opencv-python</code>) while on the anaconda environment also doesn't work:</p>
<p><code>cv2.error: OpenCV(4.0.0) [...] Build OpenCV with Inference Engine to enable loading models from Model Optimizer</code></p>
<hr />
<p>For clarification, I have successfully built the opencv examples with CMake and can run the executables. Here is the output of one of their sample programs:</p>
<pre><code>(OpenVino) C:\Intel\computer_vision_sdk_2018.5.456\opencv\build\Debug&gt;openvino_sample_opencv_version.exe
</code></pre>
<blockquote>
<p>Welcome to OpenCV 4.0.1-openvino</p>
</blockquote>
<p>Clearly, that OpenCV is usable somehow, I just can't find how to use it in my conda environment from a python script.</p>
","<p>For a clean installation of openVINO &amp; anaconda should be enough to run the environment setup, as mentioned <a href=""https://software.intel.com/en-us/forums/intel-distribution-of-openvino-toolkit/topic/802287#comment-1938106"" rel=""nofollow noreferrer"">here</a>, for Jupyter notebooks however could be better to run it explicitly at the beginning with:</p>

<p>PC</p>

<pre><code>!C:\Intel\...\bin\setupvars.bat
</code></pre>

<p>Mac</p>

<pre><code>!source /opt/intel/openvino/bin/setupvars.sh
</code></pre>
","3099","1","4","<python><opencv><anaconda><openvino>"
"54772173","Using Openvino's OpenCV build on Anaconda environment","2019-02-19 17:50:25","<p>I have recently installed the latest OpenVINO release (2018 R5 0.1) for Windows 10 which, if I understood correctly, comes with a fully built OpenCV. Many tutorials show the use of that OpenCV but I failed to make it work on my Anaconda environment (with Python 3.6).</p>
<hr />
<p>Running the environement setup <code>C:\Intel\cvsdk\bin\setupvars.bat</code> script I get the following output:</p>
<blockquote>
<p>Commande ECHO désactivée.</p>
<p>PYTHONPATH=C:\Intel\computer_vision_sdk_2018.5.456\python\python3.6;C:\Program Files\Python36;</p>
<p>[setupvars.bat] OpenVINO environment initialized</p>
</blockquote>
<p>In my conda env, if I have no opencv package installed, I get the error:</p>
<p><code>ModuleNotFoundError: No module named cv2</code></p>
<p>And if I install one with <code>conda install py-opencv</code> (or <code>opencv</code>), I get this:</p>
<p><code>cv2.error: OpenCV(3.4.2) [...] Build OpenCV with Inference Engine to enable loading models from Model Optimizer</code></p>
<p>Installing with pip (<code>pip install opencv-python</code>) while on the anaconda environment also doesn't work:</p>
<p><code>cv2.error: OpenCV(4.0.0) [...] Build OpenCV with Inference Engine to enable loading models from Model Optimizer</code></p>
<hr />
<p>For clarification, I have successfully built the opencv examples with CMake and can run the executables. Here is the output of one of their sample programs:</p>
<pre><code>(OpenVino) C:\Intel\computer_vision_sdk_2018.5.456\opencv\build\Debug&gt;openvino_sample_opencv_version.exe
</code></pre>
<blockquote>
<p>Welcome to OpenCV 4.0.1-openvino</p>
</blockquote>
<p>Clearly, that OpenCV is usable somehow, I just can't find how to use it in my conda environment from a python script.</p>
","<p>I solved the problem by using windows command prompt rather than power shell, which is recommended by the official open-vino doc.</p>
","3099","1","4","<python><opencv><anaconda><openvino>"
"54608215","When I try to compile OpenCv with inference engine enabled, I am getting an error","2019-02-09 16:31:34","<p>If I try to build OpenCV 4.0.0 with inferenca Engineen enabled, I am getting this error:</p>
<pre><code>1&gt;------ Build started: Project: opencv_dnn, Configuration: Release x64 ------
1&gt;dnn.cpp
1&gt;C:\local\opencv-4.0.0\modules\dnn\src\dnn.cpp(1595): error C2259: 'cv::dnn::InfEngineBackendNet': cannot instantiate abstract class
1&gt;C:\local\opencv-4.0.0\modules\dnn\src\dnn.cpp(1595): note: due to following members:
1&gt;C:\local\opencv-4.0.0\modules\dnn\src\dnn.cpp(1595): note: 'InferenceEngine::StatusCode InferenceEngine::ICNNNetwork::serialize(const std::string &amp;,const std::string &amp;,InferenceEngine::ResponseDesc *) noexcept const': is abstract
1&gt;C:\local\Intel\computer_vision_sdk_2018.5.445\deployment_tools\inference_engine\include\ie_icnn_network.hpp(190): note: see declaration of 'InferenceEngine::ICNNNetwork::serialize'
1&gt;C:\local\opencv-4.0.0\modules\dnn\src\dnn.cpp(2556): error C2259: 'cv::dnn::InfEngineBackendNet': cannot instantiate abstract class
1&gt;C:\local\opencv-4.0.0\modules\dnn\src\dnn.cpp(2556): note: due to following members:
1&gt;C:\local\opencv-4.0.0\modules\dnn\src\dnn.cpp(2556): note: 'InferenceEngine::StatusCode InferenceEngine::ICNNNetwork::serialize(const std::string &amp;,const std::string &amp;,InferenceEngine::ResponseDesc *) noexcept const': is abstract
1&gt;C:\local\Intel\computer_vision_sdk_2018.5.445\deployment_tools\inference_engine\include\ie_icnn_network.hpp(190): note: see declaration of 'InferenceEngine::ICNNNetwork::serialize'
1&gt;Done building project &quot;opencv_dnn.vcxproj&quot; -- FAILED.
</code></pre>
<p>Why I am getting this error?</p>
<h1>Edit</h1>
<p>Cmake output is:</p>
<pre><code>Selecting Windows SDK version 10.0.17763.0 to target Windows 10.0.17134.
AVX_512F is not supported by C++ compiler
AVX512_SKX is not supported by C++ compiler
Dispatch optimization AVX512_SKX is not available, skipped
libjpeg-turbo: VERSION = 1.5.3, BUILD = opencv-4.0.0-libjpeg-turbo
Looking for Mfapi.h
Looking for Mfapi.h - found
found Intel IPP (ICV version): 2019.0.0 [2019.0.0 Gold]
at: C:/local/opencv-build/3rdparty/ippicv/ippicv_win/icv
found Intel IPP Integration Wrappers sources: 2019.0.0
at: C:/local/opencv-build/3rdparty/ippicv/ippicv_win/iw
Could not find OpenBLAS include. Turning OpenBLAS_FOUND off
Could not find OpenBLAS lib. Turning OpenBLAS_FOUND off
Could NOT find BLAS (missing: BLAS_LIBRARIES) 
LAPACK requires BLAS
A library with LAPACK API not found. Please specify library location.
Could NOT find JNI (missing: JAVA_AWT_LIBRARY JAVA_JVM_LIBRARY JAVA_INCLUDE_PATH JAVA_INCLUDE_PATH2 JAVA_AWT_INCLUDE_PATH) 
Detected InferenceEngine: cmake package
VTK is not found. Please set -DVTK_DIR in CMake to VTK build directory, or to VTK install subdirectory with VTKConfig.cmake file
OpenCV Python: during development append to PYTHONPATH: C:/local/opencv-build/python_loader
Excluding from source files list: &lt;BUILD&gt;/modules/dnn/layers/layers_common.avx512_skx.cpp

General configuration for OpenCV 4.0.0 =====================================
  Version control:               unknown

  Platform:
    Timestamp:                   2019-02-09T13:06:47Z
    Host:                        Windows 10.0.17134 AMD64
    CMake:                       3.13.0-rc3
    CMake generator:             Visual Studio 15 2017 Win64
    CMake build tool:            C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/MSBuild/15.0/Bin/MSBuild.exe
    MSVC:                        1916

  CPU/HW features:
    Baseline:                    SSE SSE2 SSE3
      requested:                 SSE3
    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2
      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX
      SSE4_1 (7 files):          + SSSE3 SSE4_1
      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2
      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX
      AVX (5 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX
      AVX2 (13 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2

  C/C++:
    Built as dynamic libs?:      NO
    C++ Compiler:                C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe  (ver 19.16.27026.1)
    C++ flags (Release):         /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi      /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /MP4   /MT /O2 /Ob2 /DNDEBUG 
    C++ flags (Debug):           /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi      /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /MP4   /MTd /Zi /Ob0 /Od /RTC1 
    C Compiler:                  C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe
    C flags (Release):           /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi        /MP4    /MT /O2 /Ob2 /DNDEBUG 
    C flags (Debug):             /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi        /MP4  /MTd /Zi /Ob0 /Od /RTC1 
    Linker flags (Release):      /machine:x64  /NODEFAULTLIB:atlthunk.lib /INCREMENTAL:NO  /NODEFAULTLIB:libcmtd.lib /NODEFAULTLIB:libcpmtd.lib /NODEFAULTLIB:msvcrtd.lib
    Linker flags (Debug):        /machine:x64  /NODEFAULTLIB:atlthunk.lib /debug /INCREMENTAL  /NODEFAULTLIB:libcmt.lib /NODEFAULTLIB:libcpmt.lib /NODEFAULTLIB:msvcrt.lib
    ccache:                      NO
    Precompiled headers:         YES
    Extra dependencies:          IE::inference_engine ade comctl32 gdi32 ole32 setupapi ws2_32
    3rdparty dependencies:       ittnotify libprotobuf zlib libjpeg-turbo libwebp libpng libtiff libjasper IlmImf quirc ippiw ippicv

  OpenCV modules:
    To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc java_bindings_generator ml objdetect photo python3 python_bindings_generator stitching ts video videoio
    Disabled:                    world
    Disabled by dependency:      -
    Unavailable:                 java js python2
    Applications:                tests perf_tests apps
    Documentation:               NO
    Non-free algorithms:         NO

  Windows RT support:            NO

  GUI: 
    Win32 UI:                    YES
    VTK support:                 NO

  Media I/O: 
    ZLib:                        build (ver 1.2.11)
    JPEG:                        build-libjpeg-turbo (ver 1.5.3-62)
    WEBP:                        build (ver encoder: 0x020e)
    PNG:                         build (ver 1.6.35)
    TIFF:                        build (ver 42 - 4.0.9)
    JPEG 2000:                   build (ver 1.900.1)
    OpenEXR:                     build (ver 1.7.1)
    HDR:                         YES
    SUNRASTER:                   YES
    PXM:                         YES
    PFM:                         YES

  Video I/O:
    DC1394:                      NO
    FFMPEG:                      YES (prebuilt binaries)
      avcodec:                   YES (ver 58.35.100)
      avformat:                  YES (ver 58.20.100)
      avutil:                    YES (ver 56.22.100)
      swscale:                   YES (ver 5.3.100)
      avresample:                YES (ver 4.0.0)
    GStreamer:                   NO
    DirectShow:                  YES
    Media Foundation:            YES

  Parallel framework:            Concurrency

  Trace:                         YES (with Intel ITT)

  Other third-party libraries:
    Intel IPP:                   2019.0.0 Gold [2019.0.0]
           at:                   C:/local/opencv-build/3rdparty/ippicv/ippicv_win/icv
    Intel IPP IW:                sources (2019.0.0)
              at:                C:/local/opencv-build/3rdparty/ippicv/ippicv_win/iw
    Lapack:                      NO
    Inference Engine:            YES (2018040000 / 1.5.0)
                libs:            C:/local/Intel/computer_vision_sdk_2018.5.445/deployment_tools/inference_engine/lib/intel64/Release/inference_engine.lib / C:/local/Intel/computer_vision_sdk_2018.5.445/deployment_tools/inference_engine/lib/intel64/Debug/inference_engined.lib
            includes:            C:/local/Intel/computer_vision_sdk_2018.5.445/deployment_tools/inference_engine/include
    Eigen:                       NO
    Custom HAL:                  NO
    Protobuf:                    build (3.5.1)

  OpenCL:                        YES (no extra features)
    Include path:                C:/local/opencv-4.0.0/3rdparty/include/opencl/1.2
    Link libraries:              Dynamic load

  Python 3:
    Interpreter:                 C:/Program Files/Python36/python.exe (ver 3.6.5)
    Libraries:                   C:/Program Files/Python36/libs/python36.lib (ver 3.6.5)
    numpy:                       C:/Users/m/AppData/Roaming/Python/Python36/site-packages/numpy/core/include (ver 1.15.4)
    packages path:               C:/Program Files/Python36/Lib/site-packages

  Python (for build):            C:/Program Files/Python36/python.exe

  Java:                          
    ant:                         NO
    JNI:                         NO
    Java wrappers:               NO
    Java tests:                  NO

  Install to:                    C:/local/opencv
-----------------------------------------------------------------

Configuring done
</code></pre>
","<p>Summarizing our discussion in comments,</p>

<p>OpenVINO R5 released in December however attached CMake summary shows that used OpenCV version is older (November). So the solution is to update OpenCV to the latest version which supports Intel's Inference Engine from OpenVINO R5.</p>

<p>Wiki of how to build OpenCV with IE support: <a href=""https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend"" rel=""nofollow noreferrer"">https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend</a></p>
","2463","1","1","<c++><opencv><openvino><opencv4>"
"63501025","How to set environment variables dynamically by script in Dockerfile?","2020-08-20 08:17:45","<p>I build my project by Dockerfile. The project need to installation of Openvino. Openvino needs to set some environment variables dynamically by a script that depends on architecture. The sciprt is: <a href=""https://gist.github.com/ahmetanbar/10257fd888323bf3bd3e4c6e99fcec37#file-setupvars-sh-L25"" rel=""nofollow noreferrer"">script to set environment variables</a></p>
<p>As I learn, Dockerfile can't set enviroment variables to image from a script.</p>
<p>How do I follow way to solve the problem?</p>
<p>I need to set the variables because later I continue install opencv that looks the enviroment variables.</p>
<p>What I think that if I put the script to ~/.bashrc file to set variables when connect to bash, if I have any trick to start bash for a second, it could solve my problem.</p>
<p>Secondly I think that build openvino image, create container from that, connect it and initiliaze variables by running script manually in container. After that, convert the container to image. Create new Dockerfile and continue building steps by using this images for ongoing steps.</p>
<p><a href=""https://gist.github.com/ahmetanbar/c6807a5b4a9baf84b79e6fa6fde07ef1#file-openvino-dockerfile-for-linux-L35"" rel=""nofollow noreferrer"">Openvino Dockerfile exp and line that run the script</a></p>
<p>My Dockerfile:</p>
<pre><code>FROM ubuntu:18.04

ARG DOWNLOAD_LINK=http://registrationcenter-download.intel.com/akdlm/irc_nas/16612/l_openvino_toolkit_p_2020.2.120.tgz

ENV INSTALLDIR /opt/intel/openvino

# openvino download
RUN curl -LOJ &quot;${DOWNLOAD_LINK}&quot;

# opencv download
RUN wget -O opencv.zip https://github.com/opencv/opencv/archive/4.3.0.zip &amp;&amp; \
    wget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.3.0.zip

RUN apt-get -y install sudo

# openvino installation
RUN tar -xvzf ./*.tgz &amp;&amp; \
    cd l_openvino_toolkit_p_2020.2.120 &amp;&amp; \
    sed -i 's/decline/accept/g' silent.cfg &amp;&amp; \
    ./install.sh -s silent.cfg &amp;&amp; \
    # rm -rf /tmp/* &amp;&amp; \
    sudo -E $INSTALLDIR/install_dependencies/install_openvino_dependencies.sh

WORKDIR /home/sa

RUN /bin/bash -c &quot;source /opt/intel/openvino/bin/setupvars.sh&quot; &amp;&amp; \
    echo &quot;source /opt/intel/openvino/bin/setupvars.sh&quot; &gt;&gt; /home/sa/.bashrc &amp;&amp; \
    echo &quot;source /opt/intel/openvino/bin/setupvars.sh&quot; &gt;&gt; ~/.bashrc &amp;&amp; \
    $INSTALLDIR/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites.sh &amp;&amp; \
    $INSTALLDIR/deployment_tools/demo/demo_squeezenet_download_convert_run.sh

RUN bash

# opencv installation

RUN unzip opencv.zip &amp;&amp; \
    unzip opencv_contrib.zip &amp;&amp; \
    # rm opencv.zip opencv_contrib.zip &amp;&amp; \
    mv opencv-4.3.0 opencv &amp;&amp; \
    mv opencv_contrib-4.3.0 opencv_contrib &amp;&amp; \
    cd ./opencv &amp;&amp; \
    mkdir build &amp;&amp; \
    cd build &amp;&amp; \
    cmake -D CMAKE_BUILD_TYPE=RELEASE -D WITH_INF_ENGINE=ON -D ENABLE_CXX11=ON -D CMAKE_INSTALL_PREFIX=/usr/local -D INSTALL_PYTHON_EXAMPLES=OFF -D INSTALL_C_EXAMPLES=OFF -D ENABLE_PRECOMPILED_HEADERS=OFF -D OPENCV_ENABLE_NONFREE=ON -D OPENCV_EXTRA_MODULES_PATH=/home/sa/opencv_contrib/modules -D PYTHON_EXECUTABLE=/usr/bin/python3 -D WIDTH_GTK=ON -D BUILD_TESTS=OFF -D BUILD_DOCS=OFF -D WITH_GSTREAMER=OFF -D WITH_FFMPEG=ON -D BUILD_EXAMPLES=OFF .. &amp;&amp; \
    make &amp;&amp; \
    make install &amp;&amp; \
    ldconfig
</code></pre>
","<p>You need to cause the shell to load that file in every <code>RUN</code> command where you use it, and also at container startup time.</p>
<p>For startup time, you can use an entrypoint wrapper script:</p>
<pre class=""lang-sh prettyprint-override""><code>#!/bin/sh
# Load the script of environment variables
. /opt/intel/openvino/bin/setupvars.sh
# Run the main container command
exec &quot;$@&quot;
</code></pre>
<p>Then in the Dockerfile, you need to include the environment variable script in <code>RUN</code> commands, and make this script be the image's <code>ENTRYPOINT</code>.</p>
<pre class=""lang-sh prettyprint-override""><code>RUN . /opt/intel/openvino/bin/setupvars.sh &amp;&amp; \
    /opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites.sh &amp;&amp; \
    /opt/intel/openvino/deployment_tools/demo/demo_squeezenet_download_convert_run.sh

RUN ... &amp;&amp; \
    . /opt/intel/openvino/bin/setupvars.sh &amp;&amp; \
    cmake ... &amp;&amp; \
    make &amp;&amp; \
    ...

 COPY entrypoint.sh .
 ENTRYPOINT [&quot;./entrypoint.sh&quot;]
 CMD same as the command you set in the original image
</code></pre>
<p>If you <code>docker exec</code> debugging shells in the container, they won't see these environment variables and you'll need to manually re-read the environment variable script.  If you use <code>docker inspect</code> to look at low-level details of the container, it also won't show the environment variables.</p>
<hr />
<p>It looks like that script just sets a couple of environment variables (especially <code>$LD_LIBRARY_PATH</code> and <code>$PYTHONPATH</code>), if to somewhat long-winded values, and you could just set these with <code>ENV</code> statements in the Dockerfile.</p>
<p>If you look at the <code>docker build</code> output, there are lines like <code>---&gt; 0123456789ab</code> after each build step; those are valid image IDs that you can <code>docker run</code>.  You could run</p>
<pre class=""lang-sh prettyprint-override""><code>docker run --rm 0123456789ab \
  env \
  | sort &gt; env-a
docker run --rm 0123456789ab \
  sh -c '. /opt/intel/openvino/bin/setupvars.sh &amp;&amp; env' \
  | sort &gt; env-b
</code></pre>
<p>This will give you two local files with the environment variables with and without running this setup script.  Find the differences (say, with <strong>comm</strong>(1)), put <code>ENV</code> before each line, and add that to your Dockerfile.</p>
<hr />
<p>You can't really use <code>.bashrc</code> in Docker.  Many common paths don't invoke its <a href=""https://www.gnu.org/software/bash/manual/html_node/Bash-Startup-Files.html#Bash-Startup-Files"" rel=""nofollow noreferrer"">startup files</a>: in the language of that documentation, neither a Dockerfile <code>RUN</code> command nor a <code>docker run</code> instruction is an &quot;interactive shell&quot; so those don't read dot files, and usually <code>docker run ... command</code> doesn't invoke a shell at all.</p>
<p>You also don't need <code>sudo</code> (you are already running as root, and an interactive password prompt will fail); <code>RUN sh -c</code> is redundant (Docker inserts it on its own); and <code>source</code> isn't a standard shell command (prefer the standard <code>.</code>, which will work even on Alpine-based images that don't have shell extensions).</p>
","2446","1","1","<docker><dockerfile><openvino>"
"62129609","OpenVINO - Toolkit with YoloV4","2020-06-01 09:51:41","<p>I am currently working with the YoloV3-tiny.
Repository: <a href=""https://github.com/AlexeyAB/darknet"" rel=""nofollow noreferrer"">https://github.com/AlexeyAB/darknet</a></p>

<p>To import the network into C++ project I use OpenVINO-Toolkit. In more detail I use the following procedure to convert the network:
<a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_YOLO_From_Tensorflow.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_YOLO_From_Tensorflow.html</a></p>

<p>This procedure carries out a conversion and an optimization to proceed with the inference.</p>

<p>Now, I would like to try the YoloV4 because it seems to be more effective for the purpose of the project. The problem is that OpenVINO Toolkit does not yet support this version and does not report the .json (file needed for optimization) file relative to version 4 but only up to version 3.</p>

<p>What has changed in terms of structure between version 3 and version 4 of the Yolo?
Can I hopefully hope that the conversion of the YoloV3-tiny (or YoloV3) is the same as the YoloV4?
Is the YoloV4 much slower than the YoloV3-tiny using only the CPU for inference?
When will the YoloV4-tiny be available?
Anyone have information about it?</p>

<p>Thanks in advance to anyone who gives me useful information.</p>
","<ul>
<li>The difference between YoloV4 and YoloV3 is the backbone. YoloV4 has CSPDarknet53, whilst YoloV3 has Darknet53 backbone.
See <a href=""https://arxiv.org/pdf/2004.10934.pdf"" rel=""noreferrer"">https://arxiv.org/pdf/2004.10934.pdf</a>.</li>
<li>Also, YoloV4 is not supported officially by OpenVINO. However, you can still test and validate YoloV4 on your end with some workaround. There is one way for now to run YoloV4 through OpenCV which will build network using nGraph API and then pass to Inference Engine. See <a href=""https://github.com/opencv/opencv/pull/17185"" rel=""noreferrer"">https://github.com/opencv/opencv/pull/17185</a>.</li>
<li>The key problem is the Mish activation function - there is no optimized implementation yet, which is why we have to implement it by definition with tanh and exponential functions. Unfortunately, one-to-one topology comparison shows significant performance degradation. The performance results are also available in the github link above.</li>
</ul>
","2430","2","2","<yolo><openvino>"
"62129609","OpenVINO - Toolkit with YoloV4","2020-06-01 09:51:41","<p>I am currently working with the YoloV3-tiny.
Repository: <a href=""https://github.com/AlexeyAB/darknet"" rel=""nofollow noreferrer"">https://github.com/AlexeyAB/darknet</a></p>

<p>To import the network into C++ project I use OpenVINO-Toolkit. In more detail I use the following procedure to convert the network:
<a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_YOLO_From_Tensorflow.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_YOLO_From_Tensorflow.html</a></p>

<p>This procedure carries out a conversion and an optimization to proceed with the inference.</p>

<p>Now, I would like to try the YoloV4 because it seems to be more effective for the purpose of the project. The problem is that OpenVINO Toolkit does not yet support this version and does not report the .json (file needed for optimization) file relative to version 4 but only up to version 3.</p>

<p>What has changed in terms of structure between version 3 and version 4 of the Yolo?
Can I hopefully hope that the conversion of the YoloV3-tiny (or YoloV3) is the same as the YoloV4?
Is the YoloV4 much slower than the YoloV3-tiny using only the CPU for inference?
When will the YoloV4-tiny be available?
Anyone have information about it?</p>

<p>Thanks in advance to anyone who gives me useful information.</p>
","<p><a href=""https://github.com/TNTWEN/OpenVINO-YOLOV4"" rel=""nofollow noreferrer"">https://github.com/TNTWEN/OpenVINO-YOLOV4</a>
This is my project based on v3's converter (darknet -&gt; tensorflow -&gt;IR)and i have finished the adaptation of OpenVINO Yolov4,v4-relu,v4-tiny.
You could have a try.  And you can use V4's IRmodel and run on v3's c++ demo directly</p>
","2430","2","2","<yolo><openvino>"
"55345798","How to use OpenVINO pre-trained models?","2019-03-25 20:19:59","<p>I have installed OpenVINO recently but I don't know how I should give inputs and get the predict from OpenVINOs pre-trained models.</p>

<p>there is two files with .bin and .xml suffixes, I've just worked with keras so I can't use this models in opencv.</p>

<p>I find this code but it didn't work.</p>

<pre><code>import cv2 as cv

net = cv.dnn.readNet('face-detection-adas-0001.bin', 'face-detection-adas-0001.xml')

cap = cv.VideoCapture(0)

while cv.waitKey(1) &lt; 0:
    hasFrame, frame = cap.read()
    if not hasFrame:
        break

    blob = cv.dnn.blobFromImage(frame, size=(672, 384))
    net.setInput(blob)
    out = net.forward()

    for detection in out.reshape(-1, 7):
        confidence = float(detection[2])
        xmin = int(detection[3] * frame.shape[1])
        ymin = int(detection[4] * frame.shape[0])
        xmax = int(detection[5] * frame.shape[1])
        ymax = int(detection[6] * frame.shape[0])

        if confidence &gt; 0.5:
            cv.rectangle(frame, (xmin, ymin), (xmax, ymax), color=(0, 255, 0))

    cv.imshow('OpenVINO face detection', frame)
</code></pre>

<p>there's the error code:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\Ali-10\Desktop\facial_landmark\face.py"", line 3, in &lt;module&gt;
    net = cv.dnn.readNet('face-detection-adas-0001.bin', 'face-detection-adas-0001.xml')
    cv2.error: OpenCV(3.4.4) C:\projects\opencv-python\opencv\modules\dnn\src\dnn.cpp:2428: error: (-2:Unspecified error) Build OpenCV with Inference Engine to enable loading models from Model Optimizer. in function 'cv::dnn::experimental_dnn_34_v10::Net::readFromModelOptimizer'
</code></pre>

<p>I expect the prediction of the model but I just get this error.</p>
","<p>You need to build OpenCV with Inference Engine support as mentioned in the message. See wiki for details: <a href=""https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend"" rel=""nofollow noreferrer"">https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend</a>.</p>

<p>If you use OpenCV from OpenVINO distribution, it must be already built with IE (maybe except a single R5.1 release from January of 2019).</p>

<p>We also work on simplified way to build OpenCV with IE (without specifying paths but just downloading it's source code by cmake), see PR <a href=""https://github.com/opencv/opencv/pull/13965"" rel=""nofollow noreferrer"">https://github.com/opencv/opencv/pull/13965</a>.</p>
","2371","2","2","<python><intel><openvino>"
"55345798","How to use OpenVINO pre-trained models?","2019-03-25 20:19:59","<p>I have installed OpenVINO recently but I don't know how I should give inputs and get the predict from OpenVINOs pre-trained models.</p>

<p>there is two files with .bin and .xml suffixes, I've just worked with keras so I can't use this models in opencv.</p>

<p>I find this code but it didn't work.</p>

<pre><code>import cv2 as cv

net = cv.dnn.readNet('face-detection-adas-0001.bin', 'face-detection-adas-0001.xml')

cap = cv.VideoCapture(0)

while cv.waitKey(1) &lt; 0:
    hasFrame, frame = cap.read()
    if not hasFrame:
        break

    blob = cv.dnn.blobFromImage(frame, size=(672, 384))
    net.setInput(blob)
    out = net.forward()

    for detection in out.reshape(-1, 7):
        confidence = float(detection[2])
        xmin = int(detection[3] * frame.shape[1])
        ymin = int(detection[4] * frame.shape[0])
        xmax = int(detection[5] * frame.shape[1])
        ymax = int(detection[6] * frame.shape[0])

        if confidence &gt; 0.5:
            cv.rectangle(frame, (xmin, ymin), (xmax, ymax), color=(0, 255, 0))

    cv.imshow('OpenVINO face detection', frame)
</code></pre>

<p>there's the error code:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\Ali-10\Desktop\facial_landmark\face.py"", line 3, in &lt;module&gt;
    net = cv.dnn.readNet('face-detection-adas-0001.bin', 'face-detection-adas-0001.xml')
    cv2.error: OpenCV(3.4.4) C:\projects\opencv-python\opencv\modules\dnn\src\dnn.cpp:2428: error: (-2:Unspecified error) Build OpenCV with Inference Engine to enable loading models from Model Optimizer. in function 'cv::dnn::experimental_dnn_34_v10::Net::readFromModelOptimizer'
</code></pre>

<p>I expect the prediction of the model but I just get this error.</p>
","<p>I just tested this code and it works perfectly fine. You need to install openvino first and then run setupvars.bat file in order to initialize the openvino enviroment. Once that is done, you can run your code and it will start detecting your face. I tested this on Intel i5 with 12Gb RAM and I was getting 23-25fps which is good. </p>
","2371","2","2","<python><intel><openvino>"
"56682575","Running MTCNN with OpenVino","2019-06-20 09:05:58","<p>I am trying to use OpenVino python API to run MTCNN face detection, however, the performance of the converted models degraded significantly from the original model. I am wondering how I could get similar results. </p>

<p>I converted the <a href=""https://github.com/TropComplique/mtcnn-pytorch/tree/master/caffe_models"" rel=""nofollow noreferrer"">mtcnn caffe models</a> into OpenVino *.xml and *.bin files using the following commands.</p>

<pre><code>python3 mo.py --input_model path/to/PNet/det1.caffemodel --model_name det1 --output_dir path/to/output_dir
python3 mo.py --input_model path/to/RNet/det2.caffemodel --model_name det2 --output_dir path/to/output_dir
python3 mo.py --input_model path/to/ONet/det3.caffemodel --model_name det3 --output_dir path/to/output_dir
</code></pre>

<p>And used the <a href=""https://github.com/TropComplique/mtcnn-pytorch/blob/master/try_mtcnn_step_by_step.ipynb"" rel=""nofollow noreferrer"">step_by_step mtcnn jupyter notebook</a> to check the performance of the converted models.</p>

<p>But detection results using OpenVino models degraded significantly. To regenerate the results you only need to load OpenVino models instead of pytorch model in the notebook.</p>

<p>To regenerate my results do the following steps.</p>

<p>Clone <a href=""https://github.com/TropComplique/mtcnn-pytorch.git"" rel=""nofollow noreferrer"">https://github.com/TropComplique/mtcnn-pytorch.git</a></p>

<p>And use <a href=""https://dl.dropboxusercontent.com/s/u0l63dv1lueckly/openvino.ipynb"" rel=""nofollow noreferrer"">this jupyter notebbok</a></p>

<p>As you will see the detected boxes in the first stage after P-Net are more than the detected boxes in the original model <a href=""https://github.com/TropComplique/mtcnn-pytorch/blob/master/try_mtcnn_step_by_step.ipynb"" rel=""nofollow noreferrer"">step_by_step mtcnn jupyter notebook</a>.</p>

<p>Do you have any comment on this. It seems that there is no problem in model conversion the only difference is that pytorch has a variable tensor size (FloatTensor) but for OpenVino I have to reshape the input size for each scale. This might be the reason to get different results, however I have not been able to solve this problem.</p>
","<p>I went through all the possible mistake I might had made and check parameters to convert mtcnn models from list_topologies.yaml. This file comes with OpenVino installation and list the parameters like scale mean values and etc.</p>

<p>Finally, I solved the problem by using MXNET pre-trained <a href=""https://github.com/YYuanAnyVision/mxnet_mtcnn_face_detection"" rel=""nofollow noreferrer"">MTCNN networks</a>.</p>

<p>I hope this would help other users who might encounter this problem.</p>
","2323","2","1","<python><face-detection><openvino>"
"55611848","How to set input shape for model optimizer in OpenVINO for Tacotron model?","2019-04-10 11:49:55","<p>I'm trying to get KeithIto's Tacotron model run on Intel OpenVINO with NCS. The model optimizer fails to convert the frozen model to IR format. </p>

<p>After asking in the Intel Forum, I was told the 2018 R5 release didn't have GRU support and I changed it to LSTM cells. But the model still runs well in tensorflow after training it. Also I updated my OpenVINO to 2019 R1 release. But the optimizer still threw errors. The model has mainly two input nodes: inputs[N,T_in] and input_lengths[N]; where N is batch size, T_in is number of steps in the input time series, and values are character IDs with default shapes as [1,?] and [1]. 
The problem is with [1,?] as model optimizer doesn't allow for dynamic shapes. I tried different values and it always throws some errors. </p>

<p>I tried frozen graphs with output node ""model/griffinlim/Squeeze"" which is the final decoder output and also with ""model/inference/dense/BiasAdd"" as mentioned in (<a href=""https://github.com/keithito/tacotron/issues/95#issuecomment-362854371"" rel=""nofollow noreferrer"">https://github.com/keithito/tacotron/issues/95#issuecomment-362854371</a>) which is the input for the Griffin-lim vocoder so that I can do the Spectrogram2Wav part outside the model and reduce it's complexity.</p>

<pre><code>C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer&gt;python mo_tf.py --input_model ""D:\Programming\LSTM\logs-tacotron\freezeinf.pb"" --freeze_placeholder_with_value ""input_lengths-&gt;[1]"" --input inputs --input_shape [1,128] --output model/inference/dense/BiasAdd
Model Optimizer arguments:
Common parameters:
        - Path to the Input Model:      D:\Programming\Thesis\LSTM\logs-tacotron\freezeinf.pb
        - Path for generated IR:        C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer\.
        - IR output name:       freezeinf
        - Log level:    ERROR
        - Batch:        Not specified, inherited from the model
        - Input layers:         inputs
        - Output layers:        model/inference/dense/BiasAdd
        - Input shapes:         [1,128]
        - Mean values:  Not specified
        - Scale values:         Not specified
        - Scale factor:         Not specified
        - Precision of IR:      FP32
        - Enable fusing:        True
        - Enable grouped convolutions fusing:   True
        - Move mean values to preprocess section:       False
        - Reverse input channels:       False
TensorFlow specific parameters:
        - Input model in text protobuf format:  False
        - Path to model dump for TensorBoard:   None
        - List of shared libraries with TensorFlow custom layers implementation:        None
        - Update the configuration file with input/output node names:   None
        - Use configuration file used to generate the model with Object Detection API:  None
        - Operations to offload:        None
        - Patterns to offload:  None
        - Use the config file:  None
Model Optimizer version:        2019.1.0-341-gc9b66a2
[ ERROR ]  Shape [  1  -1 128] is not fully defined for output 0 of ""model/inference/post_cbhg/conv_bank/conv1d_8/batch_normalization/batchnorm/mul_1"". Use --input_shape with positive integers to override model input shapes.
[ ERROR ]  Cannot infer shapes or values for node ""model/inference/post_cbhg/conv_bank/conv1d_8/batch_normalization/batchnorm/mul_1"".
[ ERROR ]  Not all output shapes were inferred or fully defined for node ""model/inference/post_cbhg/conv_bank/conv1d_8/batch_normalization/batchnorm/mul_1"".
 For more information please refer to Model Optimizer FAQ (&lt;INSTALL_DIR&gt;/deployment_tools/documentation/docs/MO_FAQ.html), question #40.
[ ERROR ]
[ ERROR ]  It can happen due to bug in custom shape infer function &lt;function tf_eltwise_ext.&lt;locals&gt;.&lt;lambda&gt; at 0x000001F00598FE18&gt;.
[ ERROR ]  Or because the node inputs have incorrect values/shapes.
[ ERROR ]  Or because input shapes are incorrect (embedded to the model or passed via --input_shape).
[ ERROR ]  Run Model Optimizer with --log_level=DEBUG for more information.
[ ERROR ]  Exception occurred during running replacer ""REPLACEMENT_ID"" (&lt;class 'extensions.middle.PartialInfer.PartialInfer'&gt;): Stopped shape/value propagation at ""model/inference/post_cbhg/conv_bank/conv1d_8/batch_normalization/batchnorm/mul_1"" node.
 For more information please refer to Model Optimizer FAQ (&lt;INSTALL_DIR&gt;/deployment_tools/documentation/docs/MO_FAQ.html), question #38.
</code></pre>

<p>I also tried different methods for freezing the graph. </p>

<p><strong>METHODS 1:</strong>
Using freeze_graph.py provided in Tensorflow after dumping graph with:</p>

<pre><code>tf.train.write_graph(self.session.graph.as_graph_def(), ""models/"", ""graph.pb"", as_text=True)
</code></pre>

<p>followed by:</p>

<pre><code>python freeze_graph.py --input_graph .\models\graph.pb  --output_node_names ""model/griffinlim/Squeeze"" --output_graph .\logs-tacotron\freezeinf.pb --input_checkpoint .\logs-tacotron\model.ckpt-33000 --input_binary=true
</code></pre>

<p><strong>METHODS 2:</strong>
Using the following code after loading the model:</p>

<pre><code>frozen = tf.graph_util.convert_variables_to_constants(self.session,self.session.graph_def, [""model/inference/dense/BiasAdd""]) #model/griffinlim/Squeeze
graph_io.write_graph(frozen, ""models/"", ""freezeinf.pb"", as_text=False)
</code></pre>

<p>I expected the BatchNormalization and Dropout layers to be removed after the freezing, but looking at the errors it seems that it still exists.</p>

<p><strong>Environment</strong></p>

<p>OS: Windows 10 Pro</p>

<p>Python 3.6.5</p>

<p>Tensorflow 1.12.0</p>

<p>OpenVINO 2019 R1 release</p>

<p>Can anyone help with the above problems with the optimizer?</p>
","<p>OpenVINO does not support this model yet. We will keep you updated when it will be. </p>
","2091","3","1","<python><tensorflow><text-to-speech><speech-synthesis><openvino>"
"56240111","OpenVINO + HDDL plugin - Cannot run openvino samples - ""HDDL hardware initialization failed""","2019-05-21 14:07:08","<p>I'm trying to get OpenVINO samples working on an mPCIe Myriad X card (with 2 MA2485 chips).  </p>

<p>My goal is to get the samples working using the HDDL plugin, as from my understanding it should allow for working with multiple chips in parallel.</p>

<p>Using the ""MYRIAD"" plugin, the benchmarks run successfully every single time:
<code>sudo -E ./demo_squeezenet_download_convert_run.sh -d MYRIAD</code></p>

<p>however, when switching to <code>-d HDDL</code> I get the following:</p>

<pre><code>Run ./classification_sample -d HDDL -i /opt/intel/openvino_2019.1.144/deployment_tools/demo/car.png -m /home/vino/openvino_models/ir/FP16//classification/squeezenet/1.1/caffe/squeezenet1.1.xml 

[ INFO ] InferenceEngine: 
    API version ............ 1.6
    Build .................. custom_releases/2019/R1.1_28dfbfdd28954c4dfd2f94403dd8dfc1f411038b
[ INFO ] Parsing input parameters
[ INFO ] Files were added: 1
[ INFO ]     /opt/intel/openvino_2019.1.144/deployment_tools/demo/car.png
[ INFO ] Loading plugin

    API version ............ 1.6
    Build .................. 23780
    Description ....... HDDLPlugin
[ INFO ] Loading network files:
    /home/vino/openvino_models/ir/FP16//classification/squeezenet/1.1/caffe/squeezenet1.1.xml
    /home/vino/openvino_models/ir/FP16//classification/squeezenet/1.1/caffe/squeezenet1.1.bin
[ INFO ] Preparing input blobs
[ WARNING ] Image is resized from (787, 259) to (227, 227)
[ INFO ] Batch size is 1
[ INFO ] Preparing output blobs
[ INFO ] Loading model to the plugin
[16:52:27.5232][31126]I[ServiceStarter.cpp:93] Info: Found HDDL Service is not running. To start HDDL Service ...
[16:52:27.5241][31126]I[ServiceStarter.cpp:40] Info: Waiting for HDDL Service getting ready ...
Config file detected at /opt/intel/openvino_2019.1.144/deployment_tools/inference_engine/external/hddl/config/bsl.json
scan F75114 device...
found 0 F75114 device
hid-f75114 init returned status BSL_ERROR_NO_HID_DEVICE_FOUND
ioexpander is disabled by config, skipping
mcu is disabled by config, skipping
Auto-scan is disabled by config, aborting
bsl init failed for:    BSL_ERROR_NO_HID_DEVICE_FOUND
[ion_close][69]close ion_fd = 3
Config file detected at /opt/intel/openvino_2019.1.144/deployment_tools/inference_engine/external/hddl/config/bsl.json
scan F75114 device...
found 0 F75114 device
hid-f75114 init returned status BSL_ERROR_NO_HID_DEVICE_FOUND
ioexpander is disabled by config, skipping
mcu is disabled by config, skipping
Auto-scan is disabled by config, aborting
bsl init failed for:    BSL_ERROR_NO_HID_DEVICE_FOUND
## HDDL_INSTALL_DIR: /opt/intel/openvino_2019.1.144/deployment_tools/inference_engine/external/hddl
[16:52:27.5367][31131]I[ConfigParser.cpp:176] Config file '/opt/intel/openvino_2019.1.144/deployment_tools/inference_engine/external/hddl/config/hddl_service.config' has been loaded
[16:52:27.5372][31131]I[FileHelper.cpp:272] Set file:/var/tmp/hddl_service_alive.mutex owner: user-'no_change', group-'users', mode-'0660'
[16:52:27.5372][31131]I[FileHelper.cpp:272] Set file:/var/tmp/hddl_service_ready.mutex owner: user-'no_change', group-'users', mode-'0660'
[16:52:27.5373][31131]I[FileHelper.cpp:272] Set file:/var/tmp/hddl_start_exit.mutex owner: user-'no_change', group-'users', mode-'0660'
[16:52:27.5374][31131]I[AutobootStarter.cpp:150] Info: No running autoboot process. Start autoboot daemon...
Config file detected at /opt/intel/openvino_2019.1.144/deployment_tools/inference_engine/external/hddl/config/bsl.json
scan F75114 device...
found 0 F75114 device
hid-f75114 init returned status BSL_ERROR_NO_HID_DEVICE_FOUND
ioexpander is disabled by config, skipping
mcu is disabled by config, skipping
Auto-scan is disabled by config, aborting
bsl init failed for:    BSL_ERROR_NO_HID_DEVICE_FOUND
[16:52:27.5457][31133]I[ConfigParser.cpp:176] Config file '/opt/intel/openvino_2019.1.144/deployment_tools/inference_engine/external/hddl/config/hddl_autoboot.config' has been loaded
[16:52:27.5462][31133]I[FileHelper.cpp:272] Set file:/var/tmp/hddl_autoboot_alive.mutex owner: user-'no_change', group-'users', mode-'0660'
[16:52:27.5463][31133]I[FileHelper.cpp:272] Set file:/var/tmp/hddl_autoboot_ready.mutex owner: user-'no_change', group-'users', mode-'0660'
[16:52:27.5463][31133]I[FileHelper.cpp:272] Set file:/var/tmp/hddl_autoboot_start_exit.mutex owner: user-'no_change', group-'users', mode-'0660'
[16:52:27.5463][31133]I[FileHelper.cpp:272] Set file:/tmp/hddl_autoboot_device.map owner: user-'no_change', group-'users', mode-'0660'
[16:52:27.5464][31133]I[AutoBoot.cpp:282] [Firmware Config] deviceName=default deviceNum=0 firmwarePath=/opt/intel/openvino_2019.1.144/deployment_tools/inference_engine/external/hddl/lib/mvnc/MvNCAPI-ma2480.mvcmd
Reset all devices with device type 3
[16:52:27.5468][31133]ERROR[AutoBoot.cpp:444] Error: HDDL hardware initialization failed, exits now.
[ion_close][69]close ion_fd = 3
</code></pre>

<p>I've tried messing with various configuration options but to no avail.
I've also tried reinstalling Ubuntu, which did not work either.</p>

<p>Using Ubuntu 16.04.6, kernel 4.4.0-148-generic.</p>

<p>lshw entry:</p>

<pre><code>       *-pci
             description: PCI bridge
             product: 7 Series/C216 Chipset Family PCI Express Root Port 1
             vendor: Intel Corporation
             physical id: 1c
             bus info: pci@0000:00:1c.0
             version: c4
             width: 32 bits
             clock: 33MHz
             capabilities: pci pciexpress msi pm normal_decode bus_master cap_list
             configuration: driver=pcieport
             resources: irq:16 memory:f7c00000-f7cfffff
           *-usb
                description: USB controller
                product: ASM1042A USB 3.0 Host Controller
                vendor: ASMedia Technology Inc.
                physical id: 0
                bus info: pci@0000:01:00.0
                version: 00
                width: 64 bits
                clock: 33MHz
                capabilities: msi msix pm pciexpress xhci bus_master cap_list
                configuration: driver=xhci_hcd latency=0
                resources: irq:16 memory:f7c00000-f7c07fff
              *-usbhost:0
                   product: xHCI Host Controller
                   vendor: Linux 4.4.0-148-generic xhci-hcd
                   physical id: 0
                   bus info: usb@6
                   logical name: usb6
                   version: 4.04
                   capabilities: usb-3.00
                   configuration: driver=hub slots=2 speed=5000Mbit/s
              *-usbhost:1
                   product: xHCI Host Controller
                   vendor: Linux 4.4.0-148-generic xhci-hcd
                   physical id: 1
                   bus info: usb@5
                   logical name: usb5
                   version: 4.04
                   capabilities: usb-2.00
                   configuration: driver=hub slots=2 speed=480Mbit/s
                 *-usb UNCLAIMED
                      description: Generic USB device
                      product: Movidius MyriadX
                      vendor: Movidius Ltd.
                      physical id: 1
                      bus info: usb@5:1
                      version: 0.01
                      serial: 03e72485
                      capabilities: usb-2.00
                      configuration: maxpower=500mA speed=480Mbit/s
</code></pre>

<p>${HDDL_INSTALL_DIR}/config/bsl.json (tried multiple configurations here, nothing worked):</p>

<pre><code>{
  ""autoscan"": false,
  ""comment_on_autoscan"": ""auto-scan can be true or false"",
  ""hid-f75114"": {
    ""enabled"": true,
    ""Linux_comment"": ""You can use setup_tools/path_detaction.sh to get the suggested paths"",
    ""Linux_path_example"": ""/sys/devices/pci0000:00/0000:00:01.1/0000:02:00.0/0000:03:04.0/0000:07:00.0/usb9/9-2/9-2.2/9-2.2:1.0"",
    ""Windows_comment"": ""HID\\VID_2C42&amp;PID_5114\\6&amp;A471F67&amp;0&amp;0000 is for Windows"",
    ""Comment"": ""Leave empty to pass all paths"",
    ""hid_paths"": [
        ""/sys/devices/pci0000:00/0000:00:1c.0/0000:01:00.0/usb5/5-1/5-1:1.0""
    ]
  },
  ""ioexpander"": {
    ""enabled"": false,
    ""i2c_addr"": [
      37,
      39
    ]
  },
  ""mcu"": {
    ""enabled"": false,
    ""i2c_addr"": [
      31
    ]
  }
}
</code></pre>
","<p>Turned out to be a hardware issue.</p>
","1914","1","1","<openvino><hddl><movidius>"
"54321017","Building opencv with Intel Inference Engine","2019-01-23 06:13:57","<p>Trying to load ssdlite v2 model with intel inference engine on raspberry Pi 3. For this, I need to build opencv-4.0 with Intel Inference API engine. I am unable to build open CV using CMAKE with  -DWITH_INF_ENGINE=ON ^
  -DENABLE_CXX11=ON flags. Does anyone know how to do it?</p>
","<p>First you need install or compile this engine, see you <a href=""https://software.intel.com/en-us/articles/OpenVINO-Install-RaspberryPI"" rel=""nofollow noreferrer"">Intel OpenVINO</a> documentation.</p>
","1663","0","1","<opencv><intel><inference-engine><openvino>"
"59372361","Downloading pre-trained models from OpenVINO™ Toolkit Pre-Trained Models by Ubuntu Terminal","2019-12-17 10:34:51","<p>I am trying to use some pre-trained model from the intel Pretrained model zoo. Here is the address of that site <a href=""https://docs.openvinotoolkit.org/latest/_models_intel_index.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_models_intel_index.html</a>. Is there any specific command for downloading these models in a Linux system. </p>
","<p>As mentioned in the following url:-<a href=""https://docs.openvinotoolkit.org/latest/_models_intel_index.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_models_intel_index.html</a>, you can download the pretrained models using Model Downloader.(/deployment_tools/open_model_zoo/tools/downloader)</p>

<p>More details about model downloader can be found from the following url:
<a href=""https://docs.openvinotoolkit.org/latest/_tools_downloader_README.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_tools_downloader_README.html</a></p>
","1530","1","2","<linux><openvino>"
"59372361","Downloading pre-trained models from OpenVINO™ Toolkit Pre-Trained Models by Ubuntu Terminal","2019-12-17 10:34:51","<p>I am trying to use some pre-trained model from the intel Pretrained model zoo. Here is the address of that site <a href=""https://docs.openvinotoolkit.org/latest/_models_intel_index.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_models_intel_index.html</a>. Is there any specific command for downloading these models in a Linux system. </p>
","<p>downloader.py (model downloader) downloads model files from online sources and, if necessary, patches them to make them more usable with Model Optimizer;</p>

<p><strong>USAGE</strong>
The basic usage is to run the script like this:</p>

<pre><code>./downloader.py --all
</code></pre>

<p>This will download all models into a directory tree rooted in the current directory. To download into a different directory, use the -o/--output_dir option:</p>

<pre><code>./downloader.py --all --output_dir my/download/directory
</code></pre>

<p>The --all option can be replaced with other filter options to download only a subset of models. See the ""Shared options"" section.</p>

<p>You may use --precisions flag to specify comma separated precisions of weights to be downloaded.</p>

<pre><code>./downloader.py --name face-detection-retail-0004 --precisions FP16,INT8
</code></pre>

<p>By default, the script will attempt to download each file only once. You can use the --num_attempts option to change that and increase the robustness of the download process:</p>

<pre><code>./downloader.py --all --num_attempts 5 # attempt each download five times
</code></pre>

<p>You can use the --cache_dir option to make the script use the specified directory as a cache. The script will place a copy of each downloaded file in the cache, or, if it is already there, retrieve it from the cache instead of downloading it again.</p>

<pre><code>./downloader.py --all --cache_dir my/cache/directory
</code></pre>

<p>The cache format is intended to remain compatible in future Open Model Zoo versions, so you can use a cache to avoid redownloading most files when updating Open Model Zoo.</p>

<p>By default, the script outputs progress information as unstructured, human-readable text. If you want to consume progress information programmatically, use the --progress_format option:</p>

<pre><code>./downloader.py --all --progress_format=json
</code></pre>

<p>When this option is set to json, the script's standard output is replaced by a machine-readable progress report, whose format is documented in the ""JSON progress report format"" section. This option does not affect errors and warnings, which will still be printed to the standard error stream in a human-readable format.</p>

<p>You can also set this option to text to explicitly request the default text format.</p>

<p>See the ""Shared options"" section for information on other options accepted by the script.</p>

<p>More details about model downloader can be found from the following url: <a href=""https://docs.openvinotoolkit.org/latest/_tools_downloader_README.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_tools_downloader_README.html</a></p>
","1530","1","2","<linux><openvino>"
"54563695","Mask RCNN OpenVino - C++ API","2019-02-06 22:43:30","<p>I would like to implement a custom image classifier using MaskRCNN.</p>
<p>In order to increase the speed of the network, i would like to optimise the inference.</p>
<p>I already used OpenCV DNN library, but i would like to do a step forward with OpenVINO.</p>
<p>I used successfully OpenVINO Model optimiser (python), to build the .xml and .bin file representing my network.</p>
<p>I successfully builded OpenVINO Sample directory with Visual Studio 2017 and run MaskRCNNDemo project.</p>
<pre><code>mask_rcnn_demo.exe -m .\Release\frozen_inference_graph.xml -i .\Release\input.jpg

InferenceEngine:
        API version ............ 1.4
        Build .................. 19154
[ INFO ] Parsing input parameters
[ INFO ] Files were added: 1
[ INFO ]     .\Release\input.jpg
[ INFO ] Loading plugin

        API version ............ 1.5
        Build .................. win_20181005
        Description ....... MKLDNNPlugin
[ INFO ] Loading network files
[ INFO ] Preparing input blobs
[ WARNING ] Image is resized from (4288, 2848) to (800, 800)
[ INFO ] Batch size is 1
[ INFO ] Preparing output blobs
[ INFO ] Loading model to the plugin
[ INFO ] Start inference (1 iterations)

Average running time of one iteration: 2593.81 ms

[ INFO ] Processing output blobs
[ INFO ] Detected class 16 with probability 0.986519: [2043.3, 1104.9], [2412.87, 1436.52]
[ INFO ] Image out.png created!
[ INFO ] Execution successful
</code></pre>
<p><a href=""https://i.stack.imgur.com/A7mzn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A7mzn.jpg"" alt=""Oiseau VINO CPP"" /></a></p>
<p>Then i tried to reproduce this project in a separate project...
First i had to watch dependancies...</p>
<pre><code>&lt;MaskRCNNDemo&gt;
     //References
     &lt;format_reader/&gt;    =&gt; Open CV Images, resize it and get uchar data
     &lt;ie_cpu_extension/&gt; =&gt; CPU extension for un-managed layers (?)

     //Linker
     format_reader.lib         =&gt; Format Reader Lib (VINO Samples Compiled)
     cpu_extension.lib         =&gt; CPU extension Lib (VINO Samples Compiled)
     inference_engined.lib     =&gt; Inference Engine lib (VINO)
     opencv_world401d.lib      =&gt; OpenCV Lib
     libiomp5md.lib            =&gt; Dependancy
     ... (other libs)
</code></pre>
<p>With it i've build a new project, with my own classes and way to open images (multiframe tiff).
This work without problem then i will not describe (i use with a CV DNN inference engine without problem).</p>
<p>I wanted to build the same project than MaskRCNNDemo : CustomIA</p>
<pre><code>&lt;CustomIA&gt;
     //References
     None =&gt; I use my own libtiff way to open image and i resize with OpenCV
     None =&gt; I will just add include to cpu_extension source code.

     //Linker
     opencv_world345d.lib   =&gt; OpenCV 3.4.5 library
     tiffd.lib              =&gt; Libtiff Library
     cpu_extension.lib      =&gt; CPU extension compiled with sample
     inference_engined.lib  =&gt; Inference engine lib.
</code></pre>
<p>I added the following dll to the project target dir :</p>
<pre><code>cpu_extension.dll
inference_engined.dll
libiomp5md.dll
mkl_tiny_omp.dll
MKLDNNPlugind.dll
opencv_world345d.dll
tiffd.dll
tiffxxd.dll
</code></pre>
<p>I successfully compiled and execute but i faced two issues :</p>
<p>OLD CODE:</p>
<pre><code> slog::info &lt;&lt; &quot;Loading plugin&quot; &lt;&lt; slog::endl;
    InferencePlugin plugin = PluginDispatcher({ FLAGS_pp, &quot;../../../lib/intel64&quot; , &quot;&quot; }).getPluginByDevice(FLAGS_d);

    /** Loading default extensions **/
    if (FLAGS_d.find(&quot;CPU&quot;) != std::string::npos) {
        /**
         * cpu_extensions library is compiled from &quot;extension&quot; folder containing
         * custom MKLDNNPlugin layer implementations. These layers are not supported
         * by mkldnn, but they can be useful for inferring custom topologies.
        **/
        plugin.AddExtension(std::make_shared&lt;Extensions::Cpu::CpuExtensions&gt;());
    }
    /** Printing plugin version **/
    printPluginVersion(plugin, std::cout);
</code></pre>
<p><strong>OUTPUT :</strong></p>
<pre><code>[ INFO ] Loading plugin
    API version ............ 1.5
    Build .................. win_20181005
    Description ....... MKLDNNPlugin
</code></pre>
<p>NEW CODE:</p>
<pre><code>    VINOEngine::VINOEngine()
{
    // Loading Plugin
    std::cout &lt;&lt; std::endl;
    std::cout &lt;&lt; &quot;[INFO] - Loading VINO Plugin...&quot; &lt;&lt; std::endl;
    this-&gt;plugin= PluginDispatcher({ &quot;&quot;, &quot;../../../lib/intel64&quot; , &quot;&quot; }).getPluginByDevice(&quot;CPU&quot;);
    this-&gt;plugin.AddExtension(std::make_shared&lt;Extensions::Cpu::CpuExtensions&gt;());
    printPluginVersion(this-&gt;plugin, std::cout);
</code></pre>
<p><strong>OUTPUT :</strong></p>
<pre><code>[INFO] - Loading VINO Plugin...
000001A242280A18  // Like memory adress ???
</code></pre>
<p>Second Issue :</p>
<p>When i try to extract my ROI and masks from New Code, if i have a &quot;match&quot;, i always have :</p>
<ul>
<li>score =1.0</li>
<li>x1=x2=0.0</li>
<li>y1=y2=1.0</li>
</ul>
<p>But the mask looks well extracted...</p>
<p>New Code :</p>
<pre><code>        float score = box_info[2];
        if (score &gt; this-&gt;Conf_Threshold)
        {
            // On reconstruit les coordonnées de la box..
            float x1 = std::min(std::max(0.0f, box_info[3] * Image.cols), static_cast&lt;float&gt;(Image.cols));
            float y1 = std::min(std::max(0.0f, box_info[4] * Image.rows), static_cast&lt;float&gt;(Image.rows));
            float x2 = std::min(std::max(0.0f, box_info[5] * Image.cols), static_cast&lt;float&gt;(Image.cols));
            float y2 = std::min(std::max(0.0f, box_info[6] * Image.rows), static_cast&lt;float&gt;(Image.rows));
            int box_width = std::min(static_cast&lt;int&gt;(std::max(0.0f, x2 - x1)), Image.cols);
            int box_height = std::min(static_cast&lt;int&gt;(std::max(0.0f, y2 - y1)), Image.rows);
</code></pre>
<p><a href=""https://i.stack.imgur.com/bWjSx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bWjSx.jpg"" alt=""Vino Mask"" /></a></p>
<pre><code>Image is resized from (4288, 2848) to (800, 800)
Detected class 62 with probability 1: [4288, 0], [4288, 0]
</code></pre>
<p>Then it is impossible for me to place the mask in the image and resize it while i don't have correct bbox coordinate...</p>
<p>Do anybody have an idea about what i make badly ?</p>
<p>How to create and link correctly an OpenVINO project using cpu_extension ?</p>
<p>Thanks !</p>
","<p>First issue with version: look above printPluginVersion function, you will see overloaded std::ostream operators for InferenceEngine and plugin version info.</p>

<p>Second: You can try to debug your model by comparing output after very first convolution and output layer for original framework and OV. Make sure it's equal element by element.</p>

<p>In OV you can use network.addOutput(""layer_name"") to add any layer to output. Then read output by using: const Blob::Ptr debug_blob = infer_request.GetBlob(""layer_name"").</p>

<p>Most of the time with issues like this i finding missing of input pre-processing (mean, normalization, etc.)</p>

<p>cpu_extensions is a dynamic library, but you still can change cmake script to make it static and link it with your application. After that you would need to use your application path with call to IExtensionPtr extension_ptr = make_so_pointer(argv[0])</p>
","1365","0","1","<c++><opencv><inference-engine><openvino>"
"56688591","How to add OpenVino setupvars.sh into PyCharm project?","2019-06-20 14:49:08","<p>I would like to add OpenVino setupvars.sh into a PyCharm project?</p>

<p>I'm working on a project usig OpenVino and in order to use OpenVino right now I am using following bash script in .bashrc </p>

<pre><code>source /opt/intel/openvino/bin/setupvars.sh

</code></pre>

<p>However, I would like to work on the project using PyCharm which make easier to handle the project as it gets bigger.</p>

<p>How we can use OpenVino in a PyCharm project.</p>
","<p>You can set this by pointing to the correct Python Interpreter in PyCharm. To get the current working Python environment, type the following lines of code in your local system.</p>

<blockquote>
  <p>python</p>
</blockquote>

<pre><code>&gt;&gt;&gt;import sys
&gt;&gt;&gt; sys.executable
&lt;PYTHON PATH&gt;
</code></pre>

<p>Make a note of this Python path and add it to File -> Settings -> Python Interpreter (In pycharm). </p>

<p>Hope this helps!</p>
","1326","3","4","<pycharm><openvino>"
"56688591","How to add OpenVino setupvars.sh into PyCharm project?","2019-06-20 14:49:08","<p>I would like to add OpenVino setupvars.sh into a PyCharm project?</p>

<p>I'm working on a project usig OpenVino and in order to use OpenVino right now I am using following bash script in .bashrc </p>

<pre><code>source /opt/intel/openvino/bin/setupvars.sh

</code></pre>

<p>However, I would like to work on the project using PyCharm which make easier to handle the project as it gets bigger.</p>

<p>How we can use OpenVino in a PyCharm project.</p>
","<p>Work for me on Windows 10:
Run CMD, then:</p>
<blockquote>
<p>&quot;\Program Files (x86)\IntelSWTools\openvino\bin\setupvars.bat&quot;</p>
<p>&quot;C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\bin\pycharm64.exe&quot;</p>
</blockquote>
<p>On macOS work too. Dont know about linux.</p>
","1326","3","4","<pycharm><openvino>"
"56688591","How to add OpenVino setupvars.sh into PyCharm project?","2019-06-20 14:49:08","<p>I would like to add OpenVino setupvars.sh into a PyCharm project?</p>

<p>I'm working on a project usig OpenVino and in order to use OpenVino right now I am using following bash script in .bashrc </p>

<pre><code>source /opt/intel/openvino/bin/setupvars.sh

</code></pre>

<p>However, I would like to work on the project using PyCharm which make easier to handle the project as it gets bigger.</p>

<p>How we can use OpenVino in a PyCharm project.</p>
","<p>Here is a solution for Ubuntu</p>

<p>Add the setupvars.sh in bashrc file.
Open terminal and write </p>

<pre><code>gedit ~/.bashrc
</code></pre>

<p>Add these lines to the end of the file </p>

<pre><code># &lt;&lt;&lt; Initialize Intel Vino environment variables &lt;&lt;&lt;
source /opt/intel/openvino/bin/setupvars.sh
# &lt;&lt;&lt; Initialize Intel Vino environment variables &lt;&lt;&lt;
</code></pre>

<p>Save and open new terminal, now the current terminal has loaded the environment variables. You can verify that if you see this line in the terminal </p>

<pre><code>[setupvars.sh] OpenVINO environment initialized
</code></pre>

<p>Open pycharm from the terminal by executing </p>

<pre><code>pycharm-community
</code></pre>
","1326","3","4","<pycharm><openvino>"
"56688591","How to add OpenVino setupvars.sh into PyCharm project?","2019-06-20 14:49:08","<p>I would like to add OpenVino setupvars.sh into a PyCharm project?</p>

<p>I'm working on a project usig OpenVino and in order to use OpenVino right now I am using following bash script in .bashrc </p>

<pre><code>source /opt/intel/openvino/bin/setupvars.sh

</code></pre>

<p>However, I would like to work on the project using PyCharm which make easier to handle the project as it gets bigger.</p>

<p>How we can use OpenVino in a PyCharm project.</p>
","<p>I think we have met the same question.</p>
<p>Here is my solution. Hope can help you :)</p>
<h2>Make Your Own Interpreter File</h2>
<ol>
<li><p>Make a new file such as python-custom.</p>
<pre><code> touch python-custom
</code></pre>
</li>
<li><p>Open the file you just made.</p>
<pre><code> vim python-custom
</code></pre>
</li>
<li><p>Save the following code into the file.</p>
<pre><code> #!/bin/bash

 args=$@

 # needed environment path
 . /opt/intel/openvino/bin/setupvars.sh

 # Path of your normal python path
 # You can find it by &quot;which python&quot;, &quot;which python3&quot; or any python you used
 /usr/bin/python3 $args
</code></pre>
</li>
<li><p>Add executable permissions for it.</p>
<pre><code> chmod +x python-custom
</code></pre>
</li>
<li><p>Add a new interpreter in Pycharm.</p>
<p>File -- Settings -- Project Interpreter</p>
</li>
<li><p>When you choose the interpreter, use &quot;/path/to/python-custom&quot; instead of &quot;/usr/bin/python3&quot;.</p>
</li>
</ol>
","1326","3","4","<pycharm><openvino>"
"60414356","PyInstaller: ModuleNotFoundError: No module named 'encodings'","2020-02-26 12:49:08","<p>I have a GUI application made using PySide2 and it some major modules it uses are OpenVino(2019), dlib, OpenCV-contrib(4.2.x) and Postgres(psycopg2) and I am trying to freeze the application using PyInstaller (--debug is True).</p>

<p>The program gets frozen without errors but during execution, I get the following error:</p>

<pre><code>Fatal Python error: initfsencoding: unable to load the file system codec
ModuleNotFoundError: No module named 'encodings'
</code></pre>

<p>after which the application exits.</p>

<p>I have tried many suggestions provided in other stackoverflow questions/github issues but none of them have worked.</p>

<p>I have python version 3.7.6 but I have also tried with 3.6.8 (both local installation and after creating new venv in pycharm). I have tried different versions of pycharm as well(it shows som other errors below 3.5). I have tried pycharm 3.6 both develop branch and master branch.</p>

<p>I have checked my PYTHONPATH and PYTHONHOME in env variables, they are pointing to python's location.</p>

<p>I have modified my specfile to include the necessary binaries, files, imports and folders. I would share it if needed. Also any other logs during build or execution.</p>

<p>I would like to know what I should do to solve this, wheather this issue is because of some component or is this a PyInstaller issue, and if so, should I raise it on github.</p>

<p>My os is windows 10.</p>
","<p>You changed the python version. So, you have to give a new path according to the Python version.
Just remove all older version and the current one and reinstall new Python v.3.8.1</p>
","1255","0","2","<python><python-3.x><opencv><pyinstaller><openvino>"
"60414356","PyInstaller: ModuleNotFoundError: No module named 'encodings'","2020-02-26 12:49:08","<p>I have a GUI application made using PySide2 and it some major modules it uses are OpenVino(2019), dlib, OpenCV-contrib(4.2.x) and Postgres(psycopg2) and I am trying to freeze the application using PyInstaller (--debug is True).</p>

<p>The program gets frozen without errors but during execution, I get the following error:</p>

<pre><code>Fatal Python error: initfsencoding: unable to load the file system codec
ModuleNotFoundError: No module named 'encodings'
</code></pre>

<p>after which the application exits.</p>

<p>I have tried many suggestions provided in other stackoverflow questions/github issues but none of them have worked.</p>

<p>I have python version 3.7.6 but I have also tried with 3.6.8 (both local installation and after creating new venv in pycharm). I have tried different versions of pycharm as well(it shows som other errors below 3.5). I have tried pycharm 3.6 both develop branch and master branch.</p>

<p>I have checked my PYTHONPATH and PYTHONHOME in env variables, they are pointing to python's location.</p>

<p>I have modified my specfile to include the necessary binaries, files, imports and folders. I would share it if needed. Also any other logs during build or execution.</p>

<p>I would like to know what I should do to solve this, wheather this issue is because of some component or is this a PyInstaller issue, and if so, should I raise it on github.</p>

<p>My os is windows 10.</p>
","<p>You need to include base_library.zip in your application folder</p>
","1255","0","2","<python><python-3.x><opencv><pyinstaller><openvino>"
"56643774","How do I do async inference on OpenVino","2019-06-18 07:25:06","<p>I wrote a python server that uses an OpenVino network to run inference on incoming requests. In order to speed things up, I receive requests in multiple threads, and I would like to run the inferences concurrently.
It seems that whatever I do, the times I get are the same as non-concurrent solutions - which makes me think I've missed something.</p>

<p>I'm writing it in Python, using openvino 2019.1.144. I'm using multiple requests to the same plugin and network in order to try to make the inferences run concurrently.</p>

<pre class=""lang-py prettyprint-override""><code>def __init__(self, num_of_requests: int = 4):
   self._plugin = IEPlugin(""CPU"", plugin_dirs=None)
   model_path = './Det/'
   model_xml = os.path.join(model_path, ""ssh_graph.xml"")
   model_bin = os.path.join(model_path, ""ssh_graph.bin"")
   net = IENetwork(model=model_xml, weights=model_bin)
   self._input_blob = next(iter(net.inputs))

   # Load network to the plugin
   self._exec_net = self._plugin.load(network=net, num_requests=num_of_requests)
   del net

def _async_runner(detect, images_subset, idx):
    for img in images_subset:
        request_handle = self._exec_net.start_async(request_id=idx, inputs={self._input_blob: img})
        request_handle.wait()


def run_async(images):  # These are the images to infer
    det = Detector(num_of_requests=4)
    multiplier = int(len(images)/4)
    with ThreadPoolExecutor(4) as pool:
        futures = []
        for idx in range(0,3):
            images_subset = images[idx*multiplier:(idx+1)*multiplier-1]
            futures.append(pool.submit(_async_runner, det.detect, images_subset, idx))
</code></pre>

<p>When I run 800 inferences in sync mode, I get an avg. run time of 290ms
When I run in async mode I get avg run time of 280ms.
These are not substantial improvements. What am I doing wrong?</p>
","<p>You can refer to a sample code from C:\Program Files (x86)\IntelSWTools\openvino_2019.1.144\inference_engine\samples\python_samples\object_detection_demo_ssd_async\object_detection_demo_ssd_async.py or similar samples from the python_samples directory to check the way async mode is addressed.</p>
","1233","0","2","<python-multithreading><openvino>"
"56643774","How do I do async inference on OpenVino","2019-06-18 07:25:06","<p>I wrote a python server that uses an OpenVino network to run inference on incoming requests. In order to speed things up, I receive requests in multiple threads, and I would like to run the inferences concurrently.
It seems that whatever I do, the times I get are the same as non-concurrent solutions - which makes me think I've missed something.</p>

<p>I'm writing it in Python, using openvino 2019.1.144. I'm using multiple requests to the same plugin and network in order to try to make the inferences run concurrently.</p>

<pre class=""lang-py prettyprint-override""><code>def __init__(self, num_of_requests: int = 4):
   self._plugin = IEPlugin(""CPU"", plugin_dirs=None)
   model_path = './Det/'
   model_xml = os.path.join(model_path, ""ssh_graph.xml"")
   model_bin = os.path.join(model_path, ""ssh_graph.bin"")
   net = IENetwork(model=model_xml, weights=model_bin)
   self._input_blob = next(iter(net.inputs))

   # Load network to the plugin
   self._exec_net = self._plugin.load(network=net, num_requests=num_of_requests)
   del net

def _async_runner(detect, images_subset, idx):
    for img in images_subset:
        request_handle = self._exec_net.start_async(request_id=idx, inputs={self._input_blob: img})
        request_handle.wait()


def run_async(images):  # These are the images to infer
    det = Detector(num_of_requests=4)
    multiplier = int(len(images)/4)
    with ThreadPoolExecutor(4) as pool:
        futures = []
        for idx in range(0,3):
            images_subset = images[idx*multiplier:(idx+1)*multiplier-1]
            futures.append(pool.submit(_async_runner, det.detect, images_subset, idx))
</code></pre>

<p>When I run 800 inferences in sync mode, I get an avg. run time of 290ms
When I run in async mode I get avg run time of 280ms.
These are not substantial improvements. What am I doing wrong?</p>
","<p>If you use wait(), the execution thread blocks until the result is available. If you want to use a truly async mode, you will need wait(0) which does not block the execution. Just launch the inference whenever you need and store the request_id. Then, you can check if the results are available checking if the returned value of wait(0) is 0. Be careful not to use the same request_id while the IE is doing the inference, that will cause a collision and it'll raise an exception.</p>

<p>However, in the code you provided, you cannot do this, because you are creating a thread pool in wich each thread executes inference of the image subset into a unique request_id. In fact, this is a parallel execution wich will give you a pretty fine performance, but it isn't ""async"" mode.</p>

<p>A truly async mode would be something like this:</p>

<pre><code>while still_items_to_infer():
    get_item_to_infer()
    get_unused_request_id()
    launch_infer()
    do_someting()
    if results_available():
        get_inference_results()
        free_request_id()
        #This may be in a new thread
        process_inference_results()
</code></pre>

<p>This way, you are dispatching continuous inferences while waiting for them to finish.</p>
","1233","0","2","<python-multithreading><openvino>"
"58133127","OpenVINO Convert TF Model to IR file Issue","2019-09-27 10:55:54","<p>I'm trying to convert tensorflow model to OpenVINO IR files.
I have downloaded a pre-trained model from the following address:</p>

<blockquote>
  <p><a href=""http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz"" rel=""nofollow noreferrer"">http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz</a></p>
</blockquote>

<p>Then I extracted the file to get a .pb file named ""frozen_inference_graph.pb""
Then I used the conversion command in the OpenVINO folder </p>

<blockquote>
  <p>""IntelSWTools\openvino_2019.2.275\deployment_tools\model_optimizer\""</p>
</blockquote>

<p>as following:</p>

<pre><code>python mo_tf.py --input_model frozen_inference_graph.pb
</code></pre>

<p>but I got following error message.
How can I modify anything to solve this issue?</p>

<pre><code>Model Optimizer arguments:
Common parameters:
    - Path to the Input Model:  &lt;my folder&gt;\frozen_inference_graph.pb
    - Path for generated IR:    &lt;my OpenVINO folder&gt;\IntelSWTools\openvino_2019.2.275\deployment_tools\model_optimizer\.
- IR output name:   frozen_inference_graph
- Log level:    ERROR
- Batch:    Not specified, inherited from the model
- Input layers:     Not specified, inherited from the model
- Output layers:    Not specified, inherited from the model
- Input shapes:     Not specified, inherited from the model
- Mean values:  Not specified
- Scale values:     Not specified
- Scale factor:     Not specified
- Precision of IR:  FP32
- Enable fusing:    True
- Enable grouped convolutions fusing:   True
- Move mean values to preprocess section:   False
- Reverse input channels:   False
TensorFlow specific parameters:
- Input model in text protobuf format:  False
- Path to model dump for TensorBoard:   None
- List of shared libraries with TensorFlow custom layers implementation:    None
- Update the configuration file with input/output node names:   None
- Use configuration file used to generate the model with Object Detection API:  None
- Operations to offload:    None
- Patterns to offload:  None
- Use the config file:  None
Model Optimizer version:    2019.2.0-436-gf5827d4

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\framework\dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_qint8 = np.dtype([(""qint8"", np.int8, 1)])

[ ERROR ]  Shape [-1 -1 -1  3] is not fully defined for output 0 of ""image_tensor"". Use --input_shape with positive integers to override model input shapes.
[ ERROR ]  Cannot infer shapes or values for node ""image_tensor"".
[ ERROR ]  Not all output shapes were inferred or fully defined for node ""image_tensor"".  For more information please refer to Model Optimizer FAQ (https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html), question #40. 
[ ERROR ]  
[ ERROR ]  It can happen due to bug in custom shape infer function &lt;function Parameter.__init__.&lt;locals&gt;.&lt;lambda&gt; at 0x000002032A17D378&gt;.
[ ERROR ]  Or because the node inputs have incorrect values/shapes.
[ ERROR ]  Or because input shapes are incorrect (embedded to the model or passed via --input_shape).
[ ERROR ]  Run Model Optimizer with --log_level=DEBUG for more information.
[ ERROR ]  Exception occurred during running replacer ""REPLACEMENT_ID"" (&lt;class 'extensions.middle.PartialInfer.PartialInfer'&gt;): Stopped shape/value propagation at ""image_tensor"" node. 
For more information please refer to Model Optimizer FAQ (https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html), question #38. 

Process finished with exit code 1
</code></pre>

<p>I have tried many other tensorflow models but all have the same issue.
I used different tensorflow version from <strong>1.2.0</strong> to <strong>1.14.0</strong> but the same.</p>

<p>The key word about the shape seems to be the main cause. but how can I add something to avoid this issue?</p>

<pre><code>Shape [-1 -1 -1  3] is not fully defined for output 0 of ""image_tensor"". Use --input_shape with positive integers to override model input shapes.
</code></pre>

<p>I hope the IR file can be generated correctly.</p>
","<p>Openvino model optimizer (mo_tf.py) expects more arguments. Please pass the below as well.</p>

<p>python mo_tf.py --output_dir &lt;\PATH> --input_model &lt;\PATH>\mask_rcnn_inception_v2_coco_2018_01_28\frozen_inference_graph.pb  --tensorflow_use_custom_operations_config extensions\front\tf\mask_rcnn_support.json --tensorflow_object_detection_api_pipeline_config &lt;\PATH>\mask_rcnn_inception_v2_coco_2018_01_28\pipeline.config </p>

<p>mask_rcnn_inception_v2_coco model can be downloaded from <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md</a></p>

<p>For more details refer : <a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html</a></p>
","1173","4","1","<openvino>"
"54478463","How to permanently set the environment variables for OpenVino","2019-02-01 11:20:12","<p>I am setting up OpenVino on my system and I get this form the <a href=""https://software.intel.com/en-us/articles/OpenVINO-Install-Windows#next-steps"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>(Optional): OpenVINO toolkit environment variables are removed when you close the Command Prompt window. As an option, you can permanently set the environment variables manually.</p>
</blockquote>

<p>But there is no information is available on what are the required environment variables and what value they should be set. </p>

<p>I need to know the list of environmental variable needed by OpenVino and the value that they should be set to. I know how to set them in Windows (using GUI or Setx).</p>
","<p>The environment variables need to be set are given in the setupvars.bat file present at ""path_to_computer_vision_sdk_directory\bin"". </p>

<p>But I will give a general idea on the paths that need to be set - 
Set the following <strong>System variables</strong> in your Environment Variables</p>

<ol>
<li><p>Variable name: <em>INTEL_CVSDK_DIR</em>
Variable value: <em>path_to_computer_vision_sdk_directory</em> i.e. 
<em>C:\Intel\computer_vision_sdk_version_number</em>, in case you have the cvsdk setup at the default path for installation</p></li>
<li><p>Variable name: <em>OpenCV_DIR</em>
Variable value: <em>%INTEL_CVSDK_DIR%\opencv\cmake</em></p></li>
<li><p>Variable name: <em>OPENVX_FOLDER</em> 
Variable value: <em>%INTEL_CVSDK_DIR%\openvx</em></p></li>
<li><p>Variable name: <em>InferenceEngine_DIR</em>
Variable value: <em>%INTEL_CVSDK_DIR%\deployment_tools\inference_engine\share</em></p></li>
<li><p>Variable name: <em>HDDL_INSTALL_DIR</em>
Variable value: <em>%INTEL_CVSDK_DIR%\deployment_tools\inference_engine\external\hddl</em></p></li>
<li><p>Now edit the ""<em>Path</em>"" variable under System Variables to include the following values - 
<em>%INTEL_CVSDK_DIR%\opencv\x64\vc14\bin
%INTEL_CVSDK_DIR%\openvx\bin
%INTEL_CVSDK_DIR%\deployment_tools\inference_engine\bin\intel64\Release
%INTEL_CVSDK_DIR%\deployment_tools\inference_engine\bin\intel64\Debug
%HDDL_INSTALL_DIR%\bin</em></p></li>
</ol>

<p>You also nee to set the ""<em>PYTHONPATH</em>"" to <em>%INTEL_CVSDK_DIR%\python\python_version%</em></p>
","1161","1","1","<openvino>"
"62277098","Use openvino from docker","2020-06-09 07:19:22","<p>I am trying to use OpenVINO from docker container. 
I use docker file from official web site <a href=""https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_docker_linux.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_docker_linux.html</a></p>

<pre><code>FROM ubuntu:18.04
USER root
WORKDIR /
SHELL [""/bin/bash"", ""-xo"", ""pipefail"", ""-c""]
# Creating user openvino
RUN useradd -ms /bin/bash openvino &amp;&amp; \
    chown openvino -R /home/openvino
ARG DEPENDENCIES=""autoconf \
                  automake \
                  build-essential \
                  cmake \
                  cpio \
                  curl \
                  gnupg2 \
                  libdrm2 \
                  libglib2.0-0 \
                  lsb-release \
                  libgtk-3-0 \
                  libtool \
                  udev \
                  unzip \
                  dos2unix""
RUN apt-get update &amp;&amp; \
    apt-get install -y --no-install-recommends ${DEPENDENCIES} &amp;&amp; \
    rm -rf /var/lib/apt/lists/*
WORKDIR /thirdparty
RUN sed -Ei 's/# deb-src /deb-src /' /etc/apt/sources.list &amp;&amp; \
    apt-get update &amp;&amp; \
    apt-get source ${DEPENDENCIES} &amp;&amp; \
    rm -rf /var/lib/apt/lists/*
# setup Python
ENV PYTHON python3.6
RUN apt-get update &amp;&amp; \
    apt-get install -y --no-install-recommends python3-pip python3-dev lib${PYTHON}=3.6.9-1~18.04 &amp;&amp; \
    rm -rf /var/lib/apt/lists/*
ARG package_url=ARG package_url=http://registrationcenter-download.intel.com/akdlm/irc_nas/16612/l_openvino_toolkit_p_2020.2.120.tgz
ARG TEMP_DIR=/tmp/openvino_installer
WORKDIR ${TEMP_DIR}
ADD ${package_url} ${TEMP_DIR}
# install product by installation script
ENV INTEL_OPENVINO_DIR /opt/intel/openvino
RUN tar -xzf ${TEMP_DIR}/*.tgz --strip 1
RUN sed -i 's/decline/accept/g' silent.cfg &amp;&amp; \
    ${TEMP_DIR}/install.sh -s silent.cfg &amp;&amp; \
    ${INTEL_OPENVINO_DIR}/install_dependencies/install_openvino_dependencies.sh
WORKDIR /tmp
RUN rm -rf ${TEMP_DIR}
# installing dependencies for package
WORKDIR /tmp
RUN ${PYTHON} -m pip install --no-cache-dir setuptools &amp;&amp; \
    find ""${INTEL_OPENVINO_DIR}/"" -type f -name ""*requirements*.*"" -path ""*/${PYTHON}/*"" -exec ${PYTHON} -m pip install --no-cache-dir -r ""{}"" \; &amp;&amp; \
    find ""${INTEL_OPENVINO_DIR}/"" -type f -name ""*requirements*.*"" -not -path ""*/post_training_optimization_toolkit/*"" -not -name ""*windows.txt""  -not -name ""*ubuntu16.txt"" -not -path ""*/python3*/*"" -not -path ""*/python2*/*"" -exec ${PYTHON} -m pip install --no-cache-dir -r ""{}"" \;
WORKDIR ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/accuracy_checker
RUN source ${INTEL_OPENVINO_DIR}/bin/setupvars.sh &amp;&amp; \
    ${PYTHON} -m pip install --no-cache-dir -r ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/accuracy_checker/requirements.in &amp;&amp; \
    ${PYTHON} ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/accuracy_checker/setup.py install
WORKDIR ${INTEL_OPENVINO_DIR}/deployment_tools/tools/post_training_optimization_toolkit
RUN if [ -f requirements.txt ]; then \
        ${PYTHON} -m pip install --no-cache-dir -r ${INTEL_OPENVINO_DIR}/deployment_tools/tools/post_training_optimization_toolkit/requirements.txt &amp;&amp; \
        ${PYTHON} ${INTEL_OPENVINO_DIR}/deployment_tools/tools/post_training_optimization_toolkit/setup.py install; \
    fi;
# Post-installation cleanup and setting up OpenVINO environment variables
RUN if [ -f ""${INTEL_OPENVINO_DIR}""/bin/setupvars.sh ]; then \
        printf ""\nsource \${INTEL_OPENVINO_DIR}/bin/setupvars.sh\n"" &gt;&gt; /home/openvino/.bashrc; \
        printf ""\nsource \${INTEL_OPENVINO_DIR}/bin/setupvars.sh\n"" &gt;&gt; /root/.bashrc; \
    fi;
RUN find ""${INTEL_OPENVINO_DIR}/"" -name ""*.*sh"" -type f -exec dos2unix {} \;
USER openvino
WORKDIR ${INTEL_OPENVINO_DIR}
CMD [""/bin/bash""]
</code></pre>

<p>But if I try to launch python program with this line:</p>

<pre><code>from openvino.inference_engine import IECore
</code></pre>

<p>It writes ModuleNotFoundError, No module named 'openvino'. I tried to source file setupvars.sh with these commands, but it doesn't help. </p>

<pre><code>RUN /bin/bash -c ""source /opt/intel/openvino/bin/setupvars.sh""
RUN source /opt/intel/openvino/bin/setupvars.sh
</code></pre>

<p>What should I do to use openvino python apps from docker? </p>
","<p>If you are building the Docker image, and trying to run the OpenVINO Python apps outside the docker image it won't work. You can create the Docker image and run Docker image interactively to execute the Python apps within the image. Refer to <a href=""https://docs.docker.com/engine/reference/run/"" rel=""noreferrer"">https://docs.docker.com/engine/reference/run/</a> for more information on <code>docker run</code>.
<br/><br/>
Couple of issues noticed in your Dockerfile when used in my environment. After the changes and steps below, you should be able to import <em>openvino</em> module and run a Python application:
<br/><br/>
In Dockerfile line #34:</p>

<pre><code>// Before:
apt-get install -y --no-install-recommends python3-pip python3-dev lib${PYTHON}=3.6.9-1~18.04 &amp;&amp; \
// After:
apt-get install -y --no-install-recommends python3-pip python3-dev lib${PYTHON} &amp;&amp; \
</code></pre>

<p><br/>
In Dockerfile line #36:</p>

<pre><code>// Before:    
ARG package_url=ARG package_url=http://registrationcenter-download.intel.com/akdlm/irc_nas/16612/l_openvino_toolkit_p_2020.2.120.tgz
// After:   
ARG package_url=http://registrationcenter-download.intel.com/akdlm/irc_nas/16612/l_openvino_toolkit_p_2020.2.120.tgz
</code></pre>

<p><br/>
After the changes above, run <code>docker build . -t &lt;image-name&gt;</code> (i.e. <em>docker build . -t openvino-ubuntu</em>) to build Docker image. If successful you would see <code>Successfully built bf2280a70ffd Successfully tagged openvino-&lt;image-name&gt;:latest</code>.
<br/><br/>
Then run image interactively with <code>docker run -it &lt;image_name&gt;</code> (i.e. <em>docker run -it openvino-ubuntu</em>). You would see something similar to:</p>

<pre><code>[setupvars.sh] OpenVINO environment initialized
openvino@ce618ea2bc47:/opt/intel/openvino_2020.2.120$
</code></pre>

<p><br/>
To verify Python is able to import <em>IECore</em> from <em>openvino</em> module, test with Python interpreter:</p>

<pre><code>openvino@ce618ea2bc47:/opt/intel/openvino_2020.2.120$ python3
Python 3.6.9 (default, Apr 18 2020, 01:56:04) 
[GCC 8.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from openvino.inference_engine import IECore
&gt;&gt;&gt;    
</code></pre>

<p><br/>
To run OpenVINO Python app, test one of the sample apps in <em>/opt/intel/openvino/deployment_tools/inference_engine/samples/python</em>, for example <em>object_detection_sample_ssd.py</em>:</p>

<pre><code>openvino@ce618ea2bc47:/opt/intel/openvino_2020.2.120$ python3 /opt/intel/openvino/deployment_tools/inference_engine/samples/python/object_detection_sample_ssd/object_detection_sample_ssd.py

usage: object_detection_sample_ssd.py [-h] -m MODEL -i INPUT [INPUT ...]
                                  [-l CPU_EXTENSION] [-d DEVICE]
                                  [--labels LABELS] [-nt NUMBER_TOP]
object_detection_sample_ssd.py: error: the following arguments are required: -m/--model, -i/--input
</code></pre>
","1148","1","1","<docker><openvino>"
"59725646","OpenVINO - Inference library plugin libMKLDNNPlugin.so cannot resolve dependency","2020-01-13 23:28:13","<p>I am experimenting with OpenVINO APIs and below is the sample code snippet:</p>

<pre><code>plugin = InferenceEngine::PluginDispatcher(&lt;params&gt;).getPluginByDevice(""CPU"");
</code></pre>

<p>However, I get the below error:</p>

<pre><code>Cannot find plugin to use :Tried load plugin : MKLDNNPlugin,  error: Plugin MKLDNNPlugin cannot be loaded: cannot load plugin: MKLDNNPlugin from /opt/intel/openvino_2019.3.376/inference_engine/lib/intel64: Cannot load library '/opt/intel/openvino_2019.3.376/inference_engine/lib/intel64/libMKLDNNPlugin.so': libmkl_tiny_tbb.so: cannot open shared object file: No such file or directory
</code></pre>

<p>I looked for the above missing library and it actually exists:</p>

<pre><code>$ ls /opt/intel/openvino_2019.3.376/inference_engine/external/mkltiny_lnx/lib/
libmkl_tiny_tbb.so
</code></pre>

<p>It looks like some internal dependency is not resolved by inference engine lib/plugin. Could anyone help figure out why it doesn't work? </p>
","<p>Add/Update this path</p>

<pre><code>/opt/intel/openvino_2019.3.376/inference_engine/external/mkltiny_lnx/lib/
libmkl_tiny_tbb.so
</code></pre>

<p>into </p>

<blockquote>
  <p>LD_LIBRARY_PATH</p>
</blockquote>
","1046","0","2","<c++><openvino>"
"59725646","OpenVINO - Inference library plugin libMKLDNNPlugin.so cannot resolve dependency","2020-01-13 23:28:13","<p>I am experimenting with OpenVINO APIs and below is the sample code snippet:</p>

<pre><code>plugin = InferenceEngine::PluginDispatcher(&lt;params&gt;).getPluginByDevice(""CPU"");
</code></pre>

<p>However, I get the below error:</p>

<pre><code>Cannot find plugin to use :Tried load plugin : MKLDNNPlugin,  error: Plugin MKLDNNPlugin cannot be loaded: cannot load plugin: MKLDNNPlugin from /opt/intel/openvino_2019.3.376/inference_engine/lib/intel64: Cannot load library '/opt/intel/openvino_2019.3.376/inference_engine/lib/intel64/libMKLDNNPlugin.so': libmkl_tiny_tbb.so: cannot open shared object file: No such file or directory
</code></pre>

<p>I looked for the above missing library and it actually exists:</p>

<pre><code>$ ls /opt/intel/openvino_2019.3.376/inference_engine/external/mkltiny_lnx/lib/
libmkl_tiny_tbb.so
</code></pre>

<p>It looks like some internal dependency is not resolved by inference engine lib/plugin. Could anyone help figure out why it doesn't work? </p>
","<p>Run a script <code>setupvars.sh</code> before you run your program. The script resolves all dependencies needed for running OpenVINO applications.</p>

<p>The script located in <code>&lt;openvino-install-dir&gt;/bin/setupvars.sh</code></p>
","1046","0","2","<c++><openvino>"
"65898876","VCRUNTIME140_1D error in debug mode with visual studio","2021-01-26 09:34:49","<p>Hello when im runing visual studio 2017 on debug mode I got this error VCRUNTIME140_1D.dll was not found, I tried to install again visual studio 17 and redistributed c++ 17 but nothing. I check system32 and i cant find this file also there</p>
","<p>It seems like you are missing the cvruntime140_1D.dll file from your computer. You have to reinstall redistributable c++ and you can find the dll in the system folder.</p>
","1030","1","2","<openvino>"
"65898876","VCRUNTIME140_1D error in debug mode with visual studio","2021-01-26 09:34:49","<p>Hello when im runing visual studio 2017 on debug mode I got this error VCRUNTIME140_1D.dll was not found, I tried to install again visual studio 17 and redistributed c++ 17 but nothing. I check system32 and i cant find this file also there</p>
","<p>Thanks for ur answer
I found the solution, the problem is that thi sdll vcruntime140_1d.dll is a special dll for VS19. u can find it in MVSC in install directory for your VS19. and is VS17 doesnt have the same version. sso install VS19 INSTEAD OF 17.</p>
","1030","1","2","<openvino>"
"57145603","tensorflow openvino ssd-mobilnet coco custom dataset error input layer","2019-07-22 12:00:19","<p>So, I'm using TensorFlow SSD-Mobilnet V1 coco dataset. That I have further trained on my own dataset but when I try to convert it to OpenVino IR to run it on Raspberry PI with Movidius Chip. I get an error</p>

<pre class=""lang-py prettyprint-override""><code>➜  utils sudo python3 summarize_graph.py --input_model ssd.pb 
WARNING: Logging before flag parsing goes to stderr.
W0722 17:17:05.565755 4678620608 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.
W0722 17:17:06.696880 4678620608 deprecation_wrapper.py:119] From ../../mo/front/tf/loader.py:35: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.

W0722 17:17:06.697348 4678620608 deprecation_wrapper.py:119] From ../../mo/front/tf/loader.py:109: The name tf.MetaGraphDef is deprecated. Please use tf.compat.v1.MetaGraphDef instead.

W0722 17:17:06.697680 4678620608 deprecation_wrapper.py:119] From ../../mo/front/tf/loader.py:235: The name tf.NodeDef is deprecated. Please use tf.compat.v1.NodeDef instead.

1 input(s) detected:
Name: image_tensor, type: uint8, shape: (-1,-1,-1,3)
7 output(s) detected:
detection_boxes
detection_scores
detection_multiclass_scores
detection_classes
num_detections
raw_detection_boxes
raw_detection_scores
</code></pre>

<p>When I try to convert the ssd.pb(frozen model) to OpenVino IR </p>

<pre class=""lang-py prettyprint-override""><code>➜  model_optimizer sudo python3 mo_tf.py --input_model ssd.pb          
Password:
Model Optimizer arguments:
Common parameters:
    - Path to the Input Model:  /opt/intel/openvino_2019.1.144/deployment_tools/model_optimizer/ssd.pb
    - Path for generated IR:    /opt/intel/openvino_2019.1.144/deployment_tools/model_optimizer/.
    - IR output name:   ssd
    - Log level:    ERROR
    - Batch:    Not specified, inherited from the model
    - Input layers:     Not specified, inherited from the model
    - Output layers:    Not specified, inherited from the model
    - Input shapes:     Not specified, inherited from the model
    - Mean values:  Not specified
    - Scale values:     Not specified
    - Scale factor:     Not specified
    - Precision of IR:  FP32
    - Enable fusing:    True
    - Enable grouped convolutions fusing:   True
    - Move mean values to preprocess section:   False
    - Reverse input channels:   False
TensorFlow specific parameters:
    - Input model in text protobuf format:  False
    - Path to model dump for TensorBoard:   None
    - List of shared libraries with TensorFlow custom layers implementation:    None
    - Update the configuration file with input/output node names:   None
    - Use configuration file used to generate the model with Object Detection API:  None
    - Operations to offload:    None
    - Patterns to offload:  None
    - Use the config file:  None
Model Optimizer version:    2019.1.1-83-g28dfbfd
WARNING: Logging before flag parsing goes to stderr.
E0722 17:24:22.964164 4474824128 infer.py:158] Shape [-1 -1 -1  3] is not fully defined for output 0 of ""image_tensor"". Use --input_shape with positive integers to override model input shapes.
E0722 17:24:22.964462 4474824128 infer.py:178] Cannot infer shapes or values for node ""image_tensor"".
E0722 17:24:22.964554 4474824128 infer.py:179] Not all output shapes were inferred or fully defined for node ""image_tensor"". 
 For more information please refer to Model Optimizer FAQ (&lt;INSTALL_DIR&gt;/deployment_tools/documentation/docs/MO_FAQ.html), question #40. 
E0722 17:24:22.964632 4474824128 infer.py:180] 
E0722 17:24:22.964720 4474824128 infer.py:181] It can happen due to bug in custom shape infer function &lt;function tf_placeholder_ext.&lt;locals&gt;.&lt;lambda&gt; at 0x12ab64bf8&gt;.
E0722 17:24:22.964787 4474824128 infer.py:182] Or because the node inputs have incorrect values/shapes.
E0722 17:24:22.964850 4474824128 infer.py:183] Or because input shapes are incorrect (embedded to the model or passed via --input_shape).
E0722 17:24:22.965915 4474824128 infer.py:192] Run Model Optimizer with --log_level=DEBUG for more information.
E0722 17:24:22.966033 4474824128 main.py:317] Exception occurred during running replacer ""REPLACEMENT_ID"" (&lt;class 'extensions.middle.PartialInfer.PartialInfer'&gt;): Stopped shape/value propagation at ""image_tensor"" node. 
 For more information please refer to Model Optimizer FAQ (&lt;INSTALL_DIR&gt;/deployment_tools/documentation/docs/MO_FAQ.html), question #38.
</code></pre>

<p>How do you think we should fix this?</p>
","<p>When you try to convert ssd.pb(your frozen model), you are passing only the input model parameter to mo_tf.py scripts. To convert an object detection model to IR, go
to the model optimizer directory, run the mo_tf.py script with the following required parameters:</p>

<p>--input_model  :<br>
              File with a pre-trained model (binary or text .pb file after freezing)</p>

<p>--tensorflow_use_custom_operations_config    : 
              Configuration file that describes rules to convert specific TensorFlow* topologies. 
              For the models downloaded from the TensorFlow* Object Detection API zoo, you can find the configuration files in the /deployment_tools/model_optimizer/extensions/front/tf directory
              You can use ssd_v2_support.json / ssd_support.json — for frozen SSD topologies from the models zoo. It will be available in the above mentioned directory.</p>

<p>--tensorflow_object_detection_api_pipeline_config  :
              A special configuration file that describes the topology hyper-parameters and structure of the TensorFlow Object Detection API model.
              For the models downloaded from the TensorFlow* Object Detection API zoo, the configuration file is named pipeline.config. 
              If you plan to train a model yourself, you can find templates for these files in the models repository</p>

<p>--input_shape(optional):
              A custom input image shape, we need to pass these values based on the pretrained model you used.
              The model takes input image in the format [1 H W C], Where the parameter refers to the batch size, height, width, channel respectively.
              Model Optimizer does not accept negative values for batch, height, width and channel number.
              So, you need to pass a valid set of 4 positive numbers using --input_shape parameter, if input image dimensions of the model(SSD mobilenet) is known in advance.<br>
              If it is not available, you don't need to pass input shape.</p>

<p>An example mo_tf.py command which uses the model SSD-MobileNet-v2-COCO downloaded from model downloader comes up with openvino is shown below.</p>

<pre><code>python mo_tf.py  
              --input_model ""c:\Program Files (x86)\IntelSWTools\openvino_2019.1.087\deployment_tools\tools\model_downloader\object_detection\common\ssd_mobilenet_v2_coco\tf\ssd_mobilenet_v2_coco.frozen.pb"" 
              --tensorflow_use_custom_operations_config  ""c:\Program Files (x86)\IntelSWTools\openvino_2019.1.087\deployment_tools\model_optimizer\extensions\front\tf\ssd_v2_support.json""  
              --tensorflow_object_detection_api_pipeline_config  ""c:\Program Files (x86)\IntelSWTools\openvino_2019.1.087\deployment_tools\tools\model_downloader\object_detection\common\ssd_mobilenet_v2_coco\tf\ssd_mobilenet_v2_coco.config"" 
              --data_type FP16 
              --log_level DEBUG
</code></pre>

<p>For more details, refer to the link  <a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Object_Detection_API_Models.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Object_Detection_API_Models.html</a><br>
Hope it helps.</p>
","981","0","3","<python-3.x><tensorflow><raspberry-pi><openvino><movidius>"
"57145603","tensorflow openvino ssd-mobilnet coco custom dataset error input layer","2019-07-22 12:00:19","<p>So, I'm using TensorFlow SSD-Mobilnet V1 coco dataset. That I have further trained on my own dataset but when I try to convert it to OpenVino IR to run it on Raspberry PI with Movidius Chip. I get an error</p>

<pre class=""lang-py prettyprint-override""><code>➜  utils sudo python3 summarize_graph.py --input_model ssd.pb 
WARNING: Logging before flag parsing goes to stderr.
W0722 17:17:05.565755 4678620608 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.
W0722 17:17:06.696880 4678620608 deprecation_wrapper.py:119] From ../../mo/front/tf/loader.py:35: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.

W0722 17:17:06.697348 4678620608 deprecation_wrapper.py:119] From ../../mo/front/tf/loader.py:109: The name tf.MetaGraphDef is deprecated. Please use tf.compat.v1.MetaGraphDef instead.

W0722 17:17:06.697680 4678620608 deprecation_wrapper.py:119] From ../../mo/front/tf/loader.py:235: The name tf.NodeDef is deprecated. Please use tf.compat.v1.NodeDef instead.

1 input(s) detected:
Name: image_tensor, type: uint8, shape: (-1,-1,-1,3)
7 output(s) detected:
detection_boxes
detection_scores
detection_multiclass_scores
detection_classes
num_detections
raw_detection_boxes
raw_detection_scores
</code></pre>

<p>When I try to convert the ssd.pb(frozen model) to OpenVino IR </p>

<pre class=""lang-py prettyprint-override""><code>➜  model_optimizer sudo python3 mo_tf.py --input_model ssd.pb          
Password:
Model Optimizer arguments:
Common parameters:
    - Path to the Input Model:  /opt/intel/openvino_2019.1.144/deployment_tools/model_optimizer/ssd.pb
    - Path for generated IR:    /opt/intel/openvino_2019.1.144/deployment_tools/model_optimizer/.
    - IR output name:   ssd
    - Log level:    ERROR
    - Batch:    Not specified, inherited from the model
    - Input layers:     Not specified, inherited from the model
    - Output layers:    Not specified, inherited from the model
    - Input shapes:     Not specified, inherited from the model
    - Mean values:  Not specified
    - Scale values:     Not specified
    - Scale factor:     Not specified
    - Precision of IR:  FP32
    - Enable fusing:    True
    - Enable grouped convolutions fusing:   True
    - Move mean values to preprocess section:   False
    - Reverse input channels:   False
TensorFlow specific parameters:
    - Input model in text protobuf format:  False
    - Path to model dump for TensorBoard:   None
    - List of shared libraries with TensorFlow custom layers implementation:    None
    - Update the configuration file with input/output node names:   None
    - Use configuration file used to generate the model with Object Detection API:  None
    - Operations to offload:    None
    - Patterns to offload:  None
    - Use the config file:  None
Model Optimizer version:    2019.1.1-83-g28dfbfd
WARNING: Logging before flag parsing goes to stderr.
E0722 17:24:22.964164 4474824128 infer.py:158] Shape [-1 -1 -1  3] is not fully defined for output 0 of ""image_tensor"". Use --input_shape with positive integers to override model input shapes.
E0722 17:24:22.964462 4474824128 infer.py:178] Cannot infer shapes or values for node ""image_tensor"".
E0722 17:24:22.964554 4474824128 infer.py:179] Not all output shapes were inferred or fully defined for node ""image_tensor"". 
 For more information please refer to Model Optimizer FAQ (&lt;INSTALL_DIR&gt;/deployment_tools/documentation/docs/MO_FAQ.html), question #40. 
E0722 17:24:22.964632 4474824128 infer.py:180] 
E0722 17:24:22.964720 4474824128 infer.py:181] It can happen due to bug in custom shape infer function &lt;function tf_placeholder_ext.&lt;locals&gt;.&lt;lambda&gt; at 0x12ab64bf8&gt;.
E0722 17:24:22.964787 4474824128 infer.py:182] Or because the node inputs have incorrect values/shapes.
E0722 17:24:22.964850 4474824128 infer.py:183] Or because input shapes are incorrect (embedded to the model or passed via --input_shape).
E0722 17:24:22.965915 4474824128 infer.py:192] Run Model Optimizer with --log_level=DEBUG for more information.
E0722 17:24:22.966033 4474824128 main.py:317] Exception occurred during running replacer ""REPLACEMENT_ID"" (&lt;class 'extensions.middle.PartialInfer.PartialInfer'&gt;): Stopped shape/value propagation at ""image_tensor"" node. 
 For more information please refer to Model Optimizer FAQ (&lt;INSTALL_DIR&gt;/deployment_tools/documentation/docs/MO_FAQ.html), question #38.
</code></pre>

<p>How do you think we should fix this?</p>
","<p>I updated my OpenVINO to OpenVINO toolkit R2 2019 &amp; using the below command I was able to generate IR file</p>

<pre><code>python3 ~/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_model frozen_inference_graph.pb --tensorflow_use_custom_operations_config ~/intel/openvino/deployment_tools/model_optimizer/extension/front/tf/ssd_support_api_v1.14.json --tensorflow_object_detection_api_pipeline_config pipeline.config -b 1 --data_type FP16 --reverse_input_channels
</code></pre>
","981","0","3","<python-3.x><tensorflow><raspberry-pi><openvino><movidius>"
"57145603","tensorflow openvino ssd-mobilnet coco custom dataset error input layer","2019-07-22 12:00:19","<p>So, I'm using TensorFlow SSD-Mobilnet V1 coco dataset. That I have further trained on my own dataset but when I try to convert it to OpenVino IR to run it on Raspberry PI with Movidius Chip. I get an error</p>

<pre class=""lang-py prettyprint-override""><code>➜  utils sudo python3 summarize_graph.py --input_model ssd.pb 
WARNING: Logging before flag parsing goes to stderr.
W0722 17:17:05.565755 4678620608 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.
W0722 17:17:06.696880 4678620608 deprecation_wrapper.py:119] From ../../mo/front/tf/loader.py:35: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.

W0722 17:17:06.697348 4678620608 deprecation_wrapper.py:119] From ../../mo/front/tf/loader.py:109: The name tf.MetaGraphDef is deprecated. Please use tf.compat.v1.MetaGraphDef instead.

W0722 17:17:06.697680 4678620608 deprecation_wrapper.py:119] From ../../mo/front/tf/loader.py:235: The name tf.NodeDef is deprecated. Please use tf.compat.v1.NodeDef instead.

1 input(s) detected:
Name: image_tensor, type: uint8, shape: (-1,-1,-1,3)
7 output(s) detected:
detection_boxes
detection_scores
detection_multiclass_scores
detection_classes
num_detections
raw_detection_boxes
raw_detection_scores
</code></pre>

<p>When I try to convert the ssd.pb(frozen model) to OpenVino IR </p>

<pre class=""lang-py prettyprint-override""><code>➜  model_optimizer sudo python3 mo_tf.py --input_model ssd.pb          
Password:
Model Optimizer arguments:
Common parameters:
    - Path to the Input Model:  /opt/intel/openvino_2019.1.144/deployment_tools/model_optimizer/ssd.pb
    - Path for generated IR:    /opt/intel/openvino_2019.1.144/deployment_tools/model_optimizer/.
    - IR output name:   ssd
    - Log level:    ERROR
    - Batch:    Not specified, inherited from the model
    - Input layers:     Not specified, inherited from the model
    - Output layers:    Not specified, inherited from the model
    - Input shapes:     Not specified, inherited from the model
    - Mean values:  Not specified
    - Scale values:     Not specified
    - Scale factor:     Not specified
    - Precision of IR:  FP32
    - Enable fusing:    True
    - Enable grouped convolutions fusing:   True
    - Move mean values to preprocess section:   False
    - Reverse input channels:   False
TensorFlow specific parameters:
    - Input model in text protobuf format:  False
    - Path to model dump for TensorBoard:   None
    - List of shared libraries with TensorFlow custom layers implementation:    None
    - Update the configuration file with input/output node names:   None
    - Use configuration file used to generate the model with Object Detection API:  None
    - Operations to offload:    None
    - Patterns to offload:  None
    - Use the config file:  None
Model Optimizer version:    2019.1.1-83-g28dfbfd
WARNING: Logging before flag parsing goes to stderr.
E0722 17:24:22.964164 4474824128 infer.py:158] Shape [-1 -1 -1  3] is not fully defined for output 0 of ""image_tensor"". Use --input_shape with positive integers to override model input shapes.
E0722 17:24:22.964462 4474824128 infer.py:178] Cannot infer shapes or values for node ""image_tensor"".
E0722 17:24:22.964554 4474824128 infer.py:179] Not all output shapes were inferred or fully defined for node ""image_tensor"". 
 For more information please refer to Model Optimizer FAQ (&lt;INSTALL_DIR&gt;/deployment_tools/documentation/docs/MO_FAQ.html), question #40. 
E0722 17:24:22.964632 4474824128 infer.py:180] 
E0722 17:24:22.964720 4474824128 infer.py:181] It can happen due to bug in custom shape infer function &lt;function tf_placeholder_ext.&lt;locals&gt;.&lt;lambda&gt; at 0x12ab64bf8&gt;.
E0722 17:24:22.964787 4474824128 infer.py:182] Or because the node inputs have incorrect values/shapes.
E0722 17:24:22.964850 4474824128 infer.py:183] Or because input shapes are incorrect (embedded to the model or passed via --input_shape).
E0722 17:24:22.965915 4474824128 infer.py:192] Run Model Optimizer with --log_level=DEBUG for more information.
E0722 17:24:22.966033 4474824128 main.py:317] Exception occurred during running replacer ""REPLACEMENT_ID"" (&lt;class 'extensions.middle.PartialInfer.PartialInfer'&gt;): Stopped shape/value propagation at ""image_tensor"" node. 
 For more information please refer to Model Optimizer FAQ (&lt;INSTALL_DIR&gt;/deployment_tools/documentation/docs/MO_FAQ.html), question #38.
</code></pre>

<p>How do you think we should fix this?</p>
","<p>for conversion of mobilenetv2 ssd add ""Postprocessor/Cast_1"" in original ssd_v2_support.json and use following command. it should work fine.</p>

<pre><code>""instances"": {
            ""end_points"": [
                ""detection_boxes"",
                ""detection_scores"",
                ""num_detections""
            ],
            ""start_points"": [
                ""Postprocessor/Shape"",
                ""Postprocessor/scale_logits"",
                ""Postprocessor/Tile"",
                ""Postprocessor/Reshape_1"",
                ""Postprocessor/Cast_1""
            ]
        },
</code></pre>

<p>then use following command</p>

<pre><code>#### object detection conversion

import platform
is_win = 'windows' in platform.platform().lower()

mo_tf_path = '/opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py'
json_file = '/opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json'
pb_file =          'model/frozen_inference_graph.pb'
pipeline_file =       'model/pipeline.config'
output_dir =       'output/'

img_height = 300
input_shape = [1,img_height,img_height,3]
input_shape_str = str(input_shape).replace(' ','')
input_shape_str

!python3 {mo_tf_path} --input_model {pb_file}  --tensorflow_object_detection_api_pipeline_config {pipeline_file} --tensorflow_use_custom_operations_config {json_file} --output=""detection_boxes,detection_scores,num_detections"" --output_dir {output_dir} --reverse_input_channels --data_type FP16 --log_level DEBUG 

</code></pre>
","981","0","3","<python-3.x><tensorflow><raspberry-pi><openvino><movidius>"
"66965182","opencv dnn module with OpenVino","2021-04-06 08:29:18","<p>I have no problems when working with dnn module
But I have downloaded OPENVINO to use dnn with engine inference, and I can't load the opencv_dnn452d.dll library
When I go to the opencv subdirecotry in openvino, and execute opencv_version_win32d.exe, I get this output, that says that inference engine has 3 backends (ONETBB, TBB and OPENM) , but none of them can be checked correctly:</p>
<pre><code>[ INFO:0] global C:\jenkins\workspace\OpenCV\OpenVINO\2021.3\build\windows\opencv\modules\core\src\parallel\registry_parallel.impl.hpp (90) cv::parallel::ParallelBackendRegistry::ParallelBackendRegistry core(parallel): Enabled backends(3, sorted by priority): ONETBB(1000); TBB(990); OPENMP(980)
[ INFO:0] global C:\jenkins\workspace\OpenCV\OpenVINO\2021.3\build\windows\opencv\modules\core\src\utils\plugin_loader.impl.hpp (67) cv::plugin::impl::DynamicLib::libraryLoad load C:\Program Files (x86)\Intel\openvino_2021.3.394\opencv\bin\opencv_core_parallel_onetbb452_64d.dll =&gt; FAILED
[ INFO:0] global C:\jenkins\workspace\OpenCV\OpenVINO\2021.3\build\windows\opencv\modules\core\src\utils\plugin_loader.impl.hpp (67) cv::plugin::impl::DynamicLib::libraryLoad load opencv_core_parallel_onetbb452_64d.dll =&gt; FAILED
[ INFO:0] global C:\jenkins\workspace\OpenCV\OpenVINO\2021.3\build\windows\opencv\modules\core\src\utils\plugin_loader.impl.hpp (67) cv::plugin::impl::DynamicLib::libraryLoad load C:\Program Files (x86)\Intel\openvino_2021.3.394\opencv\bin\opencv_core_parallel_tbb452_64d.dll =&gt; FAILED
[ INFO:0] global C:\jenkins\workspace\OpenCV\OpenVINO\2021.3\build\windows\opencv\modules\core\src\utils\plugin_loader.impl.hpp (67) cv::plugin::impl::DynamicLib::libraryLoad load opencv_core_parallel_tbb452_64d.dll =&gt; FAILED
[ INFO:0] global C:\jenkins\workspace\OpenCV\OpenVINO\2021.3\build\windows\opencv\modules\core\src\utils\plugin_loader.impl.hpp (67) cv::plugin::impl::DynamicLib::libraryLoad load C:\Program Files (x86)\Intel\openvino_2021.3.394\opencv\bin\opencv_core_parallel_openmp452_64d.dll =&gt; FAILED
[ INFO:0] global C:\jenkins\workspace\OpenCV\OpenVINO\2021.3\build\windows\opencv\modules\core\src\utils\plugin_loader.impl.hpp (67) cv::plugin::impl::DynamicLib::libraryLoad load opencv_core_parallel_openmp452_64d.dll =&gt; FAILED
</code></pre>
<p>What do I have to do to get inference engine working correctly?
Thanks in advance for your answer</p>
","<p>First and foremost you need to install some pre-requisite for OpenVINO.</p>
<p>Then, you need to set it up properly in your system.</p>
<p>You can follow <a href=""https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_windows.html"" rel=""nofollow noreferrer"">this step by step guide</a>.</p>
","976","0","1","<c++><opencv><openvino>"
"58803122","Exporting TensorFlow 2 model to OpenVino","2019-11-11 14:17:50","<p>I want to export (optimize) a TensorFlow 2 model to OpenVino.</p>

<p>The only documentation I found regards Tensorflow 1. When followed the instructions, the OpenVino model optimization failed to work with a tf2 model.</p>
","<p>OpenVino Model Optimizer does not support Tensorflow 2.0 yet. 
But, you can use Tensorflow 1.14 freeze_graph.py to freeze a TF 2.0 model.
This frozen pb should be accepted by Model Optimizer.</p>
","899","0","1","<tensorflow><tensorflow2.0><openvino>"
"68500664","Openvino nsc2 with docker : Can't initialize GTK backend in function 'cvInitSystem'","2021-07-23 14:16:41","<p>I do have some troubles with openvino and the Neural Compute Stick 2 on docker:</p>
<p>When I try to run :</p>
<pre><code>python3 object_de
tection_demo.py -d MYRIAD -i /home/openvino/video.mp4 -m /home/openvino/person-vehicle-bike-detection-2004.xml -at ssd --labels /home/openvino/data/dataset_classes/coco_91cl_bkgr.txt -o /home/openvino/output_video.mp4
 
</code></pre>
<p>It doesn't workd and it takes 1 minute to give me this message , whicht is strange due to the ncs2.</p>
<pre><code>[ INFO ] Initializing Inference Engine...
[ INFO ] Loading network...
[ INFO ] Reading network from IR...
[ INFO ] Use BoxesLabelsParser
[ INFO ] Loading network to MYRIAD plugin...
[ INFO ] Starting inference...
To close the application, press 'CTRL+C' here or switch to the output window and press ESC key
OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 8 and format 'mp4 / MP4 (MPEG-4 Part 14)'
OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'
Unable to init server: Could not connect: Connection refused
Traceback (most recent call last):
  File &quot;object_detection_demo.py&quot;, line 350, in &lt;module&gt;
    sys.exit(main() or 0)
  File &quot;object_detection_demo.py&quot;, line 278, in main
    cv2.imshow('Detection Results', frame)
cv2.error: OpenCV(4.5.3-openvino) ../opencv/modules/highgui/src/window_gtk.cpp:635: error: (-2:Unspecified error) Can't initialize GTK backend in function 'cvInitSystem'
</code></pre>
<p>Any idea ?? Thanks !!</p>
","<p>This is an error due to X11 forwarding in docker. GTK backend can't be initialized when X11 forwarding is disabled.<br />
Follow instructions here: <a href=""https://stackoverflow.com/questions/48235040/run-x-application-in-a-docker-container-reliably-on-a-server-connected-via-ssh-w"">Run X application in a Docker container reliably on a server connected via SSH without &quot;--net host&quot;</a></p>
","898","0","1","<python><docker><helper><openvino>"
"65225588","openvino python inference api import error","2020-12-09 22:02:38","<p>I want to use openvino for object detection.<br />
I installed it in conda environment on ubuntu 20.</p>
<p>I added this line in .bashrc:</p>
<pre><code>export LD_LIBRARY_PATH=/home/user/anaconda3/envs/openvino/bin/python3/../../lib:${LD_LIBRARY_PATH}
</code></pre>
<p>I run after:</p>
<pre><code>source .bashrc

which python  # gives /home/user/anaconda3/envs/openvino/bin/python
echo $PYTHONPATH # does not return anything
</code></pre>
<p>I try to import:</p>
<pre><code>from openvino.inference_engine import IENetwork, IECore
</code></pre>
<p>I go this error:</p>
<p><strong>from .ie_api import *<br />
ImportError: libtbb.so.2: cannot open shared object file: No such file or directory</strong></p>
","<p>Make sure you had carefully followed <a href=""https://docs.openvinotoolkit.org/latest/openvino_docs_get_started_get_started_linux.html"" rel=""nofollow noreferrer"">these installation step</a>.
You should be able to run the examples if you had set the OpenVINO toolkit and its pre-requisite correctly.</p>
<p>Afterward, you can try to proceed with your attempt.
Please take note that currently only these Operating Systems are supported:</p>
<p><strong>Ubuntu 18.04.x</strong> long-term support (LTS), 64-bit</p>
<p>CentOS 7.6, 64-bit (for target only)</p>
<p>Yocto Project v3.0, 64-bit (for target only and requires modifications)</p>
","891","1","1","<opencv><deep-learning><openvino>"
"56530826","How to use the Pre trained models provided by Intel's OpenVINO","2019-06-10 17:20:34","<p>I am interested in a certain demo for OpenVino which is the smart classroom
link:<a href=""https://github.com/opencv/open_model_zoo/tree/master/demos/smart_classroom_demo"" rel=""nofollow noreferrer"">https://github.com/opencv/open_model_zoo/tree/master/demos/smart_classroom_demo</a></p>

<p>But I only want the function of detecting raised hands hence I see that it provided the pre trained model here : <a href=""https://download.01.org/opencv/2019/open_model_zoo/R1/20190404_140900_models_bin/person-detection-raisinghand-recognition-0001/FP16/"" rel=""nofollow noreferrer"">https://download.01.org/opencv/2019/open_model_zoo/R1/20190404_140900_models_bin/person-detection-raisinghand-recognition-0001/FP16/</a></p>

<p>My Question is how can I utilize the pre trained models?</p>

<p>I have basic understanding of OpenCV for both in Python and C++ so if anyone could actually lead me to articles that explain steps by steps on how to use this model, it would be very helpful.</p>
","<p>The Intel® Distribution of OpenVINO™ toolkit (formerly Intel® CV SDK) contains optimized OpenCV and OpenVX libraries, deep learning code samples, and pretrained models to enhance computer vision development.</p>

<p>It’s validated on 100+ open source and custom models, and is available absolutely free.</p>

<p>Kindly refer the below links to get an idea on this.</p>

<p><a href=""https://github.com/opencv/open_model_zoo/blob/master/intel_models/person-detection-raisinghand-recognition-0001/description/person-detection-raisinghand-recognition-0001.md"" rel=""nofollow noreferrer"">https://github.com/opencv/open_model_zoo/blob/master/intel_models/person-detection-raisinghand-recognition-0001/description/person-detection-raisinghand-recognition-0001.md</a></p>

<p><a href=""https://docs.openvinotoolkit.org/latest/_person_detection_action_recognition_0005_description_person_detection_action_recognition_0005.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_person_detection_action_recognition_0005_description_person_detection_action_recognition_0005.html</a></p>

<p><a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html</a></p>

<p><a href=""https://techdecoded.intel.io/essentials/optimize-deep-learning-inference-applications-using-openvino-toolkit/#gs.l98omp"" rel=""nofollow noreferrer"">https://techdecoded.intel.io/essentials/optimize-deep-learning-inference-applications-using-openvino-toolkit/#gs.l98omp</a></p>

<p><a href=""https://stackoverflow.com/questions/55345798/how-to-use-openvino-pre-trained-models"">How to use OpenVINO pre-trained models?</a></p>

<p>Hope these will help you.</p>
","876","-1","1","<opencv><openvino>"
"54479282","How can I run samples from openVINO","2019-02-01 12:12:53","<p>I use openVINO R5 2018.5.445. I successfully installed all the software and dependencies for ubuntu 18.04. The test was successful as well. However, the sample models I am trying to run won't build. I followed the full documentation. Using the following commands:</p>

<pre><code>cmake -DCMAKE_BUILD_TYPE=Release
</code></pre>

<p>(<a href=""https://software.intel.com/en-us/articles/OpenVINO-InferEngine"" rel=""nofollow noreferrer"">https://software.intel.com/en-us/articles/OpenVINO-InferEngine</a> > Using Inference Engine Samples).</p>

<p>How should I Build the samples?</p>
","<p>I ended up doing the following:</p>

<pre><code>cd /opt/intel/computer_vision_sdk/inference_engine/samples
</code></pre>

<p>And then:</p>

<pre><code>sudo ./build_samples.sh
</code></pre>

<p>I can now run models (in this case pose estimation) using this command:</p>

<pre><code>./human_pose_estimation_demo -m /opt/intel/computer_vision_sdk_2018.5.445/deployment_tools/intel_models/human-pose-estimation-0001/FP32/human-pose-estimation-0001.xml
</code></pre>
","871","0","2","<openvino>"
"54479282","How can I run samples from openVINO","2019-02-01 12:12:53","<p>I use openVINO R5 2018.5.445. I successfully installed all the software and dependencies for ubuntu 18.04. The test was successful as well. However, the sample models I am trying to run won't build. I followed the full documentation. Using the following commands:</p>

<pre><code>cmake -DCMAKE_BUILD_TYPE=Release
</code></pre>

<p>(<a href=""https://software.intel.com/en-us/articles/OpenVINO-InferEngine"" rel=""nofollow noreferrer"">https://software.intel.com/en-us/articles/OpenVINO-InferEngine</a> > Using Inference Engine Samples).</p>

<p>How should I Build the samples?</p>
","<p>To Build samples in custom location/path/directory :</p>
<pre class=""lang-sh prettyprint-override""><code>cd /opt/intel/computer_vision_sdk/inference_engine/samples   
mkdir build  
cd build CMAKE ..   
make 
</code></pre>
<p>By default, <code>-DCMAKE_BUILD_TYPE</code> takes <code>RELEASE</code>.</p>
","871","0","2","<openvino>"
"58584797","OpenVino model optimizer error(FusedBatchNormV3)","2019-10-28 01:24:59","<p>I ask the question because I wanted to solve the error I experienced.</p>

<p>I want to use 'SSD lite Mobilenet V2' in Raspberry Pi 3 B+ and NCS(not 2, it is NCS1).</p>

<p>So I installed OpenVINO 2019_R3 on my Pi(Raspbian stretch) and Laptop(Linux, not all programs, just Model optimizer).</p>

<p>When I optimize SSD lite mobilenet v2(trained zoo model), it was fine.</p>

<p>So, i trained my model in Google Colab using Tensorflow object detection api.</p>

<p>But when I optimize my own SSD lite model, here is log and what I typed to shell.</p>

<pre><code>sudo python3 mo_tf.py --input_model frozen_inference_graph.pb --tensorflow_use_custom_operations_config ssd_support_api_v1.14.json --tensorflow_object_detection_api_pipeline_config pipeline.config --reverse_input_channels --data_type FP16 --keep_shape_ops

[ WARNING ]  Use of deprecated cli option --disable_fusing detected. Option use in the following releases will be fatal. Please use --finegrain_fusing cli option instead
Model Optimizer arguments:
Common parameters:
        - Path to the Input Model:      /opt/intel/openvino_2019.3.334/deployment_tools/model_optimizer/frozen_inference_graph.pb
        - Path for generated IR:        /opt/intel/openvino_2019.3.334/deployment_tools/model_optimizer/.
        - IR output name:       frozen_inference_graph
        - Log level:    ERROR
        - Batch:        Not specified, inherited from the model
        - Input layers:         Not specified, inherited from the model
        - Output layers:        Not specified, inherited from the model
        - Input shapes:         Not specified, inherited from the model
        - Mean values:  Not specified
        - Scale values:         Not specified
        - Scale factor:         Not specified
        - Precision of IR:      FP16
        - Enable fusing:        False
        - Enable grouped convolutions fusing:   True
        - Move mean values to preprocess section:       False
        - Reverse input channels:       True
TensorFlow specific parameters:
        - Input model in text protobuf format:  False
        - Path to model dump for TensorBoard:   None
        - List of shared libraries with TensorFlow custom layers implementation:        None
        - Update the configuration file with input/output node names:   None
        - Use configuration file used to generate the model with Object Detection API:  /opt/intel/openvino_2019.3.334/deployment_tools/model_optimizer/pipeline.config
        - Operations to offload:        None
        - Patterns to offload:  None
        - Use the config file:  /opt/intel/openvino_2019.3.334/deployment_tools/model_optimizer/ssd_support_api_v1.14.json
Model Optimizer version:        2019.3.0-375-g332562022
The Preprocessor block has been removed. Only nodes performing mean value subtraction and scaling (if applicable) are kept.
[ ERROR ]  List of operations that cannot be converted to Inference Engine IR:
[ ERROR ]      FusedBatchNormV3 (76)
[ ERROR ]          FeatureExtractor/MobilenetV2/Conv/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          BoxPredictor_0/BoxEncodingPredictor_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/FusedBatchNormV3
[ ERROR ]          BoxPredictor_1/BoxEncodingPredictor_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/FusedBatchNormV3
[ ERROR ]          BoxPredictor_2/BoxEncodingPredictor_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/FusedBatchNormV3
[ ERROR ]          BoxPredictor_3/BoxEncodingPredictor_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/FusedBatchNormV3
[ ERROR ]          BoxPredictor_4/BoxEncodingPredictor_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/FusedBatchNormV3
[ ERROR ]          BoxPredictor_5/BoxEncodingPredictor_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          BoxPredictor_0/ClassPredictor_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          BoxPredictor_1/ClassPredictor_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          BoxPredictor_2/ClassPredictor_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          BoxPredictor_3/ClassPredictor_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          BoxPredictor_4/ClassPredictor_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]          BoxPredictor_5/ClassPredictor_depthwise/BatchNorm/FusedBatchNormV3
[ ERROR ]  Part of the nodes was not converted to IR. Stopped.
 For more information please refer to Model Optimizer FAQ (https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_...), question #24.
</code></pre>

<p>I guessed there are problems in using 'FusedBatchNormV3'. Tensorflow 1.15.0-rc0  is installed in laptop and colab.</p>

<p>So I changed Tensorflow versions both colab and laptop(from 1.15 to 1.14), but still problem remained.</p>

<p>Is there any method that I turn off the FusedBatchNormV3?(I think OpenVINO R3 doesn't support it)</p>

<p>and here is pipeline.config file.</p>

<pre><code>model {
  ssd {
    num_classes: 7
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
    }
    feature_extractor {
      type: ""ssd_mobilenet_v2""
      depth_multiplier: 1.0
      min_depth: 16
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 3.99999989895e-05
          }
        }
        initializer {
          truncated_normal_initializer {
            mean: 0.0
            stddev: 0.0299999993294
          }
        }
        activation: RELU_6
        batch_norm {
          decay: 0.999700009823
          center: true
          scale: true
          epsilon: 0.0010000000475
          train: true
        }
      }
      use_depthwise: true
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 3.99999989895e-05
            }
          }
          initializer {
            truncated_normal_initializer {
              mean: 0.0
              stddev: 0.0299999993294
            }
          }
          activation: RELU_6
          batch_norm {
            decay: 0.999700009823
            center: true
            scale: true
            epsilon: 0.0010000000475
            train: true
          }
        }
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.800000011921
        kernel_size: 3
        box_code_size: 4
        apply_sigmoid_to_scores: false
        use_depthwise: true
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.20000000298
        max_scale: 0.949999988079
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.333299994469
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 0.300000011921
        iou_threshold: 0.600000023842
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid {
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.990000009537
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 3
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
  }
}
train_config {
  batch_size: 32
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
  optimizer {
    rms_prop_optimizer {
      learning_rate {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.00400000018999
          decay_steps: 800720
          decay_factor: 0.949999988079
        }
      }
      momentum_optimizer_value: 0.899999976158
      decay: 0.899999976158
      epsilon: 1.0
    }
  }
  fine_tune_checkpoint: ""/content/confg_ssd2/model.ckpt""
  num_steps: 200000
  fine_tune_checkpoint_type: ""detection""
}
train_input_reader {
  label_map_path: ""/content/confg_ssd2/mscoco_label_map.pbtxt""
  tf_record_input_reader {
    input_path: ""/content/confg_ssd2/mscoco_train.record""
  }
}
eval_config {
  num_examples: 8000
  max_evals: 10
  use_moving_averages: false
}
eval_input_reader {
  label_map_path: ""/content/confg_ssd2/mscoco_label_map.pbtxt""
  shuffle: false
  num_readers: 1
  tf_record_input_reader {
    input_path: ""/content/confg_ssd2/mscoco_val.record""
  }
}
</code></pre>
","<p>I finally solved it! 
Downgrade tensorflow from 1.15(or 1.14) to 1.13. It worked for me!</p>
","859","2","1","<tensorflow><object-detection><object-detection-api><openvino>"
"56718095","the command ... returned a non-zero code 100 docker","2019-06-22 18:29:59","<p>I'm trying to install OpenVino on my Raspberry using Docker.</p>

<p>I have this Dockerfile:</p>

<pre><code>FROM raspbian/stretch

ARG INSTALL_DIR=""/opt/intel/inference_engine_vpu_arm""

RUN apt-get -y update \
    &amp;&amp; DEBIAN_FRONTEND=noninteractive &amp;&amp; apt-get -y upgrade &amp;&amp; apt-get autoremove &amp;&amp; \
    apt-get install -y \
    apt-transport-https \
        build-essential \
    cmake \
        cpio \
        lsb-release \
        pciutils \
        python3.5 \
        python3.5-dev \
        python3-pip \
        python3-setuptools \
    ffmpeg \
    libjpeg-dev \
        libtiff5-dev \
        libjasper-dev \
        libpng12-dev \
        libavcodec-dev \
        libavformat-dev \
        libswscale-dev \
        libv4l-dev \
        libxvidcore-dev \
        libx264-dev \
        libgtk2.0-dev \
        libgtk-3-dev \
        libatlas-base-dev \
        gfortran \
        libgstreamer1.0-0 \
        libgstreamer-plugins-base1.0-0

RUN usermod -a -G users ""$(whoami)""

COPY inference_engine_vpu_arm $INSTALL_DIR 

RUN sed -i ""s|&lt;INSTALLDIR&gt;|$INSTALL_DIR|"" $INSTALL_DIR/bin/setupvars.sh &amp;&amp; \
    echo ""source $INSTALL_DIR/bin/setupvars.sh"" &gt;&gt; $HOME/.bashrc

RUN [""/bin/bash"", ""-c"", ""source $INSTALL_DIR/bin/setupvars.sh &amp;&amp; /bin/bash $INSTALL_DIR/install_dependencies/install_NCS_udev_rules.sh""]

RUN pip3 install numpy

RUN apt autoremove -y &amp;&amp; \
    rm -rf  /var/lib/apt/lists/*

CMD [""/bin/bash""]

</code></pre>

<p>But I have this error when I try to build:</p>

<p>E: Unable to correct problems, you have held broken packages.
The command '/bin/sh -c apt-get -y update..... returned a non-zero code: 100</p>

<p>Do you have any idea?
Thanks</p>
","<p>After a some google search it seems the error happens because the apt daemon is not able to connect to the configured repositories. This is likely since the base image was not updated for a while as i can see on docker hub. </p>

<p>If you not familiar with the available repositories you can generate them easily with online tools such as: <a href=""https://debgen.simplylinux.ch/index.php?generate"" rel=""nofollow noreferrer"">https://debgen.simplylinux.ch/index.php?generate</a></p>

<p>You can put them into the docker image with a simple COPY command like </p>

<pre><code>COPY sources.list /etc/apt/sources.list
</code></pre>

<p>where the first argument refers to a local file, the second to the docker image</p>
","858","0","1","<docker><raspberry-pi><openvino>"
"54905469","Trying to get a openVino IR from a frozen TF model","2019-02-27 12:26:21","<p>I have a given inceptionV2 model I want to get working on the rPi, using the NCS2. Examples work fine. Now, the model I am given is a built upon the ssd_inceptionv2 demo, which I know works, since I've been able to convert that demo's frozen pb to IR bin and xml files, and successfully run them on the pi. However, when I try to convert the given model to an IR, it fails. To be more specific, it fails in different ways, depending on how I go about trying to convert it.</p>
<p>The given model has a frozen .pb file, checkpoint files and a .pbtxt. Converting the .pb file the command I'm using is:</p>
<pre><code>python3 /opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo_tf.py 
  --input_model frozengraph.pb 
  --tensorflow_use_custom_operations_config /opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json 
  --tensorflow_object_detection_api_pipeline &quot;PATH&quot;/pipeline.config 
  --reverse_input_channels 
  --data_type FP16
</code></pre>
<p>this gives the input shape error, which I remedy with <code>--input_shape [1,299,299,3]</code>, but it only leads to the error:</p>
<blockquote>
<p>Cannot infer shapes or values for node</p>
<p>&quot;Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField/TopKV2&quot;</p>
</blockquote>
<p>So I try both re-freezing the model and running the conversion on the graph.pbtxt. For both methods, it throws errors since the number of nodes is 0 and 1 respectively.</p>
<p>Any ideas what I could be doing wrong here?</p>
","<p>Assuming that you are able to detect the objects using the frozen graph, try once by changing the command line argument to <strong>tensorflow_object_detection_api_pipeline_config</strong> also which <strong>pipeline.config</strong> file are you using? You should be able to create IR files if you are using the <strong>pipeline.config</strong> file which was generated along with the frozen graph for your custom model.</p>
","827","1","3","<tensorflow><raspberry-pi><intel><openvino>"
"54905469","Trying to get a openVino IR from a frozen TF model","2019-02-27 12:26:21","<p>I have a given inceptionV2 model I want to get working on the rPi, using the NCS2. Examples work fine. Now, the model I am given is a built upon the ssd_inceptionv2 demo, which I know works, since I've been able to convert that demo's frozen pb to IR bin and xml files, and successfully run them on the pi. However, when I try to convert the given model to an IR, it fails. To be more specific, it fails in different ways, depending on how I go about trying to convert it.</p>
<p>The given model has a frozen .pb file, checkpoint files and a .pbtxt. Converting the .pb file the command I'm using is:</p>
<pre><code>python3 /opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo_tf.py 
  --input_model frozengraph.pb 
  --tensorflow_use_custom_operations_config /opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json 
  --tensorflow_object_detection_api_pipeline &quot;PATH&quot;/pipeline.config 
  --reverse_input_channels 
  --data_type FP16
</code></pre>
<p>this gives the input shape error, which I remedy with <code>--input_shape [1,299,299,3]</code>, but it only leads to the error:</p>
<blockquote>
<p>Cannot infer shapes or values for node</p>
<p>&quot;Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField/TopKV2&quot;</p>
</blockquote>
<p>So I try both re-freezing the model and running the conversion on the graph.pbtxt. For both methods, it throws errors since the number of nodes is 0 and 1 respectively.</p>
<p>Any ideas what I could be doing wrong here?</p>
","<p>Tensorflow models can be a bit tricky, especially when you modify those in the model zoo, which are already quite complex. There's a few things I'd like to mention for your scenario:</p>

<p>In <a href=""https://software.intel.com/en-us/articles/OpenVINO-Using-TensorFlow"" rel=""nofollow noreferrer"">this OpenVINO guide</a>, at ""<em>Using TensorFlow*-Specific Conversion Parameters</em>"", you'll find that</p>

<blockquote>
  <p>Models produced with TensorFlow* usually have not fully defined shapes (contain -1 in some dimensions). It is necessary to pass explicit shape for the input using command line parameter --input_shape or -b to override just batch dimension. If the shape is fully defined, then there is no need to specify -b or --input_shape options.</p>
</blockquote>

<p><sup>Some examples of that include <a href=""https://software.intel.com/en-us/forums/computer-vision/topic/802492#comment-1932592"" rel=""nofollow noreferrer"">this Openvino issue/answer</a>, and a couple more in the <a href=""https://software.intel.com/en-us/articles/OpenVINO-ModelOptimizer"" rel=""nofollow noreferrer"">Model Optimizer's guide</a> in the ""<em>Advanced Topics about the Model Optimizer Internals</em>"" section.</sup></p>

<p>For the record, I was able to convert a fine-tuned <strong>faster_rcnn_inception_v2</strong> from the model zoo without specifying the <code>--input_shape</code> argument, so it really depends on your model.</p>

<p>Also, this might be a typo or version problem, but the <code>--tensorflow_object_detection_api_pipeline</code> argument is officially <code>--tensorflow_object_detection_api_pipeline_config</code> (as mentionned by @Bhargavi).</p>

<p>Finally, a good way to debug your converting commands is to use the <code>--log_level DEBUG</code> parameter to see the full output.</p>
","827","1","3","<tensorflow><raspberry-pi><intel><openvino>"
"54905469","Trying to get a openVino IR from a frozen TF model","2019-02-27 12:26:21","<p>I have a given inceptionV2 model I want to get working on the rPi, using the NCS2. Examples work fine. Now, the model I am given is a built upon the ssd_inceptionv2 demo, which I know works, since I've been able to convert that demo's frozen pb to IR bin and xml files, and successfully run them on the pi. However, when I try to convert the given model to an IR, it fails. To be more specific, it fails in different ways, depending on how I go about trying to convert it.</p>
<p>The given model has a frozen .pb file, checkpoint files and a .pbtxt. Converting the .pb file the command I'm using is:</p>
<pre><code>python3 /opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo_tf.py 
  --input_model frozengraph.pb 
  --tensorflow_use_custom_operations_config /opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json 
  --tensorflow_object_detection_api_pipeline &quot;PATH&quot;/pipeline.config 
  --reverse_input_channels 
  --data_type FP16
</code></pre>
<p>this gives the input shape error, which I remedy with <code>--input_shape [1,299,299,3]</code>, but it only leads to the error:</p>
<blockquote>
<p>Cannot infer shapes or values for node</p>
<p>&quot;Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField/TopKV2&quot;</p>
</blockquote>
<p>So I try both re-freezing the model and running the conversion on the graph.pbtxt. For both methods, it throws errors since the number of nodes is 0 and 1 respectively.</p>
<p>Any ideas what I could be doing wrong here?</p>
","<p>Please refer the below link</p>

<p><a href=""http://docs.openvinotoolkit.org/R5/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Object_Detection_API_Models.html"" rel=""nofollow noreferrer"">http://docs.openvinotoolkit.org/R5/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Object_Detection_API_Models.html</a></p>

<p>Your command looks similar to :</p>

<pre><code>&lt;INSTALL_DIR&gt;/deployment_tools/model_optimizer/mo_tf.py --input_model=/tmp/ssd_inception_v2_coco_2018_01_28/frozen_inference_graph.pb --tensorflow_use_custom_operations_config &lt;INSTALL_DIR&gt;/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json --tensorflow_object_detection_api_pipeline_config /tmp/ssd_inception_v2_coco_2018_01_28/pipeline.config --reverse_input_channels
</code></pre>

<p>Please carefully read the section within Custom Input Shape in the documentation.Please add a<br>
--log_level DEBUG to see more details of your MO failure.</p>

<p>Hope this helps.</p>
","827","1","3","<tensorflow><raspberry-pi><intel><openvino>"
"60136527","CMake could not find any instance of Visual Studio, openvino","2020-02-09 12:17:40","<p>Because of avx instruction not available in my cpu, i used copy the <strong>cpu_extensionavx2.dll</strong></p>

<p>removing avx2 and copy in the build folder</p>

<p>I had already run setupvars.bat using the command:</p>

<pre><code>""C:\Program Files (x86)\IntelSWTools\openvino\bin\setupvars.bat""
</code></pre>

<p>It is working.</p>

<p>While running <code>cmake</code> by running this command:</p>

<pre><code>""C:\Program Files\CMake\bin\cmake.exe"" ^
  -G ""Visual Studio 14 Win64"" ^
   ""C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\inference_engine\samples""
</code></pre>

<p>it throws the following error, even when I changed to <strong>visual studio 16</strong> but gave same error:</p>

<pre><code>(base) C:\Users\ra\build&gt;
(base) C:\Users\ra\build&gt; ""C:\Program Files\CMake\bin\cmake.exe"" ^
More?   -G ""Visual Studio 16"" ^
More?    ""C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\inferenc
e_engine\samples""
CMake Error at CMakeLists.txt:7 (project):
  Generator

    Visual Studio 16 2019

  could not find any instance of Visual Studio.



-- Configuring incomplete, errors occurred!
See also ""C:/Users/ra/build/CMakeFiles/CMakeOutput.log"".
</code></pre>

<p>but when I used this command:</p>

<pre><code> ""C:\Program Files\CMake\bin\cmake.exe"" ^
 -G ""Visual Studio 20"" ^
 ""C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\inferene_engine\samples""
</code></pre>

<p>it throws <strong>generator error</strong> as:</p>

<p>e_engine\samples""
CMake Error at CMakeLists.txt:7 (project):
  Generator</p>

<pre><code>Visual Studio 15 2017
</code></pre>

<p>could not find any instance of Visual Studio.</p>

<p>-- Configuring incomplete, errors occurred!
See also ""C:/Users/ra/build/CMakeFiles/CMakeOutput.log"".</p>

<pre><code>(base) C:\Users\ra\build&gt; ""C:\Program Files\CMake\bin\cmake.exe"" ^
More?   -G ""Visual Studio 20"" ^
More?    ""C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\inferenc
e_engine\samples""

CMake Error: Could not create named generator Visual Studio 20
Generators
  Visual Studio 16 2019        = Generates Visual Studio 2019 project files.
                                 Use -A option to specify architecture.
  Visual Studio 15 2017 [arch] = Generates Visual Studio 2017 project files.
                                 Optional [arch] can be ""Win64"" or ""ARM"".
  Visual Studio 14 2015 [arch] = Generates Visual Studio 2015 project files.
                                 Optional [arch] can be ""Win64"" or ""ARM"".
  Visual Studio 12 2013 [arch] = Generates Visual Studio 2013 project files.
                                 Optional [arch] can be ""Win64"" or ""ARM"".
  Visual Studio 11 2012 [arch] = Generates Visual Studio 2012 project files.
                                 Optional [arch] can be ""Win64"" or ""ARM"".
  Visual Studio 10 2010 [arch] = Generates Visual Studio 2010 project files.
                                 Optional [arch] can be ""Win64"" or ""IA64"".
  Visual Studio 9 2008 [arch]  = Generates Visual Studio 2008 project files.
                                 Optional [arch] can be ""Win64"" or ""IA64"".
  Borland Makefiles            = Generates Borland makefiles.
* NMake Makefiles              = Generates NMake makefiles.
  NMake Makefiles JOM          = Generates JOM makefiles.
  MSYS Makefiles               = Generates MSYS makefiles.
  MinGW Makefiles              = Generates a make file for use with
                                 mingw32-make.
  Unix Makefiles               = Generates standard UNIX makefiles.
  Green Hills MULTI            = Generates Green Hills MULTI files
                                 (experimental, work-in-progress).
  Ninja                        = Generates build.ninja files.
  Watcom WMake                 = Generates Watcom WMake makefiles.
  CodeBlocks - MinGW Makefiles = Generates CodeBlocks project files.
  CodeBlocks - NMake Makefiles = Generates CodeBlocks project files.
  CodeBlocks - NMake Makefiles JOM
                               = Generates CodeBlocks project files.
  CodeBlocks - Ninja           = Generates CodeBlocks project files.
  CodeBlocks - Unix Makefiles  = Generates CodeBlocks project files.
  CodeLite - MinGW Makefiles   = Generates CodeLite project files.
  CodeLite - NMake Makefiles   = Generates CodeLite project files.
  CodeLite - Ninja             = Generates CodeLite project files.
  CodeLite - Unix Makefiles    = Generates CodeLite project files.
  Sublime Text 2 - MinGW Makefiles
                               = Generates Sublime Text 2 project files.
  Sublime Text 2 - NMake Makefiles
                               = Generates Sublime Text 2 project files.
  Sublime Text 2 - Ninja       = Generates Sublime Text 2 project files.
  Sublime Text 2 - Unix Makefiles
                               = Generates Sublime Text 2 project files.
  Kate - MinGW Makefiles       = Generates Kate project files.
  Kate - NMake Makefiles       = Generates Kate project files.
  Kate - Ninja                 = Generates Kate project files.
  Kate - Unix Makefiles        = Generates Kate project files.
  Eclipse CDT4 - NMake Makefiles
                               = Generates Eclipse CDT 4.0 project files.
  Eclipse CDT4 - MinGW Makefiles
                               = Generates Eclipse CDT 4.0 project files.
  Eclipse CDT4 - Ninja         = Generates Eclipse CDT 4.0 project files.
  Eclipse CDT4 - Unix Makefiles= Generates Eclipse CDT 4.0 project files.
</code></pre>

<p>I am running Windows 7 64-bit, all are installed CMake latest version.
Help me.</p>
","<p>You can build the demos automatically using the bat script:</p>
<pre><code>&quot;C:\Program Files (x86)\Intel\openvino_2021\inference_engine\demos\build_demos_msvc.bat&quot;
</code></pre>
<p>You can also build them manually if you wish.</p>
<pre><code>&quot;C:\Program Files (x86)\Intel\openvino_2021\bin\setupvars.bat&quot;
md build
cd build
cmake -A x64 &quot;C:\Program Files (x86)\Intel\openvino_2021\inference_engine\demos
cmake --build . --config Release
</code></pre>
<p>Please ensure your system meets the <a href=""https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/system-requirements.html"" rel=""nofollow noreferrer"">system requirements</a> and you are using a supported OS. For Windows, only Windows 10 is supported.</p>
<p><strong>Processors</strong></p>
<ul>
<li>6th to 11th generation Intel® Core™ processors</li>
<li>List item 1st to 3rd generation
of Intel® Xeon® Scalable processors</li>
</ul>
","822","0","1","<cmake><intel><avx><avx2><openvino>"
"61763845","converting Tensorflow Object Detection API (ssdlite_mobilenet_v3_small) to openvino IR failure","2020-05-12 23:35:57","<p>I have trained my ssdlite_mobilenet_v3 in tensorflow and export as frozen_inference_graph.pb. I am able to run it. Now I would like to convert to openvino Inference Engine files (.xml and .bin). But I encounter following errors. I include my command line below and also you may download my model files in a <a href=""https://github.com/dusty-nv/jetson-inference/issues/569"" rel=""nofollow noreferrer"">sample_model_inference.zip here</a>. Could anyone help me to find out what's missing? or how to fix it. Thanks a lot.</p>

<p>Command line:</p>

<pre><code>mo_tf.py --input_model ../sample_model_inference/frozen_inference_graph.pb --transformations_config /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json --tensorflow_object_detection_api_pipeline_config ../sample_model_inference/pipeline.config
</code></pre>

<p>Error messages:</p>

<p>(openvino) paul@tensor:~/tf1.15/models/research/object_detection/samples/sample_model_ir$ mo_tf.py --input_model ../sample_model_inference/frozen_inference_graph.pb --transformations_config /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json --tensorflow_object_detection_api_pipeline_config ../sample_model_inference/pipeline.config
Model Optimizer arguments:
Common parameters:
    - Path to the Input Model:     /home/paul/tf1.15/models/research/object_detection/samples/sample_model_ir/../sample_model_inference/frozen_inference_graph.pb
    - Path for generated IR:     /home/paul/tf1.15/models/research/object_detection/samples/sample_model_ir/.
    - IR output name:     frozen_inference_graph
    - Log level:     ERROR
    - Batch:     Not specified, inherited from the model
    - Input layers:     Not specified, inherited from the model
    - Output layers:     Not specified, inherited from the model
    - Input shapes:     Not specified, inherited from the model
    - Mean values:     Not specified
    - Scale values:     Not specified
    - Scale factor:     Not specified
    - Precision of IR:     FP32
    - Enable fusing:     True
    - Enable grouped convolutions fusing:     True
    - Move mean values to preprocess section:     False
    - Reverse input channels:     False
TensorFlow specific parameters:
    - Input model in text protobuf format:     False
    - Path to model dump for TensorBoard:     None
    - List of shared libraries with TensorFlow custom layers implementation:     None
    - Update the configuration file with input/output node names:     None
    - Use configuration file used to generate the model with Object Detection API:     /home/paul/tf1.15/models/research/object_detection/samples/sample_model_ir/../sample_model_inference/pipeline.config
    - Operations to offload:     None
    - Patterns to offload:     None
    - Use the config file:     None
Model Optimizer version:     2020.1.0-61-gd349c3ba4a
/home/paul/openvino/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/paul/openvino/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/paul/openvino/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/paul/openvino/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/paul/openvino/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/paul/openvino/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
The Preprocessor block has been removed. Only nodes performing mean value subtraction and scaling (if applicable) are kept.
[ ERROR ]  Failed to match nodes from custom replacement description with id 'ObjectDetectionAPISSDPostprocessorReplacement':
It means model and custom replacement description are incompatible.
Try to correct custom replacement description according to documentation with respect to model node names
[ ERROR ]  Cannot infer shapes or values for node ""Postprocessor/Cast_1"".
[ ERROR ]  0
[ ERROR ]<br>
[ ERROR ]  It can happen due to bug in custom shape infer function .
[ ERROR ]  Or because the node inputs have incorrect values/shapes.
[ ERROR ]  Or because input shapes are incorrect (embedded to the model or passed via --input_shape).
[ ERROR ]  Run Model Optimizer with --log_level=DEBUG for more information.
[ ANALYSIS INFO ]  Your model looks like TensorFlow Object Detection API Model.
Check if all parameters are specified:
    --tensorflow_use_custom_operations_config
    --tensorflow_object_detection_api_pipeline_config
    --input_shape (optional)
    --reverse_input_channels (if you convert a model to use with the Inference Engine sample applications)
Detailed information about conversion of this model can be found at
<a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Object_Detection_API_Models.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Object_Detection_API_Models.html</a>
[ ERROR ]  Exception occurred during running replacer ""REPLACEMENT_ID"" (): Stopped shape/value propagation at ""Postprocessor/Cast_1"" node.
 For more information please refer to Model Optimizer FAQ (<a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html</a>), question #38.</p>
","<p>You should use transformation config according to your tensorflow version
e.g. 1.14 or 1.15</p>
<p>For example</p>
<pre><code>mo_tf.py --input_model ../sample_model_inference/frozen_inference_graph.pb --transformations_config /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_support_api_v1.14.json --tensorflow_object_detection_api_pipeline_config ../sample_model_inference/pipeline.config
</code></pre>
<p>OR</p>
<pre><code>mo_tf.py --input_model ../sample_model_inference/frozen_inference_graph.pb --transformations_config /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_support_api_v1.15.json --tensorflow_object_detection_api_pipeline_config ../sample_model_inference/pipeline.config
</code></pre>
","807","1","1","<tensorflow><openvino>"
"60431499","Running an OpenVINO Python script on boot for Raspberry PI","2020-02-27 10:48:04","<p>I am using a Raspberry Pi 3 Model B Rev 1.2 running Raspbian 10 (Buster). I want to run a python script on startup that uses OpenVINO and OpenCV to detect objects and display a stream from a webcam.</p>

<p>I created a shell script <code>launcher.sh</code> that contains</p>

<pre><code>#!/bin/sh

/opt/inte/openvino/bin/setupvars.sh
/usr/bin/python3 /home/pi/project/run.py
</code></pre>

<p>I ran <code>$ chmod 775 launcher.sh</code> and confirmed that the script works with <code>$ sh launcher.sh</code>. </p>

<p>To run the script on start up I used <code>$ sudo crontab -e</code> and added <code>@reboot sh /home/pi/project/launcher.sh &gt;/home/pi/logs/cronlog 2&gt;&amp;1</code> to the bottom.</p>

<p>The script does run on reboot. The logs <strong>show that the OpenVINO environment is initialized</strong>, but the logs also indicate I get a ModuleNotFoundError: No module named 'openvino'. I'm guessing it only works when i run it from the terminal because I have my bash.rc setting up the OpenVINO env each time.</p>

<p>What am I doing wrong? Is there a better way to do this on Buster? </p>
","<p>I wasn't able to resolve my specific issue, but I did manage to find a way to run my script on boot.</p>

<p>I added the following lines the end of my .bashrc,</p>

<pre><code>source /opt/intel/openvino/bin/setupvars.sh
cd /home/pi/project
python3 run.py 
cd 
</code></pre>

<p>to initialize the OpenVINO environment and run my script every time a new terminal is opened, and then I made the LXTerminal run on boot by adding <code>@lxterminal</code> to the end of /etc/xdg/lxsession/LXDE-pi/autostart.</p>

<p>It's a pretty hacky way to do it and impractical if you're planning to use your Pi for anything else. Any advice would still be appreciated</p>
","788","3","2","<python><raspberry-pi><raspberry-pi3><openvino><raspbian-buster>"
"60431499","Running an OpenVINO Python script on boot for Raspberry PI","2020-02-27 10:48:04","<p>I am using a Raspberry Pi 3 Model B Rev 1.2 running Raspbian 10 (Buster). I want to run a python script on startup that uses OpenVINO and OpenCV to detect objects and display a stream from a webcam.</p>

<p>I created a shell script <code>launcher.sh</code> that contains</p>

<pre><code>#!/bin/sh

/opt/inte/openvino/bin/setupvars.sh
/usr/bin/python3 /home/pi/project/run.py
</code></pre>

<p>I ran <code>$ chmod 775 launcher.sh</code> and confirmed that the script works with <code>$ sh launcher.sh</code>. </p>

<p>To run the script on start up I used <code>$ sudo crontab -e</code> and added <code>@reboot sh /home/pi/project/launcher.sh &gt;/home/pi/logs/cronlog 2&gt;&amp;1</code> to the bottom.</p>

<p>The script does run on reboot. The logs <strong>show that the OpenVINO environment is initialized</strong>, but the logs also indicate I get a ModuleNotFoundError: No module named 'openvino'. I'm guessing it only works when i run it from the terminal because I have my bash.rc setting up the OpenVINO env each time.</p>

<p>What am I doing wrong? Is there a better way to do this on Buster? </p>
","<p>Thanks to Mauricio.R from Intel I was able to find a proper solution. </p>

<ol>
<li>Create a script that initializes OpenVINO and launches my python script using <code>nano ~/openvino-app-script</code> with contents:</li>
</ol>

<pre class=""lang-sh prettyprint-override""><code>   #!/bin/bash
   source /opt/intel/openvino/bin/setupvars.sh     
   /usr/bin/python3 /path/to/script/run.py
</code></pre>

<ol start=""2"">
<li><p>Change the bash script's permissions and ownership with <code>chmod u+x ~/openvino-app-script</code>. You should make sure this script works by running it with <code>bash ./openvino-app-script</code></p></li>
<li><p>Create a service file using <code>sudo nano /etc/systemd/system/openvino-app.service</code> with contents </p></li>
</ol>

<pre class=""lang-sh prettyprint-override""><code>    [Unit]
    Description=OpenVINO Python Script
    After=network.target

    [Service]
    Environment=""DISPLAY=:0""
    Environment=""XAUTHORITY=/home/pi/.Xauthority""
    ExecStart=/home/pi/openvino-app-script
    WorkingDirectory=/home/pi
    StandardOutput=inherit
    StandardError=inherit
    Restart=on-failure
    User=pi

    [Install]
    WantedBy=graphical.target
</code></pre>

<ol start=""4"">
<li>Activate the service with <code>sudo systemctl enable openvino-app.service</code></li>
</ol>

<p>You can check the status or disable the service by changing <code>enable</code> to <code>disable</code> or <code>status</code></p>

<p>This solution works great for my project, which displays a video-stream with an overlay using OpenCV and performs inference using an NCS.</p>
","788","3","2","<python><raspberry-pi><raspberry-pi3><openvino><raspbian-buster>"
"54606614","Unable to compile OpenCV with OpenVino inference, cpuid.txt cannot be read","2019-02-09 13:20:22","<p>I am trying to compile OpenCV with OpenVino inference as explained here:</p>

<p><a href=""https://github.com/opencv/opencv/wiki/Intel&#39;s-Deep-Learning-Inference-Engine-backend"" rel=""nofollow noreferrer"">https://github.com/opencv/opencv/wiki/Intel's-Deep-Learning-Inference-Engine-backend</a></p>

<p>but when I try to generate the MSVC (2017) project on windows, I am getting this error:</p>

<pre><code>CMake Error at C:/local/Intel/computer_vision_sdk_2018.5.445/deployment_tools/inference_engine/src/extension/cmake/CPUID.cmake:324 (file):
  file STRINGS file ""C:/local/opencv-build/cpuid.txt"" cannot be read.
Call Stack (most recent call first):
  C:/local/Intel/computer_vision_sdk_2018.5.445/deployment_tools/inference_engine/src/extension/cmake/feature_defs.cmake:17 (include)
  C:/local/Intel/computer_vision_sdk_2018.5.445/deployment_tools/inference_engine/src/extension/CMakeLists.txt:9 (include)
</code></pre>

<p>Tools used:</p>

<ul>
<li>OpenCV 4.0.0 source code </li>
<li>OpenVino 5.0.1</li>
<li>Visual Studio 2017</li>
<li>CMake-Gui 3.13</li>
</ul>
","<p>I think you have installed inappropriate versions for some tools.
Please try to use CMake 3.4 or higher, which is required to build the Intel® Distribution of OpenVINO.</p>

<p>Please try to follow the steps from ""<a href=""https://software.intel.com/en-us/articles/OpenVINO-Install-Windows"" rel=""nofollow noreferrer"">https://software.intel.com/en-us/articles/OpenVINO-Install-Windows</a>"" for installation and verify whether you are able to compile OpenCV with OpenVino inference.</p>
","786","0","3","<c++><opencv><openvino>"
"54606614","Unable to compile OpenCV with OpenVino inference, cpuid.txt cannot be read","2019-02-09 13:20:22","<p>I am trying to compile OpenCV with OpenVino inference as explained here:</p>

<p><a href=""https://github.com/opencv/opencv/wiki/Intel&#39;s-Deep-Learning-Inference-Engine-backend"" rel=""nofollow noreferrer"">https://github.com/opencv/opencv/wiki/Intel's-Deep-Learning-Inference-Engine-backend</a></p>

<p>but when I try to generate the MSVC (2017) project on windows, I am getting this error:</p>

<pre><code>CMake Error at C:/local/Intel/computer_vision_sdk_2018.5.445/deployment_tools/inference_engine/src/extension/cmake/CPUID.cmake:324 (file):
  file STRINGS file ""C:/local/opencv-build/cpuid.txt"" cannot be read.
Call Stack (most recent call first):
  C:/local/Intel/computer_vision_sdk_2018.5.445/deployment_tools/inference_engine/src/extension/cmake/feature_defs.cmake:17 (include)
  C:/local/Intel/computer_vision_sdk_2018.5.445/deployment_tools/inference_engine/src/extension/CMakeLists.txt:9 (include)
</code></pre>

<p>Tools used:</p>

<ul>
<li>OpenCV 4.0.0 source code </li>
<li>OpenVino 5.0.1</li>
<li>Visual Studio 2017</li>
<li>CMake-Gui 3.13</li>
</ul>
","<p>There's a <a href=""https://software.intel.com/en-us/forums/computer-vision/topic/780454#comment-1933042"" rel=""nofollow noreferrer"">forum post</a> over on OpenVino that indicates some hacky solution to this problem. Here's the suggested solution:</p>

<p>In <code>...\Intel\computer_vision_sdk_2018.5.456\deployment_tools\inference_engine\src\extension\cmake\CPUID.cmake</code> (assuming you followed the default OpenVino Toolkit installation, otherwise you'd need to first locate that file from where you installed it), try making the following change on line 251:</p>

<pre><code>// std::ofstream fo(\""cpuid.txt\""); old line
std::ofstream fo(\""${CMAKE_BINARY_DIR}/cpuid.txt\"");
</code></pre>

<p><sup>*Make sure the variable name <code>fo</code> is consistent with the code following it.</sup></p>

<p>This can potentially avoid having inconsistent paths as the line 319 of that same file gets the text file like this:</p>

<pre><code>set(_CPUID_INFO ""${CMAKE_BINARY_DIR}/cpuid.txt"")
</code></pre>

<p>From the stack trace, it seems like your <code>${CMAKE_BINARY_DIR}</code> variable is set to <code>C:/local/opencv-build</code>, so you should expect <code>cpuid.txt</code> to be generated there.</p>
","786","0","3","<c++><opencv><openvino>"
"54606614","Unable to compile OpenCV with OpenVino inference, cpuid.txt cannot be read","2019-02-09 13:20:22","<p>I am trying to compile OpenCV with OpenVino inference as explained here:</p>

<p><a href=""https://github.com/opencv/opencv/wiki/Intel&#39;s-Deep-Learning-Inference-Engine-backend"" rel=""nofollow noreferrer"">https://github.com/opencv/opencv/wiki/Intel's-Deep-Learning-Inference-Engine-backend</a></p>

<p>but when I try to generate the MSVC (2017) project on windows, I am getting this error:</p>

<pre><code>CMake Error at C:/local/Intel/computer_vision_sdk_2018.5.445/deployment_tools/inference_engine/src/extension/cmake/CPUID.cmake:324 (file):
  file STRINGS file ""C:/local/opencv-build/cpuid.txt"" cannot be read.
Call Stack (most recent call first):
  C:/local/Intel/computer_vision_sdk_2018.5.445/deployment_tools/inference_engine/src/extension/cmake/feature_defs.cmake:17 (include)
  C:/local/Intel/computer_vision_sdk_2018.5.445/deployment_tools/inference_engine/src/extension/CMakeLists.txt:9 (include)
</code></pre>

<p>Tools used:</p>

<ul>
<li>OpenCV 4.0.0 source code </li>
<li>OpenVino 5.0.1</li>
<li>Visual Studio 2017</li>
<li>CMake-Gui 3.13</li>
</ul>
","<p>I also met this error, but I changed this line of <strong>CPUID.cmake</strong> as shown in the followng:</p>

<pre><code>if(HAVE_CPUID_INFO)
    set(_CPUID_INFO ""/home/huihui/intel/computer_vision_sdk_2018.4.420/inference_engine/build/cpuid.txt"")
</code></pre>

<p>Now the error is fixed.</p>
","786","0","3","<c++><opencv><openvino>"
"57727665","how to deploy openvino-opencv in Qt","2019-08-30 13:26:39","<p>I want to use openvino-opencv for my Qt (<code>Qt5.7.1</code>) based project. I have downloaded and installed openvino411 (corresponding to opencv411) following the instructions here in windows10 <a href=""https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_windows.html#Configure_MO"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_windows.html#Configure_MO</a>. I write a <code>.pri</code> file to demploy the opencv in Qt: </p>

<pre><code>INCLUDEPATH += C:/openvino-411/openvino_2019.2.275/opencv/include

CONFIG(release, debug|release):{
    LIBS += -LC:/openvino-411/openvino_2019.2.275/opencv/lib \
            -lopencv_core411 -lopencv_highgui411 -lopencv_imgproc411 -lopencv_imgcodecs411 -lopencv_features2d411 -lopencv_ml411 -lopencv_objdetect411 -lopencv_dnn411
}
CONFIG(debug, debug|release):{
    LIBS += -LC:/openvino-411/openvino_2019.2.275/opencv/lib \
            -lopencv_core411d -lopencv_highgui411d -lopencv_imgproc411d -lopencv_imgcodecs411d -lopencv_features2d411d -lopencv_ml411d -lopencv_objdetect411d -lopencv_dnn411d
}
</code></pre>

<p>But it seeems opencv canot be run in Qt, since I tried running the qt program. The popping up cmd window goes directly to ""<code>Press &lt;RETURN&gt; to close this window...</code>"" without doing any actually. </p>
","<p>First of all, keep in mind that OpenVINO for windows is compiled against MSBUILD instead of MinGW, so if your Qt project is compiled using MinGW, OpenVINO pre-built libraries will likely fail during linking</p>

<p>That said, I managed to integrate OpenVINO Inference Engine with OpenCV succesfully in a big and already existent Qt based project (QT 5.13.1), under LINUX (Ubuntu 16.04), it apperas that under Windows the dependencies fragmentation makes it harder</p>

<p>This configuration is quite tricky and also is a work in progress (to me), I am trying to completely isolate OpenVINO dependencies aiming to deploy them completely embedded in our app, anyway like this it works:</p>

<p>First I installed OpenVINO (<a href=""https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_linux.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_linux.html</a>) paying particular attention in following each step precisely as it is described, </p>

<p>also DON'T MISS TO RUN the two examples demo_security_barrier_camera and demo_squeezenet_download_convert_run, they will produce two libraries libcpu_extension.so and libgflags_nothreads.a WITHOUT WHICH OpenVINO WILL NOT WORK UNDER YOUR PROJECT, the reason why it was made this way is unknown to me</p>

<p>I copied the following libraries under a subfolder of my project (ThirdPartyLibraries/OpenVINOInferenceEngine):</p>

<ul>
<li><strong>libinference_engine.so</strong> (found in OpenVINO installation folder: /opt/intel/openvino/inference_engine/lib/intel64/libinference_engine.so)</li>
<li><strong>libtbb.so</strong> (found in OpenVINO installation folder: /opt/intel/openvino/inference_engine/external/tbb/lib/intel64/libtbb.so)</li>
</ul>

<p>for the two ""cpu extension"" libraries, I created a subfolder named ""extension"", so:</p>

<ul>
<li><strong>extension/libgflags_nothreads.a</strong> (found in OpenVINO Inference Engine Demo BUILD FOLDER, for me it is /home/myuser/inference_engine_demos_build/Release/lib/libgflags_nothreads.a)</li>
<li><strong>extension/libcpu_extensio.so</strong> (found in OpenVINO Inference Engine Demo BUILD FOLDER, for me it is /home/myuser/inference_engine_demos_build/Release/lib/libcpu_extensio.so)</li>
</ul>

<p>Then I also copied the includes of Inference Engine and Lib Cpu Extension from their respective installation folders to my ThirdPartyLibraries:</p>

<ul>
<li>All the content found under <strong>/opt/intel/openvino/inference_engine/include/</strong> goes under <strong>/ThirdPartyLibraries/OpenVINOInferenceEngine/include</strong></li>
<li>All the content found under <strong>/opt/intel/openvino/deployment_toos/inference_engine/src/extension/</strong> goes under <strong>/ThirdPartyLibraries/OpenVINOInferenceEngine/extension/include</strong></li>
</ul>

<p>Finally here's my .pri file for Qt:</p>

<pre><code>OPENVINODIR = /home/myuser/code_qt5_HG/Libraries/ThirdPartyLibraries/OpenVINOInferenceEngine

LIBS_OPENVINO  += -L$$OPENVINODIR \
                  -linference_engine \
                  -ltbb \
                  -L$$OPENVINODIR/extension \
                  -lcpu_extension

INCLUDES_OPENVINO  += $$OPENVINODIR/include \
                   += $$OPENVINODIR/extension/include

LIBS += $$LIBS_OPENVINO

INCLUDEEPATH += $$INCLUDES_OPENVINO
</code></pre>

<p>That's it, doing so allows me to reference and use Inference Engine in my project like this:</p>

<pre><code> #include &lt;ie_core.hpp&gt;
 #include &lt;ie_plugin_config.hpp&gt;
 #include &lt;cpp/ie_cnn_net_reader.h&gt;
 #include &lt;ext_list.hpp&gt;

 .....

 InferenceEngine::Core ie;
 ie.AddExtension(std::make_shared&lt;InferenceEngine::Extensions::Cpu::CpuExtensions&gt;(), ""CPU"");
 InferenceEngine::CNNNetReader netReader;
 netReader.ReadNetwork(detectorXmlPath);
 netReader.getNetwork().setBatchSize(1);
 netReader.ReadWeights(detectorBinPath);
 InferenceEngine::InputsDataMap inputInfo(netReader.getNetwork().getInputsInfo());

 .....
</code></pre>

<p>to deploy my App to a third party machine I need to install OpenVINO on the machine following the regular procedure (<a href=""https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_linux.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_linux.html</a>) and to deploy my App as I usually do, the dependencies are then correctly resolved.</p>

<p>My last two cents: I am in direct contact with Intel, which is supporting me with the OpenVINO integration, according to them ""all the .so files in in /deployment_tools/inference_engine/lib/intel64, from /deployment_tools/inference_engine/external/mkltiny_lnx/lib, and /deployment_tools/inference_engine/external/tbb/lib are pretty much all the dependencies required"", I still didn't have the time to confirm that yet</p>
","775","0","1","<qt><opencv><openvino>"
"59807682","How to use intel openvino toolkit in google colab?","2020-01-19 06:22:13","<p>Due to unfortunate reason of cpu, CPU dont support avx instruction and so Openvino toolkit not working in my machine.</p>

<p>I have model in my drive , 
How can i use the openvino run convert the model in IR using google colab.</p>
","<p>You can try some example Google Colab notebook like this one: <a href=""https://colab.research.google.com/drive/1xla23daYYbTIfbdHF0nyHzHyoAvVtyaG"" rel=""nofollow noreferrer"">link</a>. Better to post more specific questions on Stack Overflow regarding how to install as opposed to say asking like an open question. </p>

<p>But if I were you, I would use the official information as a reference since Google Colab runs on a Linux-based hosted machine: <a href=""https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_linux.html"" rel=""nofollow noreferrer"">Official documentation</a>.</p>
","767","0","4","<intel><google-colaboratory><openvino>"
"59807682","How to use intel openvino toolkit in google colab?","2020-01-19 06:22:13","<p>Due to unfortunate reason of cpu, CPU dont support avx instruction and so Openvino toolkit not working in my machine.</p>

<p>I have model in my drive , 
How can i use the openvino run convert the model in IR using google colab.</p>
","<p>I will just put commands here, might explain lines later. I have set up openvino 2019 R3 version with google colab, steps are as follows:</p>
<ol>
<li><p><code>!sudo apt-get install pciutils</code></p>
</li>
<li><p><code>!sudo apt-get update -y &amp;&amp; sudo apt-get install -y cpio</code></p>
</li>
<li><p><code>!lscpu</code></p>
</li>
<li><p><code>!cat /etc/os-release</code></p>
</li>
<li><p><code>!wget http://registrationcenter-download.intel.com/akdlm/irc_nas/16057/l_openvino_toolkit_p_2019.3.376.tgz</code></p>
</li>
<li><p><code>!tar -xvzf l_openvino_toolkit_p_2019.3.376.tgz</code></p>
</li>
<li><p><code>!cd l_openvino_toolkit_p_2019.3.376/ &amp;&amp; bash install.sh</code></p>
</li>
<li><p><code>!source /opt/intel/openvino/bin/setupvars.sh</code></p>
</li>
<li><p><code>!/opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites /install_prerequisites.sh</code></p>
</li>
<li><p><code>!source /opt/intel/openvino/bin/setupvars.sh &amp;&amp; \    /opt/intel/openvino/deployment_tools/demo/demo_squeezenet_download_convert_run.sh</code></p>
</li>
</ol>
","767","0","4","<intel><google-colaboratory><openvino>"
"59807682","How to use intel openvino toolkit in google colab?","2020-01-19 06:22:13","<p>Due to unfortunate reason of cpu, CPU dont support avx instruction and so Openvino toolkit not working in my machine.</p>

<p>I have model in my drive , 
How can i use the openvino run convert the model in IR using google colab.</p>
","<p>This package seems to be a good way to use openvino in google colab:
<a href=""https://github.com/alihussainia/openvino-colab"" rel=""nofollow noreferrer"">https://github.com/alihussainia/openvino-colab</a></p>
<pre><code>!pip install openvino-colab

import openvino_colab
</code></pre>
<p>And you are ready to use openvino at /opt/intel/openvino ...</p>
","767","0","4","<intel><google-colaboratory><openvino>"
"59807682","How to use intel openvino toolkit in google colab?","2020-01-19 06:22:13","<p>Due to unfortunate reason of cpu, CPU dont support avx instruction and so Openvino toolkit not working in my machine.</p>

<p>I have model in my drive , 
How can i use the openvino run convert the model in IR using google colab.</p>
","<p>I managed to run the inference engine on colab. Also, it is possible to create xml and bin files from pretrained models.</p>
<p>Explained here: <a href=""https://medium.com/analytics-vidhya/intel-openvino-on-google-colab-20ac8d2eede6"" rel=""nofollow noreferrer"">https://medium.com/analytics-vidhya/intel-openvino-on-google-colab-20ac8d2eede6</a></p>
","767","0","4","<intel><google-colaboratory><openvino>"
"54644391","How to optimize keras model with batchnorm layers with Intel inference engine (OpenVINO)?","2019-02-12 06:54:55","<p>Failed to optimize keras model with Intel inference engine (OpenVINO toolkit R.5)</p>

<p>I freeze my model just like following <a href=""https://www.dlology.com/blog/how-to-run-keras-model-inference-x3-times-faster-with-cpu-and-intel-openvino-1/"" rel=""nofollow noreferrer"">tutorial</a> suggests. The keras model is trained and tested. I need to optimize it for inference.
However I get an error while running model optimizer (mo.py script) on custom model.</p>

<pre><code>[ ERROR ] shapes (128,9) and (0,) not aligned: 9 (dim 1) != 0 (dim 0)
</code></pre>

<p>Last few layers of my model (9 is number of output of classes) are:</p>

<pre><code>conv2d_4 (Conv2D) (None, 4, 4, 128) 204928 batch_normalization_3[0][0]
__________________________________________________________________________________________________
activation_4 (Activation) (None, 4, 4, 128) 0 conv2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 4, 4, 128) 512 activation_4[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 1, 1, 128) 0 batch_normalization_4[0][0]
__________________________________________________________________________________________________
dropout_2 (Dropout) (None, 1, 1, 128) 0 average_pooling2d_2[0][0]
__________________________________________________________________________________________________
flatten (Flatten) (None, 128) 0 dropout_2[0][0]
__________________________________________________________________________________________________
dense (Dense) (None, 128) 16512 flatten[0][0]
__________________________________________________________________________________________________
activation_5 (Activation) (None, 128) 0 dense[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 128) 512 activation_5[0][0]
__________________________________________________________________________________________________
dropout_3 (Dropout) (None, 128) 0 batch_normalization_5[0][0]
__________________________________________________________________________________________________
dense_1 (Dense) (None, 9) 1161 dropout_3[0][0]
__________________________________________________________________________________________________
color_prediction (Activation) (None, 9) 0 dense_1[0][0]
__________________________________________________________________________________________________
</code></pre>

<p>Model optimizer fails due to presence of BatchNormalization layers. When I remove them it runs successfully. However I freeze graph with</p>

<pre><code>tf.keras.backend.set_learning_phase(0) 
</code></pre>

<p>So nodes like BatchNormalization and Dropout must be removed in freezed graph, I can't figure out why they don't removed.</p>

<p>Thanks a lot!</p>
","<p>I managed to run OpenVINO model optimizer on Keras model with Batch Normalization layers. The model also seemed to converge little faster. Though test classification rate was lower for about 5-7% (and a gap between classification rate on testing and training datasets was bigger) than one of the model without BN. I am not sure if BatchNormalization is properly removed from model in my solution (but openVINO model file doesn't include one so it's removed).</p>

<p>Remove BN and Dropout layers:</p>

<pre><code>#Clear any previous session.
tf.keras.backend.clear_session()
#This line must be executed before loading Keras model.
tf.keras.backend.set_learning_phase(0) 
model = tf.keras.models.load_model(weights_path)

for layer in model.layers:
    layer.training = False
    if isinstance(layer, tf.keras.layers.BatchNormalization):
        layer._per_input_updates = {}
    elif isinstance(layer, tf.keras.layers.Dropout):
        layer._per_input_updates = {}
</code></pre>

<p>And than freeze session:</p>

<pre><code>def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):
""""""
Freezes the state of a session into a pruned computation graph.

Creates a new computation graph where variable nodes are replaced by
constants taking their current value in the session. The new graph will be
pruned so subgraphs that are not necessary to compute the requested
outputs are removed.
@param session The TensorFlow session to be frozen.
@param keep_var_names A list of variable names that should not be frozen,
                    or None to freeze all the variables in the graph.
@param output_names Names of the relevant graph outputs.
@param clear_devices Remove the device directives from the graph for better portability.
@return The frozen graph definition.
""""""
from tensorflow.python.framework.graph_util import convert_variables_to_constants
graph = session.graph
with graph.as_default():
    freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))
    output_names = output_names or []
    output_names += [v.op.name for v in tf.global_variables()]
    # Graph -&gt; GraphDef ProtoBuf
    input_graph_def = graph.as_graph_def()
    if clear_devices:
        for node in input_graph_def.node:
            node.device = """"
    frozen_graph = convert_variables_to_constants(session, input_graph_def,
                                                output_names, freeze_var_names)
    return frozen_graph
</code></pre>
","743","0","1","<tensorflow><keras><inference-engine><openvino>"
"56534725","How to add custom Opencv library into pycharm","2019-06-10 23:40:06","<p>Basically I am developing a project using OpenVino and OpenCV,to do so I cannot use the normal and easy way of using pip to install opencv library but instead Intel provided their own optimized version OpenCV.</p>

<p>I cannot find a place to add the path for the custom OpenCV in pycharm.
If anybody can enlighten me,please do so.</p>

<p>Thank you in advance.</p>
","<p>please try the below steps.</p>

<p>Install Python 2.7.10</p>

<p>Install Pycharm(If not installed previously)</p>

<p>Download the OpenCV executable.</p>

<p>Install OpenCV</p>

<p>Add OpenCV in the system path(%OPENCV_DIR% = /path/of/opencv/directory)</p>

<p>Goto C:\opencv\build\python\2.7\x86 folder and copy cv2.pyd file.</p>

<p>Goto C:\Python27\DLLs directory and paste the cv2.pyd file.</p>

<p>Goto C:\Python27\Lib\site-packages directory and paste the cv2.pyd file.</p>

<p>Goto PyCharm IDE and goto DefaultSettings>PythonInterpreter.</p>

<p>Select the Python which you have installed on Step1.</p>

<p>Install the packages numpy,matplotlib and pip in pycharm.</p>

<p>Restart your PyCharm.</p>

<p>PyCharm now has OpenCV library installed and working.</p>

<p>Hope this will solve your issue</p>
","731","1","1","<opencv><pycharm><openvino>"
"59970309","OpenVino Model Optimizer Error when converting TensorFlow model","2020-01-29 15:26:08","<p>I have created a custom image classification .pb model file using the python scripts in the TensorFlow for Poets 2 repo (<a href=""https://github.com/googlecodelabs/tensorflow-for-poets-2"" rel=""nofollow noreferrer"">https://github.com/googlecodelabs/tensorflow-for-poets-2</a>). </p>

<p>I tried converting it to Intermediate Representation using the OpenVino Model Optimizer using the below scripts:</p>

<p><code>python mo_tf.py --input_model retrained_graph.pb</code></p>

<p><code>python mo_tf.py --input_model retrained_graph.pb --mean_values [127.5,127.5,127.5] --input Mul</code></p>

<p>In both cases this is what happened:</p>

<pre><code>Model Optimizer arguments:
Common parameters:
        - Path to the Input Model:      C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\retrained_graph.pb
        - Path for generated IR:        C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\.
        - IR output name:       retrained_graph
        - Log level:    ERROR
        - Batch:        Not specified, inherited from the model
        - Input layers:         Not specified, inherited from the model
        - Output layers:        Not specified, inherited from the model
        - Input shapes:         Not specified, inherited from the model
        - Mean values:  Not specified
        - Scale values:         Not specified
        - Scale factor:         Not specified
        - Precision of IR:      FP32
        - Enable fusing:        True
        - Enable grouped convolutions fusing:   True
        - Move mean values to preprocess section:       False
        - Reverse input channels:       False
TensorFlow specific parameters:
        - Input model in text protobuf format:  False
        - Path to model dump for TensorBoard:   None
        - List of shared libraries with TensorFlow custom layers implementation:        None
        - Update the configuration file with input/output node names:   None
        - Use configuration file used to generate the model with Object Detection API:  None
        - Operations to offload:        None
        - Patterns to offload:  None
        - Use the config file:  None
Model Optimizer version:        2019.3.0-408-gac8584cb7
[ ERROR ]  -------------------------------------------------
[ ERROR ]  ----------------- INTERNAL ERROR ----------------
[ ERROR ]  Unexpected exception happened.
[ ERROR ]  Please contact Model Optimizer developers and forward the following information:
[ ERROR ]  local variable 'new_attrs' referenced before assignment
[ ERROR ]  Traceback (most recent call last):
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\extractor.py"", line 608, in extract_node_attrs
    supported, new_attrs = extractor(Node(graph, node))
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\pipeline\tf.py"", line 132, in &lt;lambda&gt;
    extract_node_attrs(graph, lambda node: tf_op_extractor(node, check_for_duplicates(tf_op_extractors)))
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\tf\extractor.py"", line 109, in tf_op_extractor
    attrs = tf_op_extractors[op](node)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\tf\extractor.py"", line 65, in &lt;lambda&gt;
    return lambda node: pb_extractor(node.pb)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\tf\extractors\const.py"", line 31, in tf_const_ext
    result['value'] = tf_tensor_content(pb_tensor.dtype, result['shape'], pb_tensor)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\tf\extractors\utils.py"", line 76, in tf_tensor_content
    dtype=type_helper[0]),
UnicodeDecodeError: 'ascii' codec can't decode byte 0xff in position 0: ordinal not in range(128)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\main.py"", line 298, in main
    return driver(argv)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\main.py"", line 247, in driver
    is_binary=not argv.input_model_is_text)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\pipeline\tf.py"", line 132, in tf2nx
    extract_node_attrs(graph, lambda node: tf_op_extractor(node, check_for_duplicates(tf_op_extractors)))
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\extractor.py"", line 614, in extract_node_attrs
    new_attrs['name'] if 'name' in new_attrs else '&lt;UNKNOWN&gt;',
UnboundLocalError: local variable 'new_attrs' referenced before assignment

[ ERROR ]  ---------------- END OF BUG REPORT --------------
[ ERROR ]  ------------------------------------------------- 
</code></pre>

<p>Does anyone know how to fix it?</p>
","<p>Eventually I found a solution that worked for me.</p>

<p>I checked the OpenVino Toolkit documentation and found <a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html"" rel=""nofollow noreferrer"">this</a> (02/03/2020)</p>

<p>Under 'Supported Topologies' there is a list with the topologies that work with the OpenVino model optimizer. When you create a model using TensorFlow for Poets 2, you need make sure you choose an architecture that is supported by the model optimizer.</p>
","701","2","1","<computer-vision><object-recognition><openvino><tensorflow-slim><movidius>"
"59810911","OpenVino: how to add support to FusedBatchNormV3 in model optimizer?","2020-01-19 14:20:39","<p>I am trying to understand how to add support for the TensorFlow layer FusedBatchNormV3 at the model optimizer of OpenVino. I am running on an Ubuntu 18.03 and using Tensorflow 15.</p>

<p>My goal is to do several tests with some pre-trained standard network on the Neural Computer Stick 2, and I am working with ResNet50 by now. I have downloaded the network as follows:</p>

<pre><code>import tensorflow as tf
keras = tf.keras

input_shape = (200,200,3)
model = keras.applications.resnet50.ResNet50(input_shape=input_shape,
                                              include_top=False, 
                                              weights='imagenet')
</code></pre>

<p>After I have frozen <code>model</code> as described in <a href=""https://www.dlology.com/blog/how-to-convert-trained-keras-model-to-tensorflow-and-make-prediction/"" rel=""nofollow noreferrer"">this post</a>.</p>

<p>I am running the model optimizer with the command:</p>

<pre><code>sudo python3 mo.py \
--input_model ~&lt;PATH_TO_MODEL&gt;/model.pb \
--output_dir ~&lt;PATH_TO_MODEL&gt; \
--data_type FP16 -b 1
</code></pre>

<p>But I am getting this error message:</p>

<pre><code>[ ERROR ]  1 elements of 64 were clipped to infinity while converting a blob for node [['conv1_bn_1/cond/FusedBatchNormV3_1/ReadVariableOp_1/Output_0/Data__const']] to &lt;class 'numpy.float16'&gt;. 
 For more information please refer to Model Optimizer FAQ (https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html), question #76. 
[ ERROR ]  List of operations that cannot be converted to Inference Engine IR:
[ ERROR ]      FusedBatchNormV3 (53)
[ ERROR ]          conv1_bn_1/cond/FusedBatchNormV3_1
[ ERROR ]          conv2_block1_0_bn_1/cond/FusedBatchNormV3_1
[ ERROR ]          conv2_block1_1_bn_2/cond/FusedBatchNormV3_1
...
[ ERROR ]          conv5_block3_3_bn_1/cond/FusedBatchNormV3_1
[ ERROR ]  Part of the nodes was not converted to IR. Stopped.
</code></pre>

<p>I have found <a href=""https://stackoverflow.com/questions/58584797/openvino-model-optimizer-errorfusedbatchnormv3"">this forum post</a> suggesting to downgrade TensorFlow to version 13, but after doing so I also have got in another error with the same layer:</p>

<pre><code>[ ERROR ]  Cannot infer shapes or values for node ""conv1_bn_1/cond/FusedBatchNormV3_1"".
[ ERROR ]  Op type not registered 'FusedBatchNormV3' in binary running on &lt;USER&gt;. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
</code></pre>

<p>My current idea is to add support for the FusedBatchNormV3 by using the Sub-Graph replacement introduced in the model optimizer (described in <a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_customize_model_optimizer_Subgraph_Replacement_Model_Optimizer.html"" rel=""nofollow noreferrer"">this official page</a>). I would like to replace the function <code>FusedBatchNormV3</code> by the <code>ScaleShift</code> operation, since <a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Supported_Frameworks_Layers.html#tensorflow_supported_operations_and_the_mapping_to_intermediate_representation_layers"" rel=""nofollow noreferrer"">here</a> <code>FusedBatchNorm</code> is said to be associated to it, but I do not know how to find this <code>ScaleShift</code> object. Can someone please help me?</p>
","<p>Unfortunately, I cannot help with the replacement mechanism but I have another thing which should help.</p>

<p>According to the comment from <a href=""https://github.com/opencv/dldt/issues/352"" rel=""nofollow noreferrer"">https://github.com/opencv/dldt/issues/352</a> you can pretend that FusedBatchNormV3 behaves the same way like FusedBatchNorm and it does not lead to accuracy drop.</p>

<p>I added a patch to model optimizer which implements the behavior described above. Please check it out:
<a href=""https://github.com/ArtemSkrebkov/dldt/tree/askrebko/treat_bnv3_as_bn"" rel=""nofollow noreferrer"">https://github.com/ArtemSkrebkov/dldt/tree/askrebko/treat_bnv3_as_bn</a></p>

<p>I checked inference results on the IR generated (using one picture) and I got the same top-3 as the Keras model gives.</p>

<p>Model optimizer command I used (Not sure about preprocessing parameters):
<code>python3 ./mo_tf.py --input_model ~/workspace/reps/keras_to_tensorflow/resnet-50.pb --input_shape [1,224,224,3] --mean_values [103.939,116.779,123.68]</code></p>

<p>Is that solution OK to you?</p>
","692","2","1","<python><tensorflow><openvino>"
"56701721","How to set environment vars permanently with a shell script in Windows","2019-06-21 10:33:22","<p>I have a .bat I use to set some environment vars before running a few programs.</p>

<p>I'd like to permanently set these environment vars, but I don't want to do it manually if possible. Is there a shortcut here? Is there any flag I can set to permanently add to PATH ? </p>

<p>The code is from setupvars.bat made available with OpenVino : </p>

<pre><code>set ROOT=%~dp0
call :GetFullPath ""%ROOT%\.."" ROOT
set SCRIPT_NAME=%~nx0

set ""INTEL_OPENVINO_DIR=%ROOT%""
set ""INTEL_CVSDK_DIR=%INTEL_OPENVINO_DIR%""

where /q libmmd.dll || echo Warning: libmmd.dll couldn't be found in %%PATH%%. Please check if the redistributable package for Intel(R) C++ Compiler is installed and the library path is added to the PATH environment variable. System reboot can be required to update the system environment.

:: OpenCV
if exist ""%INTEL_OPENVINO_DIR%\opencv\setupvars.bat"" (
call ""%INTEL_OPENVINO_DIR%\opencv\setupvars.bat""
) else (
set ""OpenCV_DIR=%INTEL_OPENVINO_DIR%\opencv\x64\vc14\lib""
set ""PATH=%INTEL_OPENVINO_DIR%\opencv\x64\vc14\bin;%PATH%""
)

:: OpenVX
set ""OPENVX_FOLDER=%INTEL_OPENVINO_DIR%\openvx""
set ""PATH=%INTEL_OPENVINO_DIR%\openvx\bin;%PATH%""

:: Inference Engine
set ""InferenceEngine_DIR=%INTEL_OPENVINO_DIR%\deployment_tools\inference_engine\share""
set ""HDDL_INSTALL_DIR=%INTEL_OPENVINO_DIR%\deployment_tools\inference_engine\external\hddl""
set ""PATH=%INTEL_OPENVINO_DIR%\deployment_tools\inference_engine\bin\intel64\Release;%INTEL_OPENVINO_DIR%\deployment_tools\inference_engine\bin\intel64\Debug;%HDDL_INSTALL_DIR%\bin;%PATH%""
if exist ""%INTEL_OPENVINO_DIR%\deployment_tools\inference_engine\bin\intel64\arch_descriptions"" (
set ""ARCH_ROOT_DIR=%INTEL_OPENVINO_DIR%\deployment_tools\inference_engine\bin\intel64\arch_descriptions""
)
:: Check if Python is installed
python --version 2&gt;NUL
if errorlevel 1 (
   echo Error^: Python is not installed. Please install Python 3.5. or 3.6  ^(64-bit^) from https://www.python.org/downloads/
   exit /B 1
)

:: Check Python version
for /F ""tokens=* USEBACKQ"" %%F IN (`python --version 2^&gt;^&amp;1`) DO (
   set version=%%F
)
echo %var%

for /F ""tokens=1,2,3 delims=. "" %%a in (""%version%"") do (
   set Major=%%b
   set Minor=%%c
)

if ""%Major%"" geq ""3"" (
   if ""%Minor%"" geq ""5"" (
      set python_ver=okay
   )
   if ""%Minor%"" geq ""6"" (
     set python_ver=okay
   )
)

if not ""%python_ver%""==""okay"" (
   echo Unsupported Python version. Please install Python 3.5 or 3.6  ^(64-bit^) from https://www.python.org/downloads/
   exit /B 1
)

:: Check Python bitness
python -c ""import sys; print(64 if sys.maxsize &gt; 2**32 else 32)"" 2 &gt; NUL
if errorlevel 1 (
   echo Error^: Error during installed Python bitness detection
   exit /B 1
)

for /F ""tokens=* USEBACKQ"" %%F IN (`python -c ""import sys; print(64 if sys.maxsize &gt; 2**32 else 32)"" 2^&gt;^&amp;1`) DO (
   set bitness=%%F
)

if not ""%bitness%""==""64"" (
   echo Unsupported Python bitness. Please install Python 3.5 or 3.6  ^(64-bit^) from https://www.python.org/downloads/
   exit /B 1
)

set PYTHONPATH=%INTEL_OPENVINO_DIR%\python\python%Major%.%Minor%;%PYTHONPATH%

echo PYTHONPATH=%PYTHONPATH%

echo [setupvars.bat] OpenVINO environment initialized

exit /B 0

:GetFullPath
SET %2=%~f1

GOTO :EOF
</code></pre>
","<p>For OpenVINO, you need to run the setupvars.sh script every time you open a command line. However, you can also add all the variables needed to your systems environment variables.</p>
<p>On your Windows® 10 system, go to Control Panel &gt; System and Security &gt; System &gt; Advanced System Settings &gt; Environment Variables.</p>
<p>See <a href=""https://www.intel.com/content/www/us/en/support/articles/000033440/software/development-software.html"" rel=""nofollow noreferrer"">this document</a> for a list of required variables and their values.</p>
","676","1","1","<windows><batch-file><windows-10><environment-variables><openvino>"
"54891713","Running Facenet using OpenVINO","2019-02-26 18:14:00","<p>I am stuck at a problem using OpenVINO. I am trying to run the facenet after converting model using OpenVINO toolkit but I am unable to use <strong>.npy</strong> and <strong>.pickle</strong> for complete face recognition. I am successful in converting <strong>.pb</strong> file to <strong>.bin</strong> and <strong>.xml</strong> file using OpenVino toolkit.</p>
","<p>openvino converts the model to intermediate representation, which is compatible across multiple hardwares. It can improve the performance of your model too. Since you have already mentioned that yopu were ableto convert your model to IR format, the next phase is inference for which you can use the .xml and .bin files.
Can you please state what exactly do you intend to carryout with .npy or .pickle files?</p>
","619","0","2","<python><tensorflow><openvino>"
"54891713","Running Facenet using OpenVINO","2019-02-26 18:14:00","<p>I am stuck at a problem using OpenVINO. I am trying to run the facenet after converting model using OpenVINO toolkit but I am unable to use <strong>.npy</strong> and <strong>.pickle</strong> for complete face recognition. I am successful in converting <strong>.pb</strong> file to <strong>.bin</strong> and <strong>.xml</strong> file using OpenVino toolkit.</p>
","<p>You can use Python APIs to integrate OV inference into your Python application. Please see inference_engine/samples/python_samples folder for existing Python samples.</p>
","619","0","2","<python><tensorflow><openvino>"
"57040760","Cannot open ip camera with opencv4.1.0-openvino on respberry pi 4","2019-07-15 13:25:19","<p>I need to access video stream from AXIS M1125 Network Camera with my raspberry pi 4. </p>

<p>I have a code that works on my laptop and raspberry pi 3. I use opencv 4.1 which comes in openvino distribution for raspbian.</p>

<pre class=""lang-py prettyprint-override""><code>camera = cv2.VideoCapture('http://192.168.1.38/axis-cgi/jpg/image.cgi')

</code></pre>

<p>When I run the code and debug OPENCV_VIDEOCAPTURE_DEBUG the output is:</p>

<pre><code>[ WARN:0] VIDEOIO(FFMPEG): trying capture filename='http://192.168.1.38/mjpg/1/video.mjpg' ...
[ WARN:0] VIDEOIO(FFMPEG): backend is not available (plugin is missing, or can't be loaded due dependencies or it is not compatible)
[ WARN:0] VIDEOIO(GSTREAMER): trying capture filename='http://192.168.1.38/mjpg/1/video.mjpg' ...
(python3:6939): GStreamer-CRITICAL **: 14:32:38.521: 
Trying to dispose element appsink0, but it is in READY instead of the NULL state.
You need to explicitly set elements to the NULL state before
dropping the final reference, to allow them to clean up.
This problem may also be caused by a refcounting bug in the
application or some element.
...
[ WARN:0] VIDEOIO(GSTREAMER): can't create capture
[ WARN:0] VIDEOIO(V4L2): trying capture filename='http://192.168.1.38/mjpg/1/video.mjpg' ...
[ WARN:0] VIDEOIO(V4L2): can't create capture
[ WARN:0] VIDEOIO(CV_IMAGES): trying capture filename='http://192.168.1.38/mjpg/1/video.mjpg' ...
[ WARN:0] VIDEOIO(CV_IMAGES): created, isOpened=0
[ WARN:0] VIDEOIO(CV_MJPEG): trying capture filename='http://192.168.1.38/mjpg/1/video.mjpg' ...
[ WARN:0] VIDEOIO(CV_MJPEG): can't create capture
</code></pre>

<p>The output from cv2.getBuildInformation():</p>

<pre><code>Platform:
    Timestamp:                   2019-03-19T16:11:44Z
    Host:                        Linux 4.13.0-45-generic x86_64
    Target:                      Linux 1 arm
    CMake:                       3.7.2
    CMake generator:             Ninja
    CMake build tool:            /usr/bin/ninja
    Configuration:               Release

Video I/O:
    FFMPEG:                      YES
      avcodec:                   YES (57.64.101)
      avformat:                  YES (57.56.101)
      avutil:                    YES (55.34.101)
      swscale:                   YES (4.2.100)
      avresample:                NO
    GStreamer:                   YES (1.10.4)
    v4l/v4l2:                    YES (linux/videodev2.h)
</code></pre>
","<p>Have you completed installing all the dependencies? under <code>/opt/intel/openvino/install_dependencies</code>. I also suggest to check running the demo application if it is running and you still have the problem. </p>
","617","1","2","<opencv><raspberry-pi><ip-camera><openvino>"
"57040760","Cannot open ip camera with opencv4.1.0-openvino on respberry pi 4","2019-07-15 13:25:19","<p>I need to access video stream from AXIS M1125 Network Camera with my raspberry pi 4. </p>

<p>I have a code that works on my laptop and raspberry pi 3. I use opencv 4.1 which comes in openvino distribution for raspbian.</p>

<pre class=""lang-py prettyprint-override""><code>camera = cv2.VideoCapture('http://192.168.1.38/axis-cgi/jpg/image.cgi')

</code></pre>

<p>When I run the code and debug OPENCV_VIDEOCAPTURE_DEBUG the output is:</p>

<pre><code>[ WARN:0] VIDEOIO(FFMPEG): trying capture filename='http://192.168.1.38/mjpg/1/video.mjpg' ...
[ WARN:0] VIDEOIO(FFMPEG): backend is not available (plugin is missing, or can't be loaded due dependencies or it is not compatible)
[ WARN:0] VIDEOIO(GSTREAMER): trying capture filename='http://192.168.1.38/mjpg/1/video.mjpg' ...
(python3:6939): GStreamer-CRITICAL **: 14:32:38.521: 
Trying to dispose element appsink0, but it is in READY instead of the NULL state.
You need to explicitly set elements to the NULL state before
dropping the final reference, to allow them to clean up.
This problem may also be caused by a refcounting bug in the
application or some element.
...
[ WARN:0] VIDEOIO(GSTREAMER): can't create capture
[ WARN:0] VIDEOIO(V4L2): trying capture filename='http://192.168.1.38/mjpg/1/video.mjpg' ...
[ WARN:0] VIDEOIO(V4L2): can't create capture
[ WARN:0] VIDEOIO(CV_IMAGES): trying capture filename='http://192.168.1.38/mjpg/1/video.mjpg' ...
[ WARN:0] VIDEOIO(CV_IMAGES): created, isOpened=0
[ WARN:0] VIDEOIO(CV_MJPEG): trying capture filename='http://192.168.1.38/mjpg/1/video.mjpg' ...
[ WARN:0] VIDEOIO(CV_MJPEG): can't create capture
</code></pre>

<p>The output from cv2.getBuildInformation():</p>

<pre><code>Platform:
    Timestamp:                   2019-03-19T16:11:44Z
    Host:                        Linux 4.13.0-45-generic x86_64
    Target:                      Linux 1 arm
    CMake:                       3.7.2
    CMake generator:             Ninja
    CMake build tool:            /usr/bin/ninja
    Configuration:               Release

Video I/O:
    FFMPEG:                      YES
      avcodec:                   YES (57.64.101)
      avformat:                  YES (57.56.101)
      avutil:                    YES (55.34.101)
      swscale:                   YES (4.2.100)
      avresample:                NO
    GStreamer:                   YES (1.10.4)
    v4l/v4l2:                    YES (linux/videodev2.h)
</code></pre>
","<p>Regardless of the function of OpenVINO, it can be processed by OpenCV.</p>

<pre><code>import cv2
cap = cv2.VideoCapture(""http://username:password@xxx.xxx.xxx.xxx:port/xxxx"")
#cap = cv2.VideoCapture(""http://username:password@xxx.xxx.xxx.xxx:port"")
while(True):
ret, frame = cap.read()
cv2.imshow('frame',frame)
if cv2.waitKey(1) &amp; 0xFF == ord('q'):
break
cap.release()
cv2.destroyAllWindows()
</code></pre>

<p>Please refer <a href=""https://software.intel.com/en-us/forums/computer-vision/topic/801714"" rel=""nofollow noreferrer"">https://software.intel.com/en-us/forums/computer-vision/topic/801714</a></p>

<p>Hope this helps.</p>
","617","1","2","<opencv><raspberry-pi><ip-camera><openvino>"
"54018696","Using model optimizer for tensorflow slim models","2019-01-03 08:28:33","<p>I am aiming to inference tensorflow slim model with Intel OpenVINO optimizer. Using <a href=""https://software.intel.com/en-us/articles/OpenVINO-Using-TensorFlow#inpage-nav-9"" rel=""nofollow noreferrer"">open vino docs</a> and <a href=""http://www.delta-course.org/docs/delta10/DELTA10D3L2.pdf"" rel=""nofollow noreferrer"">slides</a> for inference and <a href=""https://github.com/tensorflow/models/blob/master/research/slim/README.md"" rel=""nofollow noreferrer"">tf slim docs</a> for training model.</p>

<p>It's a multi-class classification problem. I have trained tf slim mobilnet_v2 model from scratch <em>(using sript train_image_classifier.py)</em>. Evaluation of trained model on test set gives relatively good results to begin with <em>(using script eval_image_classifier.py)</em>:</p>

<p><em>eval/Accuracy[0.8017]eval/Recall_5[0.9993]</em></p>

<p>However, single <code>.ckpt</code> file is not saved (even though at the end of train_image_classifier.py run there is a message like <em>""model.ckpt is saved to checkpoint_dir""</em>), there are 3 files (<code>.ckpt-180000.data-00000-of-00001</code>, <code>.ckpt-180000.index</code>, <code>.ckpt-180000.meta</code>) instead.</p>

<p>OpenVINO model optimizer requires a single checkpoint file.</p>

<p>According to <a href=""https://software.intel.com/en-us/articles/OpenVINO-Using-TensorFlow#inpage-nav-9"" rel=""nofollow noreferrer"">docs</a> I call <em>mo_tf.py</em> with following params:</p>

<pre><code>python mo_tf.py --input_model D:/model/mobilenet_v2_224.pb --input_checkpoint D:/model/model.ckpt-180000 -b 1
</code></pre>

<p>It gives the error (same if pass --input_checkpoint D:/model/model.ckpt):</p>

<pre><code>[ ERROR ]  The value for command line parameter ""input_checkpoint"" must be existing file/directory,  but ""D:/model/model.ckpt-180000"" does not exist.
</code></pre>

<p>Error message is clear, there are not such files on disk. But as I know most tf utilities convert .ckpt-????.meta to .ckpt under the hood.</p>

<p>Trying to call:</p>

<pre><code>python mo_tf.py --input_model D:/model/mobilenet_v2_224.pb --input_meta_graph D:/model/model.ckpt-180000.meta -b 1
</code></pre>

<p>Causes:</p>

<pre><code>[ ERROR ]  Unknown configuration of input model parameters
</code></pre>

<p>It doesn't matter for me in which way I will transfer graph to OpenVINO intermediate representation, just need to reach that result.</p>

<p>Thanks a lot.</p>

<p><strong>EDIT</strong></p>

<p>I managed to run OpenVINO model optimizer on frozen graph of tf slim model. However I still have no idea why had my previous attempts (based on docs) failed.</p>
","<p>you can try converting the model to frozen format (.pb) and then convert the model using OpenVINO. </p>

<p>.ckpt-meta has the metagraph. The computation graph structure without variable values.
the one you can observe in tensorboard.</p>

<p>.ckpt-data has the variable values,without the skeleton or structure. to restore a  model we need both meta and data files.</p>

<p>.pb file saves the whole graph (meta+data)</p>

<p>As per the documentation of OpenVINO:</p>

<p>When a network is defined in Python* code, you have to create an inference graph file. Usually, graphs are built in a form that allows model training. That means that all trainable parameters are represented as variables in the graph. To use the graph with the Model Optimizer, it should be frozen.
<a href=""https://software.intel.com/en-us/articles/OpenVINO-Using-TensorFlow"" rel=""nofollow noreferrer"">https://software.intel.com/en-us/articles/OpenVINO-Using-TensorFlow</a></p>

<p>the OpenVINO optimizes the model by converting the weighted graph passed in frozen form. </p>
","610","1","1","<tensorflow><tf-slim><tensorflow-slim><inference-engine><openvino>"
"61954099","Run multiple networks on the same Intel Neural Compute Stick 2 (NCS2/MYRIAD)?","2020-05-22 11:34:41","<p>I want to load and <strong>run multiple networks on the same NCS2</strong>: a one-class object detection network (like a person detector), and a network for some recognition on that detection (like gesture recognition).
I tried to load the networks on one NCS2 through two different threads. But when loading the second network, the program exits without any warning or error; the networks are separately working fine (one at a time).</p>

<p>I am using <strong>Python</strong> on <em>Raspberry pi 4/Raspbian Buster</em>, and the networks are in <strong>IR (xml + bin)</strong> format.</p>

<ol>
<li>Is it possible to load multiple networks on the same NCS2 at all?</li>
<li>If yes, what do I miss? Do I have to do some configuration or so?</li>
</ol>
","<p>Yes. It is possible. No specific configuration actions required. </p>

<p>There are examples of such functionality in a repo <a href=""https://github.com/opencv/open_model_zoo"" rel=""nofollow noreferrer"">open-model-zoo</a>.
For example, <a href=""https://github.com/opencv/open_model_zoo/tree/master/demos/python_demos/action_recognition"" rel=""nofollow noreferrer"">this one</a>. Action recognition demo based on two networks. The demo is implemented using Python.</p>

<p>Any chance to share source code of your app? It would be a lot easier to understand what may go wrong.</p>
","588","2","1","<raspbian><openvino><movidius>"
"55488687","Is there a guide on Openvino Inference Engine for Windows os?","2019-04-03 06:44:28","<p>What ever I do, I cant get any inference engine samples from Openvino toolkit to run at all. I set all the paths, build ALLBUILD debug and release for every sample, but I cant run the pedestrian crossing demo not matter where I put my video that I downloaded from! I too cant find any tutorials for Windows OS since everyone is using linux. Please help?</p>
","<p>You can refer the below link ,which helps you to implement OpenVINO on Windows.
<a href=""https://software.intel.com/en-us/articles/OpenVINO-Install-Windows"" rel=""nofollow noreferrer"">https://software.intel.com/en-us/articles/OpenVINO-Install-Windows</a></p>

<p>For more information you can refer the documentation present within openvino which has further links for inference engine.</p>

<p>You can find the documentation from the below path in your system.</p>

<p>C:/Intel/computer_vision_sdk_/deployment_tools/documentation/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html </p>
","571","0","1","<visual-studio-2017><openvino>"
"56767629","How to convert VGG from darknet to tensorflow?","2019-06-26 07:38:31","<p>I am trying to convert an existing VGG model (trained on 1 class) from Darknet format (including a .cfg file and .weights file) to the TensorFlow format. The end goal is to use the files in TensorFlow format with the Intel OpenVINO Toolkit.</p>

<p>As an initial attempt, I have tried using the <a href=""https://github.com/jinyu121/DW2TF"" rel=""nofollow noreferrer"">DW2TF</a> but encountered errors while running the program.</p>

<p>I first attempted with the official files provided on <a href=""https://pjreddie.com/darknet/imagenet/"" rel=""nofollow noreferrer"">pjreddie's website</a> and was met with the following error.</p>

<p>I am not specifically looking for a direct solution to the error below, instead I am looking for the best way I can convert VGG in Darknet format to TensorFlow, or any other pathways so that I can use the model with the Intel OpenVINO Toolkit. Any comments or suggestions are welcome.</p>

<p>Thanks for reading.</p>

<hr>

<p>Command used:</p>

<pre><code>python3 main.py\
--cfg data/vgg-16.cfg\
--weights data/vgg-16.weights\
--output output\
--gpu 0
</code></pre>

<p>Error received:</p>

<pre><code>0 Tensor(""network/net1:0"", shape=(?, 256, 256, 3), dtype=float32)
=&gt; Ignore:  {'name': 'crop', 'crop_height': '224', 'crop_width': '224', 'flip': '1', 'exposure': '1', 'saturation': '1', 'angle': '0'}
1 Tensor(""network/net1:0"", shape=(?, 256, 256, 3), dtype=float32)
2 Tensor(""network/convolutional1/Activation:0"", shape=(?, 256, 256, 64), dtype=float32)
3 Tensor(""network/convolutional2/Activation:0"", shape=(?, 256, 256, 64), dtype=float32)
4 Tensor(""network/maxpool1/MaxPool:0"", shape=(?, 128, 128, 64), dtype=float32)
5 Tensor(""network/convolutional3/Activation:0"", shape=(?, 128, 128, 128), dtype=float32)
6 Tensor(""network/convolutional4/Activation:0"", shape=(?, 128, 128, 128), dtype=float32)
7 Tensor(""network/maxpool2/MaxPool:0"", shape=(?, 64, 64, 128), dtype=float32)
8 Tensor(""network/convolutional5/Activation:0"", shape=(?, 64, 64, 256), dtype=float32)
9 Tensor(""network/convolutional6/Activation:0"", shape=(?, 64, 64, 256), dtype=float32)
10 Tensor(""network/convolutional7/Activation:0"", shape=(?, 64, 64, 256), dtype=float32)
11 Tensor(""network/maxpool3/MaxPool:0"", shape=(?, 32, 32, 256), dtype=float32)
12 Tensor(""network/convolutional8/Activation:0"", shape=(?, 32, 32, 512), dtype=float32)
13 Tensor(""network/convolutional9/Activation:0"", shape=(?, 32, 32, 512), dtype=float32)
14 Tensor(""network/convolutional10/Activation:0"", shape=(?, 32, 32, 512), dtype=float32)
15 Tensor(""network/maxpool4/MaxPool:0"", shape=(?, 16, 16, 512), dtype=float32)
16 Tensor(""network/convolutional11/Activation:0"", shape=(?, 16, 16, 512), dtype=float32)
17 Tensor(""network/convolutional12/Activation:0"", shape=(?, 16, 16, 512), dtype=float32)
18 Tensor(""network/convolutional13/Activation:0"", shape=(?, 16, 16, 512), dtype=float32)
19 Tensor(""network/maxpool5/MaxPool:0"", shape=(?, 8, 8, 512), dtype=float32)
=&gt; Ignore:  {'name': 'connected', 'output': '4096', 'activation': 'relu'}
20 Tensor(""network/maxpool5/MaxPool:0"", shape=(?, 8, 8, 512), dtype=float32)
=&gt; Ignore:  {'name': 'dropout', 'probability': '.5'}
21 Tensor(""network/maxpool5/MaxPool:0"", shape=(?, 8, 8, 512), dtype=float32)
=&gt; Ignore:  {'name': 'connected', 'output': '4096', 'activation': 'relu'}
22 Tensor(""network/maxpool5/MaxPool:0"", shape=(?, 8, 8, 512), dtype=float32)
=&gt; Ignore:  {'name': 'dropout', 'probability': '.5'}
23 Tensor(""network/maxpool5/MaxPool:0"", shape=(?, 8, 8, 512), dtype=float32)
=&gt; Ignore:  {'name': 'connected', 'output': '1000', 'activation': 'linear'}
24 Tensor(""network/maxpool5/MaxPool:0"", shape=(?, 8, 8, 512), dtype=float32)
Traceback (most recent call last):
  File ""/home/acusensus/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1628, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[1], expected a dimension of 1, got 8 for 'network/softmax1/Squeeze' (op: 'Squeeze') with input shapes: [?,8,8,512].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 112, in &lt;module&gt;
    main(args)
  File ""main.py"", line 53, in main
    parse_net(args.layers, args.cfg, args.weights, args.training)
  File ""main.py"", line 33, in parse_net
    training=training, const_inits=const_inits, verbose=verbose)
  File ""/home/acusensus/usb_accel_proj/DW2TF/util/cfg_layer.py"", line 198, in get_cfg_layer
    layer = _cfg_layer_dict.get(layer_name, cfg_ignore)(B, H, W, C, net, param, weights_walker, stack, output_index, scope, training, const_inits, verbose)
  File ""/home/acusensus/usb_accel_proj/DW2TF/util/cfg_layer.py"", line 169, in cfg_softmax
    net = tf.squeeze(net, axis=[1, 2], name=scope+'/Squeeze')
  File ""/home/acusensus/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/acusensus/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 2573, in squeeze
    return gen_array_ops.squeeze(input, axis, name)
  File ""/home/acusensus/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 8236, in squeeze
    ""Squeeze"", input=input, squeeze_dims=axis, name=name)
  File ""/home/acusensus/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/acusensus/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/acusensus/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/home/acusensus/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1792, in __init__
    control_input_ops)
  File ""/home/acusensus/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1631, in _create_c_op
    raise ValueError(str(e))
ValueError: Can not squeeze dim[1], expected a dimension of 1, got 8 for 'network/softmax1/Squeeze' (op: 'Squeeze') with input shapes: [?,8,8,512].

</code></pre>
","<p>I suggest you to look at below tutorial for a good guidance for converting Yolov3 for OpenVINO xml,bin format.</p>

<ul>
<li><a href=""https://github.com/intel-iot-devkit/smart-video-workshop/blob/master/object-detection/README_yolov3.md"" rel=""nofollow noreferrer"">https://github.com/intel-iot-devkit/smart-video-workshop/blob/master/object-detection/README_yolov3.md</a> </li>
</ul>

<p>You should also check your version of Tensorflow, try downgrading it to 1.12. Methods in this tutorial worked for me. </p>

<p>Idea is to get weight conversion tool from</p>

<ul>
<li><a href=""https://github.com/mystic123/tensorflow-yolo-v3"" rel=""nofollow noreferrer"">https://github.com/mystic123/tensorflow-yolo-v3</a></li>
</ul>

<p>Then, convert it using the convert_weights_pb.py script to make it ready for IR format conversion. </p>
","556","0","1","<tensorflow><yolo><vgg-net><openvino>"
"59953314","OpenVINO GPU performance optimization","2020-01-28 16:42:42","<p>I'm trying to speed up the inference on a people counter application, in order to use the GPU I've set the inference engine configuration setting as described: </p>

<pre><code>device_name = ""GPU""
ie.SetConfig({ {PluginConfigParams::KEY_CONFIG_FILE, ""./cldnn_global_custom_kernels/cldnn_global_custom_kernels.xml""} }, device_name);
</code></pre>

<p>and loading the network on the inference engine I've set the target device like described below: </p>

<pre><code>CNNNetwork net = netReader.getNetwork();

TargetDevice t_device = InferenceEngine::TargetDevice::eGPU; 

network.setTargetDevice(t_device);

const std::map&lt;std::string, std::string&gt; dyn_config = { { PluginConfigParams::KEY_DYN_BATCH_ENABLED, PluginConfigParams::YES } };

ie_.LoadNetwork(network,device_name, dyn_config);
</code></pre>

<p>but the inference engine use the CPU yet, and this slow down the inference time. There is a way to use the Intel GPU at maximum power to do inference on a particular network? I'm using the person-detection-retail-0013 model. </p>

<p>Thank's. </p>
","<p>Have you meant person-detection-retail-0013? Because I haven't found pedestrian-detection-retail-013 in open_model_zoo repo.</p>

<p>This might be expected that you see a slowdown while using GPU. The network, you tested, has the following layers as part of the network topology: PriorBox, DetectionOutput . Those layers are executed on CPU as documentation says: <a href=""https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_CL_DNN.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_CL_DNN.html</a>
I have a guess that this may be the reason of the slowdown.</p>

<p>But to be 100% percent sure I would suggest to run benchmark_app tool to do bench-marking of the model. This tool can print detailed performance information about each layer. It should help to shed light what is the real root cause of the slowdown. More information about benchmark_app can be found here: <a href=""https://docs.openvinotoolkit.org/latest/_inference_engine_samples_benchmark_app_README.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_inference_engine_samples_benchmark_app_README.html</a></p>

<p>PS: Just a piece of advice regarding usage of IE API. <code>network.setTargetDevice(t_device);</code> - setTargetDevice is a deprecated method. It is enough to set a device using <code>LoadNetwork</code> like in your example: <code>ie_.LoadNetwork(network,device_name, dyn_config);</code></p>

<p>Hope it will help.</p>
","548","1","1","<c++><computer-vision><inference><openvino>"
"57744633","Interpretation script for Yolo's region based output to openvino","2019-09-01 09:02:39","<p>Hello I am new to OPENCV/CVAT, I use openvino to run auto annotation, I want to use YoloV3 for this mission.</p>

<p>I need to convert Yolo model to OpenVINO format for opencv/cvat/auto_annotation.
 <a href=""https://github.com/opencv/cvat/tree/develop/cvat/apps/auto_annotation"" rel=""nofollow noreferrer"">https://github.com/opencv/cvat/tree/develop/cvat/apps/auto_annotation</a>.</p>

<p>To annotate a task with a custom model I need to prepare 4 files:</p>

<ol>
<li>Model config (*.xml) - a text file with network configuration.</li>
<li>Model weights (*.bin) - a binary file with trained weights.</li>
<li>Label map (*.json) - a simple json file with label_map dictionary
like object with string values for label numbers.</li>
<li>Interpretation script (*.py) - a file used to convert net output
layer to a predefined structure which can be processed by CVAT. This
code will be run inside a restricted python's environment, but it's
possible to use some builtin functions like str, int, float, max,
min, range.</li>
</ol>

<p>I converted Yolo model to OpenVINO format and created xml and bin files. I write the mapping lson file.
Now I need to write interpretation python script for Yolo's region based output. How can I do that?
Is there an interrupt file from tensorflow models to openvino?</p>
","<p>Recommend you to start with Yolo V3 C++ or Python sample.</p>

<p>For C++:</p>

<p><a href=""https://github.com/opencv/open_model_zoo/tree/master/demos/object_detection_demo_yolov3_async"" rel=""nofollow noreferrer"">https://github.com/opencv/open_model_zoo/tree/master/demos/object_detection_demo_yolov3_async</a></p>

<p>For python sample:</p>

<p><a href=""https://github.com/opencv/open_model_zoo/tree/master/demos/python_demos/object_detection_demo_yolov3_async"" rel=""nofollow noreferrer"">https://github.com/opencv/open_model_zoo/tree/master/demos/python_demos/object_detection_demo_yolov3_async</a></p>

<p>Similar discussion can be found in <a href=""https://software.intel.com/en-us/forums/computer-vision/topic/816875"" rel=""nofollow noreferrer"">https://software.intel.com/en-us/forums/computer-vision/topic/816875</a></p>
","538","0","2","<python><opencv><tensorflow><yolo><openvino>"
"57744633","Interpretation script for Yolo's region based output to openvino","2019-09-01 09:02:39","<p>Hello I am new to OPENCV/CVAT, I use openvino to run auto annotation, I want to use YoloV3 for this mission.</p>

<p>I need to convert Yolo model to OpenVINO format for opencv/cvat/auto_annotation.
 <a href=""https://github.com/opencv/cvat/tree/develop/cvat/apps/auto_annotation"" rel=""nofollow noreferrer"">https://github.com/opencv/cvat/tree/develop/cvat/apps/auto_annotation</a>.</p>

<p>To annotate a task with a custom model I need to prepare 4 files:</p>

<ol>
<li>Model config (*.xml) - a text file with network configuration.</li>
<li>Model weights (*.bin) - a binary file with trained weights.</li>
<li>Label map (*.json) - a simple json file with label_map dictionary
like object with string values for label numbers.</li>
<li>Interpretation script (*.py) - a file used to convert net output
layer to a predefined structure which can be processed by CVAT. This
code will be run inside a restricted python's environment, but it's
possible to use some builtin functions like str, int, float, max,
min, range.</li>
</ol>

<p>I converted Yolo model to OpenVINO format and created xml and bin files. I write the mapping lson file.
Now I need to write interpretation python script for Yolo's region based output. How can I do that?
Is there an interrupt file from tensorflow models to openvino?</p>
","<p>This is now bundled with CVAT. The interpret script and json file can be found in the repo here: <a href=""https://github.com/opencv/cvat/tree/develop/utils/open_model_zoo/yolov3"" rel=""nofollow noreferrer"">https://github.com/opencv/cvat/tree/develop/utils/open_model_zoo/yolov3</a></p>
","538","0","2","<python><opencv><tensorflow><yolo><openvino>"
"56483931","How to run environment initialization shell script from Dockerfile","2019-06-06 19:27:51","<p>I am trying to build an API wrapped in a docker image that serves Openvino model. How do I run the ""setupvars.sh"" from Dockerfile itself so that my application can access it?  </p>

<p>I have tried running the script using RUN. For ex:  RUN /bin/bash setupvars.sh
or RUN ./setupvars.sh . However, none of them work and I get ModelNotFoundError: no module named openvino</p>

<pre><code>RUN $INSTALL_DIR/install_dependencies/install_openvino_dependencies.sh
RUN cd /opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites &amp;&amp; sudo ./install_prerequisites_tf.sh
COPY . /app
WORKDIR /app
RUN apt autoremove -y &amp;&amp; \
    rm -rf /openvino /var/lib/apt/lists/*
RUN /bin/bash -c ""source $INSTALL_DIR/bin/setupvars.sh""
RUN echo ""source $INSTALL_DIR/bin/setupvars.sh"" &gt;&gt; /root/.bashrc
CMD [""/bin/bash""]
RUN python3 -m pip install opencv-python
RUN python3 test.py
</code></pre>

<p>I want OpenVino accessible to my gunicorn application that will serve the model in a docker image</p>
","<p>Next commands works for me.</p>

<pre><code>ARG OPENVINO_DIR=/opt/intel/computer_vision_sdk


# Unzip the OpenVINO installer
RUN cd ${APP_DIR} &amp;&amp; tar -xvzf l_openvino_toolkit*

# installing OpenVINO dependencies
RUN cd ${APP_DIR}/l_openvino_toolkit* &amp;&amp; \
    ./install_cv_sdk_dependencies.sh

# installing OpenVINO itself
RUN cd ${APP_DIR}/l_openvino_toolkit* &amp;&amp; \
    sed -i 's/decline/accept/g' silent.cfg &amp;&amp; \
    ./install.sh --silent silent.cfg

# Setup the OpenVINO environment
RUN /bin/bash -c ""source ${OPENVINO_DIR}/bin/setupvars.sh""
</code></pre>
","531","1","3","<python-3.x><docker><openvino>"
"56483931","How to run environment initialization shell script from Dockerfile","2019-06-06 19:27:51","<p>I am trying to build an API wrapped in a docker image that serves Openvino model. How do I run the ""setupvars.sh"" from Dockerfile itself so that my application can access it?  </p>

<p>I have tried running the script using RUN. For ex:  RUN /bin/bash setupvars.sh
or RUN ./setupvars.sh . However, none of them work and I get ModelNotFoundError: no module named openvino</p>

<pre><code>RUN $INSTALL_DIR/install_dependencies/install_openvino_dependencies.sh
RUN cd /opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites &amp;&amp; sudo ./install_prerequisites_tf.sh
COPY . /app
WORKDIR /app
RUN apt autoremove -y &amp;&amp; \
    rm -rf /openvino /var/lib/apt/lists/*
RUN /bin/bash -c ""source $INSTALL_DIR/bin/setupvars.sh""
RUN echo ""source $INSTALL_DIR/bin/setupvars.sh"" &gt;&gt; /root/.bashrc
CMD [""/bin/bash""]
RUN python3 -m pip install opencv-python
RUN python3 test.py
</code></pre>

<p>I want OpenVino accessible to my gunicorn application that will serve the model in a docker image</p>
","<p>You need to re-run it every time you start the container, because those variables are only for the session.</p>

<p>Option 1: 
Run your application something like this:</p>

<pre><code>CMD /bin/bash -c ""source /opt/intel/openvino/bin/setupvars.sh &amp;&amp; python test.py""
</code></pre>

<p>Option 2 (not tested):
Add the source command to your .bashrc so it will be run every time on startup</p>

<pre><code># Assuming running as root
RUN echo ""/bin/bash -c 'source /opt/intel/openvino/bin/setupvars.sh'"" &gt;&gt; ~root/.bashrc
CMD python test.py
</code></pre>

<p>For the rest of the Dockerfile, there is a guide here (also not tested, and it doesn't cover the above):
<a href=""https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_docker_linux.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_docker_linux.html</a></p>
","531","1","3","<python-3.x><docker><openvino>"
"56483931","How to run environment initialization shell script from Dockerfile","2019-06-06 19:27:51","<p>I am trying to build an API wrapped in a docker image that serves Openvino model. How do I run the ""setupvars.sh"" from Dockerfile itself so that my application can access it?  </p>

<p>I have tried running the script using RUN. For ex:  RUN /bin/bash setupvars.sh
or RUN ./setupvars.sh . However, none of them work and I get ModelNotFoundError: no module named openvino</p>

<pre><code>RUN $INSTALL_DIR/install_dependencies/install_openvino_dependencies.sh
RUN cd /opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites &amp;&amp; sudo ./install_prerequisites_tf.sh
COPY . /app
WORKDIR /app
RUN apt autoremove -y &amp;&amp; \
    rm -rf /openvino /var/lib/apt/lists/*
RUN /bin/bash -c ""source $INSTALL_DIR/bin/setupvars.sh""
RUN echo ""source $INSTALL_DIR/bin/setupvars.sh"" &gt;&gt; /root/.bashrc
CMD [""/bin/bash""]
RUN python3 -m pip install opencv-python
RUN python3 test.py
</code></pre>

<p>I want OpenVino accessible to my gunicorn application that will serve the model in a docker image</p>
","<p>As mentioned in the two previous answers, the <code>setupvars.sh</code> script sets the environment variables required by OpenVINO.</p>
<p>But rather than running this every time, you can add the variables to your Dockerfile. While writing your Dockerfile run:</p>
<p><code>CMD /bin/bash -c &quot;source /opt/intel/openvino/bin/setupvars.sh &amp;&amp; printenv&quot;</code></p>
<p>This will give you the values that the environment variables are set to. You might also want to run <code>printenv</code> without setting the OpenVINO variables:</p>
<p><code>CMD /bin/bash printenv</code></p>
<p>Comparing the two outputs will let you figure out exactly what the <code>setupvars.sh</code> script is setting.</p>
<p>Once you know the values set by the script, you can set these as part of the Dockerfile using the <code>ENV</code> instruction. I <strong>wouldn't copy</strong> this because it's likely to be specific to your setup, but in my case, this ended up looking like:</p>
<pre><code>ENV PATH=/opt/intel/openvino/deployment_tools/model_optimizer:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
ENV LD_LIBRARY_PATH=/opt/intel/openvino/opencv/lib:/opt/intel/openvino/deployment_tools/ngraph/lib:/opt/intel/openvino/deployment_tools/inference_engine/external/tbb/lib::/opt/intel/openvino/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino/deployment_tools/inference_engine/external/omp/lib:/opt/intel/openvino/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64
ENV INTEL_CVSDK_DIR=/opt/intel/openvino
ENV OpenCV_DIR=/opt/intel/openvino/opencv/cmake
ENV TBB_DIR=/opt/intel/openvino/deployment_tools/inference_engine/external/tbb/cmake
# The next one will be whatever your working directory is
ENV PWD=/workspace
ENV InferenceEngine_DIR=/opt/intel/openvino/deployment_tools/inference_engine/share
ENV ngraph_DIR=/opt/intel/openvino/deployment_tools/ngraph/cmake
ENV SHLVL=1
ENV PYTHONPATH=/opt/intel/openvino/python/python3.6:/opt/intel/openvino/python/python3:/opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit:/opt/intel/openvino/deployment_tools/open_model_zoo/tools/accuracy_checker:/opt/intel/openvino/deployment_tools/model_optimizer
ENV HDDL_INSTALL_DIR=/opt/intel/openvino/deployment_tools/inference_engine/external/hddl
ENV _=/usr/bin/printenv
</code></pre>
","531","1","3","<python-3.x><docker><openvino>"
"63108736","Why error occurs during net.forward using second image in cv2?","2020-07-27 04:31:38","<p>I am testing out python opencv (cv2) to detect multiple images using openvino DNN models.
My code for first detection:</p>
<pre><code>import cv2
dog=cv2.imread(&quot;dog.jfif&quot;)
cat=cv2.imread(&quot;cat.jfif&quot;)
net=cv2.dnn.readNet(&quot;ssd_mobilenetv2_fp16_scale2.xml&quot;,&quot;ssd_mobilenetv2_fp16_scale2.bin&quot;)
blob=cv2.dnn.blobFromImage(dog)
net.setInput(blob)
out=net.forward()
</code></pre>
<p>Until here, no error is shown and <code>print (out)</code> shows that the &quot;dog&quot; detection is successful.
But then when i continue the next detection of &quot;cat&quot; image by adding the next few lines:</p>
<pre><code>blob=cv2.dnn.blobFromImage(cat)
net.setInput(blob)
out=net.forward()
</code></pre>
<p>And I get:</p>
<pre><code>&gt;&gt;&gt; out = net.forward()
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
cv2.error: OpenCV(4.3.0-openvino) ../opencv/modules/dnn/src/ie_ngraph.cpp:522: error: (-215:Assertion failed) !isInitialized() in function 'initPlugin'
</code></pre>
<p>Why does this error occur? What is the correct way to do for the second detection?</p>
","<p>Probably you can try to run the dog and cat detection separately because the program might confuse of those 2 inputs that came at the same time during the implementation of blob.</p>
<p>Another workaround is, if you still want them to work simultaneously, name the blob differently such as:</p>
<p>For dog:</p>
<pre><code>blob1=cv2.dnn.blobFromImage(dog)
net.setInput(blob1)
</code></pre>
<p>For cat</p>
<pre><code>blob2=cv2.dnn.blobFromImage(cat)
net.setInput(blob2)
</code></pre>
<p>so that the function  net.setInput(blob) doesn't get confuse on which input it should refer to.</p>
<p>Thanks!</p>
","525","0","1","<python-3.x><opencv-python><openvino>"
"63814047","Openvino movidius cant read IP camera","2020-09-09 14:41:50","<p>id like to make my movidius NCS1 read a IP camera.
this is my code:</p>
<pre><code>import cv2

#print(&quot;Before URL&quot;)
cap = cv2.VideoCapture('rtsp://admin:sphbr7410@192.168.240.151:554/cam/realmonitor?channel=1&amp;subtype=0')
#print(&quot;After URL&quot;)

while True:

    #print('About to start the Read command')
    ret, frame = cap.read()
    #print('About to show frame of Video.')
    print(ret,frame)

    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
</code></pre>
<p>but im my debug tests. i created a new user in my raspberry and installed the opencv for it using pip.
i if run this code in this user that dont have openvino initialized it run fine. but in Openvino env it cant read the camera steam.</p>
<p>how to solve it?</p>
","<p>Are you getting an &quot;MFX can't initialize session error&quot; while running with openvino environment?</p>
<p>OpenCV that comes with openvino is built with MFX enabled. MFX will be used for decoding your video frames. If you are able to decode frames in non-openvino environment, I would suggest you to disable MFX in openvino's opencv or uninstall opencv. In the setupvars file from intel/openvino/bin folder, you can change the opencv path to point to another installed opencv.</p>
","518","0","1","<python><opencv><ip-camera><openvino><movidius>"
"66593210","convert yolov5s.pt to openvino IR format success, predict stuff failed","2021-03-12 02:08:21","<p>This is begin from curious....I download the pretrained yolov5s.pt from public google drive, and convert it as yolov5s.onnx file with input shape [1,3,640,640] by using yolov5's models/export.py. Then I use openvino's deployment tools/mo.py to convert the yolov5s.onnx into openvino inference engines file (.xml+.bin). The conversion is success without error. At last, I try to run the predict by using these files by using openvino's demo program the prediction is successfully return the result. But all the result return negative numbers, and the array level is wrong. Is it impossible to convert the yolov5 weights for openvino?</p>
","<p>YOLOv5 is currently not an officially supported topology by the OpenVINO toolkit. Please see the list of validated ONNX and PyTorch topologies here <a href=""https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html</a></p>
<p>However, we have one recommendation for you to try, but it was no guaranteed it will succeed. You can do it by change export.py to include the Detect layer:
<a href=""https://github.com/ultralytics/yolov5/blob/a1c8406af3eac3e20d4dd5d327fd6cbd4fbb9752/models/export.py#L28"" rel=""nofollow noreferrer"">yolov5/models/export.py</a></p>
<p>Line 28 in a1c8406</p>
<pre><code>model.model[-1].export = True  # set Detect() layer export=True 
</code></pre>
<p>The above needs to be changed from <strong>True</strong> to <strong>False</strong>. For more detail, you can follow this thread <a href=""https://github.com/ultralytics/yolov5/issues/343"" rel=""nofollow noreferrer"">here</a>.</p>
","515","0","2","<openvino><yolov5>"
"66593210","convert yolov5s.pt to openvino IR format success, predict stuff failed","2021-03-12 02:08:21","<p>This is begin from curious....I download the pretrained yolov5s.pt from public google drive, and convert it as yolov5s.onnx file with input shape [1,3,640,640] by using yolov5's models/export.py. Then I use openvino's deployment tools/mo.py to convert the yolov5s.onnx into openvino inference engines file (.xml+.bin). The conversion is success without error. At last, I try to run the predict by using these files by using openvino's demo program the prediction is successfully return the result. But all the result return negative numbers, and the array level is wrong. Is it impossible to convert the yolov5 weights for openvino?</p>
","<p>Try this :<code>python mo.py  --input_model yolov5s.onnx -s 255 --reverse_input_channels --output Conv_245,Conv_261,Conv_277</code> (use Netron to check your architecture)</p>
","515","0","2","<openvino><yolov5>"
"61951365","Raspberry PI4 - OpenVino - Open CV DNN - net.forward - Segmentation fault - NCS2 - Myriad","2020-05-22 09:00:45","<p>I am using OpenVino recent kit : l_openvino_toolkit_runtime_raspbian_p_2020.2.120.tgz
Raspberry  - Pi4.
Open CV 4.3.0-openvino.</p>
<p>Trying to use the OpenCV DNN with tensorflow net and also caffe. But getting segmentation fault at net.forward() call.</p>
<p>Below is the code for tensorflow in Python . I do not know how to proceed and resolve it. I tried searching and could not find a working solution for this.</p>
<p>Steps I am doing ( the paths are configured correctly )</p>
<h1>Initialize the tensorflow net</h1>
<p>tensorflowNet = cv2.dnn.readNetFromTensorflow(PATH_TO_CKPT, PATH_TO_LABELS)</p>
<p>tensorflowNet.setPreferableTarget(cv2.dnn.DNN_TARGET_MYRIAD) ## set to use the NCS2</p>
<h1>PASS THE BLOB TO TENSORFLOW NETWORK</h1>
<p>blob = cv2.dnn.blobFromImage(frame, size=(300, 300), swapRB=True, crop=False)</p>
<h1>pass the blob through the network and obtain the detections and</h1>
<p>tensorflowNet.setInput(blob)</p>
<p>print (&quot;Before forward&quot;)</p>
<p><strong>detections =tensorflowNet.forward()</strong></p>
<p>print(&quot;after forward&quot;)</p>
<p>It fails at .forward method.Console out put is as below.</p>
<p>pi@raspberrypi:~/guardeyelite $ python3 OpenCVTensorFlow.py
Frame found</p>
<p>processing frame done</p>
<p>Before forward</p>
<p>Segmentation fault</p>
","<p>Glad you were able to resolve your issue by <a href=""https://www.intel.com/content/www/us/en/support/articles/000057005/boards-and-kits.html"" rel=""nofollow noreferrer"">building from source</a>. There have been a couple of OpenVINO Releases since this discussion. Please let us know if you continue to see this issue on the latest <a href=""https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_raspbian.html"" rel=""nofollow noreferrer"">OpenVINO for Raspbian OS</a> Release.</p>
","511","1","1","<opencv><raspberry-pi><intel><openvino>"
"62791268","python run bat file to setup environment variables","2020-07-08 09:05:17","<p>i have a .bat file which executes multiple set command to add environment variables, for further scripts to execute,</p>
<p>i tried below code which is not working</p>
<pre><code>import subprocess
subprocess.call([r'C:\Users\User\Desktop\my_vars.bat'])   
</code></pre>
<p>below is my bat file</p>
<pre><code>SET ROOT=C:\Program Files (x86)\IntelSWTools\openvino
set &quot;INTEL_OPENVINO_DIR=C:\Program Files (x86)\IntelSWTools\openvino&quot;
set &quot;INTEL_CVSDK_DIR=C:\Program Files (x86)\IntelSWTools\openvino&quot;
set &quot;OpenCV_DIR=C:\Program Files (x86)\IntelSWTools\openvino\opencv\cmake&quot;
set &quot;InferenceEngine_DIR=C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\inference_engine\share&quot;
set &quot;HDDL_INSTALL_DIR=C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\inference_engine\external\hddl&quot;
set &quot;ngraph_DIR=C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\ngraph\cmake&quot;
set PYTHONPATH=C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\open_model_zoo\tools\accuracy_checker;C:\Program Files (x86)\IntelSWTools\openvino\python\python3.7;C:\Program Files (x86)\IntelSWTools\openvino\python\python3;C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer;
set &quot;PATH=C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\ngraph\lib;C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\inference_engine\external\tbb\bin;C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\inference_engine\bin\intel64\Release;C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\inference_engine\bin\intel64\Debug;C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\inference_engine\external\hddl\bin;C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer;C:\Program Files (x86)\IntelSWTools\openvino\opencv\bin;C:\Program Files (x86)\Common Files\Intel\Shared Libraries\redist\intel64_win\compiler;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;%PY_HOME%;%PY_HOME%\Lib;%PY_HOME%\DLLs;%PY_HOME%\Lib\lib-tk;%PY_HOME%\Scripts;C:\Program Files (x86)\Windows Kits\10\Windows Performance Toolkit\;C:\Program Files\Git\cmd;C:\Program Files\dotnet\;C:\Users\LaserTrac\Pictures\osm2pgsql-bin;C:\Program Files\PostgreSQL\10\bin;C:\mapnik-v2.2.0\lib;C:\Program Files\TortoiseSVN\bin;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\ms4w\Apache\cgi-bin;C:\Program Files\nodejs\;C:\ProgramData\chocolatey\bin;C:\Users\LaserTrac\Downloads\geckodriver-v0.26.0-win64;C:\Users\LaserTrac\Documents\ffmpeg\ffmpeg-20200209-5ad1c1a-win64-static\bin;C:\Program Files\PuTTY\;C:\apache-maven-3.6.3\bin;C:\Program Files\Google Protobuf\bin;C:\tensorflow1\models\research\object_detection;C:\tensorflow1\models\research\slim;C:\tensorflow1\models\research;C:\ProgramData\Anaconda3;C:\ProgramData\Anaconda3\Scripts;C:\ProgramData\Anaconda3\Library\bin;C:\Program Files\Java\jdk-13.0.2\bin;C:\ninja;C:\Users\LaserTrac\Downloads\sample\mingw\mingw32\bin;C:\msys64\usr\bin;C:\Program Files\CMake\bin;C:\Users\LaserTrac\AppData\Local\Microsoft\WindowsApps;C:\mapnik-v2.2.0\bin;C:\mapnik-v2.2.0\lib;C:\Users\LaserTrac\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\LaserTrac\AppData\Roaming\npm;C:\Program Files\Oracle\VirtualBox;&quot;
</code></pre>
<p>Edit: below script is from opnevino toolkit samples</p>
<pre><code>from __future__ import print_function
import subprocess
import os
#os.system(&quot;C:\\Windows\\System32\\cmd.exe /c C:\\Users\\LaserTrac\\Desktop\\my_vars.bat&quot;)
#subprocess.call([r'C:\Program Files (x86)\IntelSWTools\openvino\bin\setupvars.bat'])
subprocess.call([r'C:\Users\LaserTrac\Desktop\my_vars.bat'])


#!/usr/bin/env python
  

import sys
import os
from argparse import ArgumentParser, SUPPRESS
import cv2
import numpy as np
import logging as log
from openvino.inference_engine import IECore
import datetime

#log.basicConfig(format=&quot;[ %(levelname)s ] %(message)s&quot;, level=log.INFO, stream=sys.stdout)
log.info(&quot;Loading Inference Engine&quot;)
ie = IECore()

# --------------------------- 1. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
#model_xml = args.model
model_xml = &quot;D:/openvino/2/frozen_inference_graph.xml&quot;
model_bin = os.path.splitext(model_xml)[0] + &quot;.bin&quot;
#log.info(&quot;Loading network files:\n\t{}\n\t{}&quot;.format(model_xml, model_bin))
net = ie.read_network(model=model_xml, weights=model_bin)
# -----------------------------------------------------------------------------------------------------
</code></pre>
<p>as per suggestion in comments i tried setting os.environ as below and checked with print().</p>
<pre><code>import os
#os.system(&quot;C:\\Windows\\System32\\cmd.exe /c C:\\Users\\LaserTrac\\Desktop\\my_vars.bat&quot;)
#subprocess.call([r'C:\Program Files (x86)\IntelSWTools\openvino\bin\setupvars.bat'])
# subprocess.call([r'C:\Users\LaserTrac\Desktop\sample.bat'])
# import set_vars
os.environ[&quot;ROOT&quot;] = &quot;C:\\Program Files (x86)\\IntelSWTools\\openvino&quot;
os.environ[&quot;INTEL_OPENVINO_DIR&quot;] = &quot;C:\\Program Files (x86)\\IntelSWTools\\openvino&quot;
os.environ[&quot;INTEL_CVSDK_DIR&quot;] = &quot;C:\Program Files (x86)\IntelSWTools\openvino&quot;
os.environ[&quot;OpenCV_DIR&quot;] = &quot;C:\\Program Files (x86)\\IntelSWTools\\openvino\\opencv\\cmake&quot;
os.environ[&quot;InferenceEngine_DIR&quot;] = &quot;C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\inference_engine\\share&quot;
os.environ[&quot;HDDL_INSTALL_DIR&quot;] = &quot;C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\inference_engine\\external\\hddl&quot;
os.environ[&quot;ngraph_DIR&quot;] = &quot;C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\ngraph\\cmake&quot;
os.environ[&quot;PYTHONPATH&quot;] = &quot;C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\open_model_zoo\\tools\\accuracy_checker;C:\\Program Files (x86)\\IntelSWTools\\openvino\\python\\python3.7;C:\\Program Files (x86)\\IntelSWTools\\openvino\\python\\python3;C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\model_optimizer&quot;

os.environ[&quot;PATH&quot;] = &quot;C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\ngraph\\lib;C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\inference_engine\\external\\tbb\\bin;C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\inference_engine\\bin\\intel64\\Release;C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\inference_engine\\bin\\intel64\\Debug;C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\inference_engine\\external\\hddl\\bin;C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\model_optimizer;C:\\Program Files (x86)\\IntelSWTools\\openvino\\opencv\\bin;C:\\Program Files (x86)\\Common Files\\Intel\\Shared Libraries\\redist\\intel64_win\\compiler;C:\\Program Files (x86)\\Intel\\iCLS Client\\;C:\\Program Files\\Intel\\iCLS Client\\;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;%PY_HOME%;%PY_HOME%\\Lib;%PY_HOME%\\DLLs;%PY_HOME%\\Lib\\lib-tk;%PY_HOME%\\Scripts;C:\\Program Files (x86)\\Windows Kits\\10\\Windows Performance Toolkit\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\dotnet\\;C:\\Users\\LaserTrac\\Pictures\\osm2pgsql-bin;C:\\Program Files\\PostgreSQL\\10\\bin;C:\\mapnik-v2.2.0\\lib;C:\\Program Files\\TortoiseSVN\\bin;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\ms4w\\Apache\\cgi-bin;C:\\Program Files\\nodejs\\;C:\\ProgramData\\chocolatey\\bin;C:\\Users\\LaserTrac\\Downloads\\geckodriver-v0.26.0-win64;C:\\Users\\LaserTrac\\Documents\\ffmpeg\\ffmpeg-20200209-5ad1c1a-win64-static\\bin;C:\\Program Files\\PuTTY\\;C:\\apache-maven-3.6.3\\bin;C:\\Program Files\\Google Protobuf\\bin;C:\\tensorflow1\\models\\research\\object_detection;C:\\tensorflow1\\models\\research\\slim;C:\\tensorflow1\\models\\research;C:\\ProgramData\\Anaconda3;C:\\ProgramData\\Anaconda3\\Scripts;C:\\ProgramData\\Anaconda3\\Library\\bin;C:\\Program Files\\Java\\jdk-13.0.2\\bin;C:\\ninja;C:\\Users\\LaserTrac\\Downloads\\sample\\mingw\\mingw32\\bin;C:\\msys64\\usr\\bin;C:\\Program Files\\CMake\\bin;C:\\Users\\LaserTrac\\AppData\\Local\\Microsoft\\WindowsApps;C:\\mapnik-v2.2.0\\bin;C:\\mapnik-v2.2.0\\lib;C:\\Users\\LaserTrac\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\LaserTrac\\AppData\\Roaming\\npm;C:\\Program Files\\Oracle\\VirtualBox&quot;

print(os.environ[&quot;PYTHONPATH&quot;])
import from openvino.inference_engine import IECore
</code></pre>
<p>print(os.environ[&quot;PYTHONPATH&quot;]) prints correct value what i set but error i got is no module named openvino, if i set these variabel from bat file directly in cmd and then execute my py file in same cmd, it works.</p>
<p>problem is i want to set physical path of openvino installation in environment variable so in the next line i can do  &quot;import openvino&quot;, and os.environ sets environment variable after python.exe process starts , so may be this is not possible, os environ is may be only for data variable type purpose. am i right??</p>
","<p>I'm not sure if what you are trying to do is possible. However, there is a guide to setup the <a href=""https://www.intel.com/content/www/us/en/support/articles/000033440/software/development-software.html"" rel=""nofollow noreferrer"">OpenVINO environment variables permanently on Windows 10</a>. This way you don't have to call the setupvars.bat script every time. The guide has a list of the variables needed for OpenVINO and their path.</p>
","462","0","1","<python><windows><batch-file><openvino>"
"66831806","Loading openvino python library on Raspebrry pi 4","2021-03-27 13:21:49","<p>After followign the guides (and coming through the github trackers), I was able to get OpenVINO to install on my pi4 and can run /opt/intel/openvino/bin/armv7l/Release/object_detection_sample_ssd  successfully using my own trained model.</p>
<p>I used the follow cmake command most recently, but I've done pretty much all iterations I could find on the several instruction pages.</p>
<pre><code>cmake -DCMAKE_BUILD_TYPE=Release /
      -DENABLE_SSE42=OFF /
      -DTHREADING=SEQ /
      -DENABLE_GNA=OFF /
      -DENABLE_PYTHON=ON /
      -DPYTHON_EXECUTABLE=/usr/bin/python3 /
      -DPYTHON_LIBRARY=/usr/lib/arm-linux-gnueabihf/libpython3.7m.so /
      -DPYTHON_INCLUDE_DIR=/usr/include/python3.7m /
      -NGRAPH_PYTHON_BUILD_ENABLE=ON /
      -DCMAKE_CXX_FLAGS=-latomic /
      -DOPENCV_EXTRA_EXE_LINKER_FLAGS=-latomic /
      -D CMAKE_INSTALL_PREFIX=/usr/local  /
      .. &amp;&amp; make
</code></pre>
<p>When I try one of the python examples I get</p>
<pre><code>ModuleNotFoundError: No module named 'ngraph'

</code></pre>
<p>Looking more into it now and it appears the issue is the &quot;setupvars.sh&quot; script not calling into the right directory. I was able to get the module openvino to load by adjusting the export path. I must say the amount of documentation that is, quite frankly all over the place and seems to have wrong directory structures left and right.</p>
","<p>As of right now, the builds for the Raspberry Pi are not being made with NGRAPH compiled. The user has to compile it themselves from scratch per this thread. <a href=""https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/No-ngraph-bindings-for-python-in-raspbian-distribution/m-p/1263401#M23093"" rel=""nofollow noreferrer"">https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/No-ngraph-bindings-for-python-in-raspbian-distribution/m-p/1263401#M23093</a></p>
<p>The comment by the Intel &quot;Support&quot; is wrong and won't work on new builds.</p>
","462","0","2","<python><openvino>"
"66831806","Loading openvino python library on Raspebrry pi 4","2021-03-27 13:21:49","<p>After followign the guides (and coming through the github trackers), I was able to get OpenVINO to install on my pi4 and can run /opt/intel/openvino/bin/armv7l/Release/object_detection_sample_ssd  successfully using my own trained model.</p>
<p>I used the follow cmake command most recently, but I've done pretty much all iterations I could find on the several instruction pages.</p>
<pre><code>cmake -DCMAKE_BUILD_TYPE=Release /
      -DENABLE_SSE42=OFF /
      -DTHREADING=SEQ /
      -DENABLE_GNA=OFF /
      -DENABLE_PYTHON=ON /
      -DPYTHON_EXECUTABLE=/usr/bin/python3 /
      -DPYTHON_LIBRARY=/usr/lib/arm-linux-gnueabihf/libpython3.7m.so /
      -DPYTHON_INCLUDE_DIR=/usr/include/python3.7m /
      -NGRAPH_PYTHON_BUILD_ENABLE=ON /
      -DCMAKE_CXX_FLAGS=-latomic /
      -DOPENCV_EXTRA_EXE_LINKER_FLAGS=-latomic /
      -D CMAKE_INSTALL_PREFIX=/usr/local  /
      .. &amp;&amp; make
</code></pre>
<p>When I try one of the python examples I get</p>
<pre><code>ModuleNotFoundError: No module named 'ngraph'

</code></pre>
<p>Looking more into it now and it appears the issue is the &quot;setupvars.sh&quot; script not calling into the right directory. I was able to get the module openvino to load by adjusting the export path. I must say the amount of documentation that is, quite frankly all over the place and seems to have wrong directory structures left and right.</p>
","<p>Please refer to the steps below for building Open Source OpenVINO™ Toolkit for Raspbian OS:</p>
<ol>
<li><p>Set up build environment and install build tools</p>
<p><code>sudo apt update &amp;&amp; sudo apt upgrade -y</code></p>
<p><code>sudo apt install build-essential</code></p>
</li>
<li><p>Install CMake from source</p>
<p><code>cd ~/</code></p>
<p><code>wget https://github.com/Kitware/CMake/releases/download/v3.14.4/cmake-3.14.4.tar.gz</code></p>
<p><code>tar xvzf cmake-3.14.4.tar.gz</code></p>
<p><code>cd ~/cmake-3.14.4</code></p>
<p><code>./bootstrap</code></p>
<p><code>make -j4 &amp;&amp; sudo make install</code></p>
</li>
<li><p>Install OpenCV from source</p>
<p><code>sudo apt install git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev python3-scipy libatlas-base-dev</code></p>
<p><code>cd ~/</code></p>
<p><code>git clone --depth 1 --branch 4.5.2 https://github.com/opencv/opencv.git</code></p>
<p><code>cd opencv &amp;&amp; mkdir build &amp;&amp; cd build</code></p>
<p><code>cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local ..</code></p>
<p><code>make -j4 &amp;&amp; sudo make install</code></p>
</li>
<li><p>Download source code and install dependencies</p>
<p><code>cd ~/</code></p>
<p><code>git clone --depth 1 --branch 2021.3 https://github.com/openvinotoolkit/openvino.git</code></p>
<p><code>cd ~/openvino</code></p>
<p><code>git submodule update --init --recursive</code></p>
<p><code>sh ./install_build_dependencies.sh</code></p>
<p><code>cd ~/openvino/inference-engine/ie_bridges/python/</code></p>
<p><code>pip3 install -r requirements.txt</code></p>
</li>
<li><p>Start CMake build</p>
<p><code>export OpenCV_DIR=/usr/local/lib/cmake/opencv4</code></p>
<p><code>cd ~/openvino</code></p>
<p><code>mkdir build &amp;&amp; cd build</code></p>
<p><code>cmake -DCMAKE_BUILD_TYPE=Release \</code></p>
<p><code>-DCMAKE_INSTALL_PREFIX=/home/pi/openvino_dist \</code></p>
<p><code>-DENABLE_MKL_DNN=OFF \</code></p>
<p><code>-DENABLE_CLDNN=OFF \</code></p>
<p><code>-DENABLE_GNA=OFF \</code></p>
<p><code>-DENABLE_SSE42=OFF \</code></p>
<p><code>-DTHREADING=SEQ \</code></p>
<p><code>-DENABLE_OPENCV=OFF \</code></p>
<p><code>-DNGRAPH_PYTHON_BUILD_ENABLE=ON \</code></p>
<p><code>-DNGRAPH_ONNX_IMPORT_ENABLE=ON \</code></p>
<p><code>-DENABLE_PYTHON=ON \</code></p>
<p><code>-DPYTHON_EXECUTABLE=$(which python3.7) \</code></p>
<p><code>-DPYTHON_LIBRARY=/usr/lib/arm-linux-gnueabihf/libpython3.7m.so \</code></p>
<p><code>-DPYTHON_INCLUDE_DIR=/usr/include/python3.7 \</code></p>
<p><code>-DCMAKE_CXX_FLAGS=-latomic ..</code></p>
<p><code>make -j4 &amp;&amp; sudo make install</code></p>
</li>
<li><p>Configure the Intel® Neural Compute Stick 2 Linux USB Driver</p>
<p><code>sudo usermod -a -G users &quot;$(whoami)&quot;</code></p>
<p><code>source /home/pi/openvino_dist/bin/setupvars.sh</code></p>
<p><code>sh /home/pi/openvino_dist/install_dependencies/install_NCS_udev_rules.sh</code></p>
</li>
<li><p>Verify nGraph module binding to Python</p>
<p><code>cd /home/pi/openvino_dist/deployment_tools/inference_engine/samples/python/object_detection_sample_ssd</code></p>
<p><code>python3 object_detection_sample_ssd.py -h</code></p>
</li>
</ol>
","462","0","2","<python><openvino>"
"55390264","OpenVINO unable to get optimum performance while running multiple inference engines","2019-03-28 04:38:22","<p>I am running multiple python processes( 4 in this case using multiprocessing module) for person detection (using ssd mobilenet model), each having it's own inference engine of OpenVINO. I am getting a very low FPS (not more than 10) for each process.  My suspicion is the CPUs are not getting utilized optimally because the number of threads being spawned by each engine are high, which is adding to the overhead and also the sharing of CPUs across processes.<br>
Also for single process, I am getting upto 60fps with OMP_NUM_THREADS set to 4.  </p>

<pre><code>My CPU details are:-
2 Sockets 
4 cores each socket 
1 thread each core 
Total - 8 CPUs
</code></pre>

<p>So what would be the</p>

<ol>
<li>Optimal value for OMP_NUM_THREADS in this case? </li>
<li>How can I avoid Sharing of CPUs across each process?</li>
</ol>

<p>Currently I am playing with OMP_NUM_THREADS and KMP_AFFINITY variables, but just doing a hit and trail on setting the values. Any detail on how to set would be really helpful. Thanks</p>
","<p>In case of multiple networks inference you may try to set <code>OMP_WAIT_POLICY</code> to <code>PASSIVE</code>.</p>

<p>BTW, OpenVINO 2019R1 moved from OpenMP to TBB. It might give better efficiency in case of deep learning networks pipeline.</p>
","459","0","2","<python-multithreading><openvino>"
"55390264","OpenVINO unable to get optimum performance while running multiple inference engines","2019-03-28 04:38:22","<p>I am running multiple python processes( 4 in this case using multiprocessing module) for person detection (using ssd mobilenet model), each having it's own inference engine of OpenVINO. I am getting a very low FPS (not more than 10) for each process.  My suspicion is the CPUs are not getting utilized optimally because the number of threads being spawned by each engine are high, which is adding to the overhead and also the sharing of CPUs across processes.<br>
Also for single process, I am getting upto 60fps with OMP_NUM_THREADS set to 4.  </p>

<pre><code>My CPU details are:-
2 Sockets 
4 cores each socket 
1 thread each core 
Total - 8 CPUs
</code></pre>

<p>So what would be the</p>

<ol>
<li>Optimal value for OMP_NUM_THREADS in this case? </li>
<li>How can I avoid Sharing of CPUs across each process?</li>
</ol>

<p>Currently I am playing with OMP_NUM_THREADS and KMP_AFFINITY variables, but just doing a hit and trail on setting the values. Any detail on how to set would be really helpful. Thanks</p>
","<p>In case if you are using the same model for all the processes consider to use OV multi-stream inference. Using this you can load single network and next to create a multiple infer requests. Using this you will have a better CPU utilization (if compare to running one infer request across multiple cores) and in result better throughput. </p>

<p>To understand how to use multi stream inference take a look on inference_engine/samples/python_samples/benchmark_app/benchmark sample</p>

<p>As well you can use benchmark sample to do a grid search to find an optimal configuration (number of streams, batch size).</p>
","459","0","2","<python-multithreading><openvino>"
"65253014","Memory corruption when using OnnxRuntime with OpenVINO on the Intel MyriadX and Raspberry Pi 4B","2020-12-11 14:18:48","<p>I'm trying to run Inference on the Intel Compute Stick 2 (MyriadX chip) connected to a Raspberry Pi 4B using OnnxRuntime and OpenVINO.
I have everything set up, the openvino provider gets recognized by onnxruntime and I can see the myriad in the list of available devices.</p>
<p>However, I always get some kind of memory corruption when trying to run inference on the myriad.
I'm not sure where this is coming from. If I use the default CPU inference instead of openvino, everythin works fine. Maybe the way I'm creating the <code>Ort::MemoryInfo</code> object is incorrect.</p>
<p><strong>output</strong></p>
<pre><code>Available execution providers:
        CPUExecutionProvider
        OpenVINOExecutionProvider
Available OpenVINO devices:
        MYRIAD
Starting Session
[...]
2020-12-11 13:43:13.962093843 [I:onnxruntime:, openvino_execution_provider.h:124 OpenVINOExecutionProviderInfo] [OpenVINO-EP]Choosing Device: MYRIAD , Precision: FP16
[...]
2020-12-11 13:43:13.972813082 [I:onnxruntime:, capability_2021_1.cc:854 GetCapability_2021_1] [OpenVINO-EP] Model is fully supported by OpenVINO
[...]
Loading data
Running Inference
2020-12-11 13:43:21.838737814 [I:onnxruntime:, sequential_executor.cc:157 Execute] Begin execution
2020-12-11 13:43:21.838892108 [I:onnxruntime:, backend_manager.cc:253 Compute] [OpenVINO-EP] Creating concrete backend for key: MYRIAD|50,28,28,1,|10,|84,10,|84,|120,84,|6,1,5,5,|16,|6,|400,120,|16,6,5,5,|120,|
2020-12-11 13:43:21.838926959 [I:onnxruntime:, backend_manager.cc:255 Compute] [OpenVINO-EP] Backend created for graph OpenVINOExecutionProvider_OpenVINO-EP-subgraph_1_0
2020-12-11 13:43:21.845913973 [I:onnxruntime:, backend_utils.cc:65 CreateCNNNetwork] ONNX Import Done
malloc(): unsorted double linked list corrupted
Aborted
</code></pre>
<p>Here is the <strong>code</strong> I'm using</p>
<pre><code>#include &lt;iostream&gt;
#include &lt;iomanip&gt;
#include &lt;chrono&gt;
#include &lt;array&gt;
#include &lt;cmath&gt;
#include &lt;MNIST-Loader/MNIST.h&gt;
#include &lt;onnxruntime_cxx_api.h&gt;
#include &lt;core/framework/allocator.h&gt;
#include &lt;ie_core.hpp&gt; //openvino inference_engine

int main()
{
        constexpr const char* modelPath = &quot;/home/pi/data/lenet_mnist.onnx&quot;;
        constexpr const char* mnistPath = &quot;/home/pi/data/mnist/&quot;;
        constexpr size_t batchSize = 50;

        std::cout &lt;&lt; &quot;Available execution providers:\n&quot;;
        for(const auto&amp; s : Ort::GetAvailableProviders()) std::cout &lt;&lt; '\t' &lt;&lt; s &lt;&lt; '\n';

        std::cout &lt;&lt; &quot;Available OpenVINO devices:\n&quot;;
        { //new scope so the core gets destroyed when leaving
                InferenceEngine::Core ieCore;
                for(const auto&amp; d : ieCore.GetAvailableDevices()) std::cout &lt;&lt; '\t' &lt;&lt; d &lt;&lt; '\n';
        }

        // ----------- create session -----------
        std::cout &lt;&lt; &quot;Starting Session\n&quot;;
        Ort::Env env(ORT_LOGGING_LEVEL_INFO);
        OrtOpenVINOProviderOptions ovOptions;
        ovOptions.device_type = &quot;MYRIAD_FP16&quot;;
        Ort::SessionOptions sessionOptions;
        sessionOptions.SetExecutionMode(ORT_SEQUENTIAL);
        sessionOptions.SetGraphOptimizationLevel(ORT_DISABLE_ALL);
        sessionOptions.AppendExecutionProvider_OpenVINO(ovOptions);
        Ort::Session session(env, modelPath, sessionOptions);

        // ----------- load data -----------
        std::cout &lt;&lt; &quot;Loading data\n&quot;;
        MNIST data(mnistPath);
        const std::array&lt;int64_t, 4&gt; inputShape{batchSize, 28, 28, 1};
        //const auto memoryInfo = Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPUInput);
        auto openvinoMemInfo = new OrtMemoryInfo(&quot;OpenVINO&quot;, OrtDeviceAllocator);
        const Ort::MemoryInfo memoryInfo(openvinoMemInfo);
        std::array&lt;float, batchSize*28*28&gt; batch;
        for(size_t i = 0; i &lt; batchSize; ++i)
        {
                const auto pixels = data.trainingData.at(i).pixelData;
                for(size_t k = 0; k &lt; 28*28; ++k)
                {
                        batch[k + (i*28*28)] = (pixels[k] == 0) ? 0.f : 1.f;
                }
        }
        const Ort::Value inputValues[] = {Ort::Value::CreateTensor&lt;float&gt;(memoryInfo, batch.data(), batch.size(), inputShape.data(), inputShape.size())};
        
        // ----------- run inference -----------
        std::cout &lt;&lt; &quot;Running Inference\n&quot;;
        Ort::RunOptions runOptions;
        Ort::AllocatorWithDefaultOptions alloc;
        const char* inputNames [] = { session.GetInputName (0, alloc) };
        const char* outputNames[] = { session.GetOutputName(0, alloc) };
        const auto start = std::chrono::steady_clock::now();
        auto results = session.Run(runOptions, inputNames, inputValues, 1, outputNames, 1);
        const auto end = std::chrono::steady_clock::now();
        std::cout &lt;&lt; &quot;\nRuntime: &quot; &lt;&lt; std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(end-start).count() &lt;&lt; &quot;ms\n&quot;;

        // ----------- print results -----------
        std::cout &lt;&lt; &quot;Results:&quot; &lt;&lt; std::endl;
        for(Ort::Value&amp; r : results)
        {
                const auto dims = r.GetTensorTypeAndShapeInfo().GetShape();
                for(size_t i = 0; i &lt; dims[0]; ++i)
                {
                        std::cout &lt;&lt; &quot;Label: &quot; &lt;&lt; data.trainingData.at(i).label &lt;&lt; &quot;\tprediction: [ &quot; &lt;&lt; std::fixed &lt;&lt; std::setprecision(3);
                        for(size_t k = 0; k &lt; dims[1]; ++k) std::cout &lt;&lt; r.At&lt;float&gt;({i, k}) &lt;&lt; ' ';
                        std::cout &lt;&lt; &quot;]\n&quot;;
                }
        }
        std::cout.flush();
}
</code></pre>
","<p>This component (OpenVINO Execution Provider) is not part of the OpenVINO toolkit, hence we require you to post your questions on the ONNX Runtime GitHub as it will help us identify issues with OpenVINO Execution Provider separately from the main OpenVINO toolkit.</p>
<p>We opened a case on Github on your behalf and we should get a reply soon within this thread - <a href=""https://github.com/microsoft/onnxruntime/issues/6304"" rel=""nofollow noreferrer"">https://github.com/microsoft/onnxruntime/issues/6304</a></p>
","457","0","1","<c++><raspberry-pi><memory-corruption><openvino><onnxruntime>"
"55008642","ssd_resnet50 model stuck when loading IR to the plugin","2019-03-05 17:41:56","<p>I'm trying to run <code>SSD ResNet50 FPN COCO</code> (<code>ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03</code>) model on NCS2 using MYRIAD, Python API but it stucks when loading IR to the plugin with the following error.</p>

<pre><code>E: [xLink] [     80143] handleIncomingEvent:240 handleIncomingEvent() Read failed -4

E: [xLink] [     80143] dispatcherEventReceive:308  dispatcherEventReceive() Read failed -4 | event 0x7f35137fde80 USB_WRITE_REQ

E: [xLink] [     80143] eventReader:256 eventReader stopped
E: [xLink] [     80144] dispatcherEventSend:908 Write failed event -4

E: [watchdog] [     81144] sendPingMessage:164  Failed send ping message: X_LINK_ERROR
E: [watchdog] [     82144] sendPingMessage:164  Failed send ping message: X_LINK_ERROR
E: [watchdog] [     83144] sendPingMessage:164  Failed send ping message: X_LINK_ERROR
E: [watchdog] [     84145] sendPingMessage:164  Failed send ping message: X_LINK_ERROR
</code></pre>

<p>...</p>

<p>The <code>Failed send ping message: X_LINK_ERROR</code> keeps showing until I pressed ctrl+C to kill the script. I noticed the <code>USB_WRITE_REQ</code> in the error so I thought it has something to do with USB3 port but when I tried a lighter model <code>ssd_mobilenet_v2_coco</code>, it worked like a charm.</p>

<p>This is the script to generate IR (IR generated successfully)</p>

<pre><code>python mo_tf.py --input_model ~/workspace/pi/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/frozen_inference_graph.pb --output_dir ~/workspace/pi/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/openvino_model/FP16 --tensorflow_use_custom_operations_config ~/intel/computer_vision_sdk/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json --tensorflow_object_detection_api_pipeline_config ~/workspace/pi/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/pipeline.config --data_type FP16
</code></pre>

<p>This is the script I used to test</p>

<pre><code>python test.py -m ~/workspace/pi/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/openvino_model/FP16/frozen_inference_graph.xml -i ~/workspace/object-detection/test_images/image.jpg -d MYRIAD
</code></pre>

<p>Here's the snippet of Python script</p>

<pre><code>plugin = IEPlugin(device=args.device, plugin_dirs=args.plugin_dir)
if args.cpu_extension and 'CPU' in args.device:
    plugin.add_cpu_extension(args.cpu_extension)
# Read IR
log.info(""Reading IR..."")
net = IENetwork(model=model_xml, weights=model_bin)

if plugin.device == ""CPU"":
    supported_layers = plugin.get_supported_layers(net)
    not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]
    if len(not_supported_layers) != 0:
        log.error(""Following layers are not supported by the plugin for specified device {}:\n {}"".
                  format(plugin.device, ', '.join(not_supported_layers)))
        log.error(""Please try to specify cpu extensions library path in demo's command line parameters using -l ""
                  ""or --cpu_extension command line argument"")
        sys.exit(1)
assert len(net.inputs.keys()) == 1, ""Demo supports only single input topologies""
assert len(net.outputs) == 1, ""Demo supports only single output topologies""
input_blob = next(iter(net.inputs))
out_blob = next(iter(net.outputs))

n, c, h, w = net.inputs[input_blob].shape

log.info(""Loading IR to the plugin..."")
exec_net = plugin.load(network=net) # &lt;== stuck at this line
</code></pre>

<p>The only reason I could think of why <code>ssd_mobilenet_v2_coco_2018_03_29</code> works and <code>ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03</code> not is the size which is 33MB for the former and about 100MB for the latter. I think the SSD Resnet50 model may have reached my laptop resource limitation. If this is the cause, how can I get around it? I'm using <code>l_openvino_toolkit_p_2018.5.455</code> on Ubuntu 18.04.</p>

<p>The <code>SSD ResNet50 FPN COCO</code> model is from TensorFlow Object Detection Models Zoo and supported by Openvino toolkit (<a href=""https://software.intel.com/en-us/articles/OpenVINO-Using-TensorFlow"" rel=""nofollow noreferrer"">https://software.intel.com/en-us/articles/OpenVINO-Using-TensorFlow</a>).</p>
","<p>This model is currently not supported on MYRIAD, this issue is known by the development team. We will keep you update when we will support it.</p>
","453","0","1","<python><object-detection-api><openvino>"
"58195886","openvino crashes after running inference some seconds in raspberry pi 4","2019-10-02 05:51:41","<p>I tried to use Intel Neural Compute Stick 2 as an inference engine for my smart car.</p>
<p>I installed l_openvino_toolkit_runtime_raspbian_p_2019.2.242.tgz followed by <a href=""https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_raspbian.html"" rel=""nofollow noreferrer"">this link</a>, and run <a href=""https://raw.githubusercontent.com/PINTO0309/MobileNet-SSD-RealSense/master/SingleStickSSDwithUSBCamera_OpenVINO_NCS2.py"" rel=""nofollow noreferrer"">the code</a> for testing.</p>
<p>Everything was fine at the beginning of 10 - 20 seconds(sometimes less, sometimes longer), Then crashed with the following errors:</p>
<blockquote>
<p>E: [xLink] [    327401] [EventRead00Thr] dispatcherEventReceive:336     dispatcherEventReceive() Read failed (err -4) | event 0xaf1fdddc XLINK_READ_REL_REQ</p>
<p>E: [xLink] [    327401] [EventRead00Thr] eventReader:223        eventReader thread stopped (err -4) E: [xLink] [    327402] [python3] XLinkReadDataWithTimeOut:1323 Event data is invalid</p>
<p>E: [ncAPI] [    327402] [python3] ncFifoReadElem:3445   Packet reading is failed. terminate called after throwing an instance of 'InferenceEngine::details::InferenceEngineException'</p>
<p>what():  Failed to read output from FIFO: NC_ERROR</p>
<p>Aborted</p>
</blockquote>
<p>I tried to insert it into the USB hub with a self-power adapter, but it still got the same errors.
I also tried to insert it into the USB2.0, but still not working.</p>
<p>I checked out the dmesg, I found the Intel Neural Compute Stick 2 will be auto-mounted when I called net.forward() at some point.</p>
<p>I got dmesg information by following steps:</p>
<ol>
<li>check the ncs2 device is ok(I can find the device) before running the code.</li>
<li>clear dmesg</li>
<li>running the code</li>
<li>waiting for it crashed. then, check out the dmesg.</li>
</ol>
<p>the dmesg showing:</p>
<blockquote>
<p>[87255.685160] usb 1-1.1: USB disconnect, device number 25</p>
<p>[87255.831256] usb 2-1: new SuperSpeed Gen 1 USB device number 18 using xhci_hcd</p>
<p>[87255.861963] usb 2-1: New USB device found, idVendor=03e7, idProduct=f63b, bcdDevice= 1.00</p>
<p>[87255.861970] usb 2-1: New USB device strings: Mfr=1, Product=2, SerialNumber=3</p>
<p>[87255.861975] usb 2-1: Product: VSC Loopback Device</p>
<p>[87255.861980] usb 2-1: Manufacturer: Intel Corporation</p>
<p>[87255.861985] usb 2-1: SerialNumber: 41440410119541BC00</p>
<p>[87280.181479] usb 1-1.1: new high-speed USB device number 26 using xhci_hcd</p>
<p>[87280.312042] usb 1-1.1: New USB device found, idVendor=03e7, idProduct=2485, bcdDevice= 0.01</p>
<p>[87280.312048] usb 1-1.1: New USB device strings: Mfr=1, Product=2, SerialNumber=3</p>
<p>[87280.312053] usb 1-1.1: Product: Movidius MyriadX</p>
<p>[87280.312058] usb 1-1.1: Manufacturer: Movidius Ltd.</p>
<p>[87280.312063] usb 1-1.1: SerialNumber: 03e72485</p>
<p>[87280.691784] usb 2-1: USB disconnect, device number 18</p>
</blockquote>
<p>My environment is:
Raspberry Pi 4 (4GB)
Raspbian Buster</p>
<p>I have debugged it serval days, but have no idea to solve this problem.
Is there anything I missed?</p>
<p>Thanks!</p>
","<p>The error seems to be related to the NCS2 device being reset for inference and fail to initialize during runtime. Verify you have added the USB Rules for the Intel® Neural Compute Stick 2 device by performing the following steps.</p>
<ol>
<li><p>Add the current Linux user to the users group:</p>
<p>$ sudo usermod -a -G users &quot;$(whoami)&quot;</p>
<p>Log out and log in for it to take effect.</p>
</li>
<li><p>Run setupvars.sh again after logging in:</p>
<p>$ source /opt/intel/openvino/bin/setupvars.sh</p>
</li>
<li><p>Install the USB rules running the install_NCS_udev_rules.sh script:</p>
<p>$ sh /opt/intel/openvino/install_dependencies/install_NCS_udev_rules.sh</p>
</li>
<li><p>Proceed to run the demo application:</p>
<p>$ python3 SingleStickSSDwithUSBCamera_OpenVINO_NCS2.py</p>
</li>
</ol>
<p>Note the <em>l_openvino_toolkit_runtime_raspbian_p_2019.2.242</em> is now quite outdated, and there have been newer software packages released. Checkout the <a href=""https://storage.openvinotoolkit.org/repositories/openvino/packages/"" rel=""nofollow noreferrer"">OpenVINO™ Toolkit packages storage</a> for the most recent release available (at the time of this post, 2021.4.1 - <em>l_openvino_toolkit_runtime_raspbian_p_2021.4.689.tgz</em>).</p>
<p>I've executed the demo application you listed with OpenVINO 2021.4.1 for longer than 10 minutes and the error can't reproduce:</p>
<p><a href=""https://i.stack.imgur.com/Q9POT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q9POT.png"" alt=""demo application running"" /></a></p>
","436","1","1","<python-3.x><opencv><raspberry-pi><openvino><movidius>"
"62894561","openvino FormatReader vs cv::Mat image = cv::imdecode","2020-07-14 11:52:51","<p>i am trying to convert openvino c++ object_detection_ssd sample to dll library, so that i can use it in my c# application.</p>
<p>below is my code c++ code to export function</p>
<pre><code>__declspec(dllexport) void Classify_Image(unsigned char* img_pointer, long data_len,
    char* out_result, int length_of_out_result, int top_n_results)
{
    
    std::vector&lt;unsigned char&gt; inputImageBytes(img_pointer, img_pointer + data_len);
    cv::Mat image = cv::imdecode(inputImageBytes,cv::IMREAD_COLOR);
    //cv::imwrite(&quot;lala.jpg&quot;, image);
    
    if (inputInfo == nullptr) {
        initialize();
    }

    // --------------------------- 8. Create infer request -------------------------------------------------
            //slog::info &lt;&lt; &quot;Create infer request&quot; &lt;&lt; slog::endl;
    InferRequest infer_request = executable_network.CreateInferRequest();
    // -----------------------------------------------------------------------------------------------------

    std::vector&lt;std::shared_ptr&lt;unsigned char&gt;&gt; imagesData, originalImagesData;
    std::vector&lt;size_t&gt; imageWidths, imageHeights;
    //FormatReader::ReaderPtr reader(&quot;C:\\Users\\Sam\\Desktop\\la.jpg&quot;);
    cv::Mat dst;

    cv::resize(image, dst, cv::Size(400, 225));
    cv::imwrite(&quot;lala_m.jpg&quot;, dst);
    //std::shared_ptr&lt;unsigned char&gt; originalData(image.data);
    std::shared_ptr&lt;unsigned char&gt; originalData(image.data);

    std::shared_ptr&lt;unsigned char&gt; data1(dst.data);
    //
    originalImagesData.push_back(originalData);
    imagesData.push_back(data1);
    imageWidths.push_back(1280);
    imageHeights.push_back(720);

    if (imagesData.empty()) throw std::logic_error(&quot;Valid input images were not found!&quot;);

    size_t batchSize = network.getBatchSize();
    
    if (batchSize != imagesData.size()) {
        slog::warn &lt;&lt; &quot;Number of images &quot; + std::to_string(imagesData.size()) + \
            &quot; doesn't match batch size &quot; + std::to_string(batchSize) &lt;&lt; slog::endl;
        batchSize = std::min(batchSize, imagesData.size());
        slog::warn &lt;&lt; &quot;Number of images to be processed is &quot; &lt;&lt; std::to_string(batchSize) &lt;&lt; slog::endl;
    }
    
    ///** Creating input blob **/
    Blob::Ptr imageInput = infer_request.GetBlob(imageInputName);
    
    ///** Filling input tensor with images. First b channel, then g and r channels **/
    MemoryBlob::Ptr mimage = as&lt;MemoryBlob&gt;(imageInput);
    if (!mimage) {
        slog::err &lt;&lt; &quot;We expect image blob to be inherited from MemoryBlob, but by fact we were not able &quot;
            &quot;to cast imageInput to MemoryBlob&quot; &lt;&lt; slog::endl;
        return;
    }
    
    //// locked memory holder should be alive all time while access to its buffer happens
    auto minputHolder = mimage-&gt;wmap();
    size_t num_channels = mimage-&gt;getTensorDesc().getDims()[1];
    size_t image_size = mimage-&gt;getTensorDesc().getDims()[3] * mimage-&gt;getTensorDesc().getDims()[2];

    unsigned char *data = minputHolder.as&lt;unsigned char *&gt;();

    /** Iterate over all input images **/
    for (size_t image_id = 0; image_id &lt; std::min(imagesData.size(), batchSize); ++image_id) {
        /** Iterate over all pixel in image (b,g,r) **/
        for (size_t pid = 0; pid &lt; image_size; pid++) {
            /** Iterate over all channels **/
            for (size_t ch = 0; ch &lt; num_channels; ++ch) {
                /**          [images stride + channels stride + pixel id ] all in bytes            **/
                data[image_id * image_size * num_channels + ch * image_size + pid] = imagesData.at(image_id).get()[pid*num_channels + ch];
            }
        }
    }

    if (imInfoInputName != &quot;&quot;) {
        Blob::Ptr input2 = infer_request.GetBlob(imInfoInputName);
        auto imInfoDim = inputsInfo.find(imInfoInputName)-&gt;second-&gt;getTensorDesc().getDims()[1];

        /** Fill input tensor with values **/
        MemoryBlob::Ptr minput2 = as&lt;MemoryBlob&gt;(input2);
        if (!minput2) {
            slog::err &lt;&lt; &quot;We expect input2 blob to be inherited from MemoryBlob, but by fact we were not able &quot;
                &quot;to cast input2 to MemoryBlob&quot; &lt;&lt; slog::endl;
            return;
        }
        // locked memory holder should be alive all time while access to its buffer happens
        auto minput2Holder = minput2-&gt;wmap();
        float *p = minput2Holder.as&lt;PrecisionTrait&lt;Precision::FP32&gt;::value_type *&gt;();

        for (size_t image_id = 0; image_id &lt; std::min(imagesData.size(), batchSize); ++image_id) {
            p[image_id * imInfoDim + 0] = static_cast&lt;float&gt;(inputsInfo[imageInputName]-&gt;getTensorDesc().getDims()[2]);
            p[image_id * imInfoDim + 1] = static_cast&lt;float&gt;(inputsInfo[imageInputName]-&gt;getTensorDesc().getDims()[3]);
            for (size_t k = 2; k &lt; imInfoDim; k++) {
                p[image_id * imInfoDim + k] = 1.0f;  // all scale factors are set to 1.0
            }
        }
    }
    // -----------------------------------------------------------------------------------------------------

    // --------------------------- 10. Do inference ---------------------------------------------------------
    slog::info &lt;&lt; &quot;Start inference&quot; &lt;&lt; slog::endl;
    infer_request.Infer();
    // -----------------------------------------------------------------------------------------------------

    // --------------------------- 11. Process output -------------------------------------------------------
    slog::info &lt;&lt; &quot;Processing output blobs&quot; &lt;&lt; slog::endl;

    const Blob::Ptr output_blob = infer_request.GetBlob(outputName);
    MemoryBlob::CPtr moutput = as&lt;MemoryBlob&gt;(output_blob);
    if (!moutput) {
        throw std::logic_error(&quot;We expect output to be inherited from MemoryBlob, &quot;
            &quot;but by fact we were not able to cast output to MemoryBlob&quot;);
    }
    // locked memory holder should be alive all time while access to its buffer happens
    auto moutputHolder = moutput-&gt;rmap();
    const float *detection = moutputHolder.as&lt;const PrecisionTrait&lt;Precision::FP32&gt;::value_type *&gt;();

    std::vector&lt;std::vector&lt;int&gt; &gt; boxes(batchSize);
    std::vector&lt;std::vector&lt;int&gt; &gt; classes(batchSize);

    ///* Each detection has image_id that denotes processed image */
    std::string result = &quot;OB_DATA=&quot;;
    int num_detect = 0;
    
    for (int curProposal = 0; curProposal &lt; maxProposalCount; curProposal++) {
        auto image_id = static_cast&lt;int&gt;(detection[curProposal * objectSize + 0]);
        if (image_id &lt; 0 ) {
            slog::info &lt;&lt; &quot;ends with break &quot;&lt;&lt;slog::endl;
            break;
        }

        float confidence = detection[curProposal * objectSize + 2];
        auto label = static_cast&lt;int&gt;(detection[curProposal * objectSize + 1]);
        auto xmin = static_cast&lt;int&gt;(detection[curProposal * objectSize + 3] * imageWidths[image_id]);
        auto ymin = static_cast&lt;int&gt;(detection[curProposal * objectSize + 4] * imageHeights[image_id]);
        auto xmax = static_cast&lt;int&gt;(detection[curProposal * objectSize + 5] * imageWidths[image_id]);
        auto ymax = static_cast&lt;int&gt;(detection[curProposal * objectSize + 6] * imageHeights[image_id]);

        std::cout &lt;&lt; &quot;[&quot; &lt;&lt; curProposal &lt;&lt; &quot;,&quot; &lt;&lt; label &lt;&lt; &quot;] element, prob = &quot; &lt;&lt; confidence &lt;&lt;
                &quot;    (&quot; &lt;&lt; xmin &lt;&lt; &quot;,&quot; &lt;&lt; ymin &lt;&lt; &quot;)-(&quot; &lt;&lt; xmax &lt;&lt; &quot;,&quot; &lt;&lt; ymax &lt;&lt; &quot;)&quot; &lt;&lt; &quot; batch id : &quot; &lt;&lt; image_id;

        if (confidence &gt; 0.5) {
            num_detect += 1;
            result += std::to_string(confidence);
            
            result += &quot;,&quot;+ std::to_string(label);
            result += &quot;,&quot; + std::to_string(xmin);
            result += &quot;,&quot; + std::to_string(ymin);
            result += &quot;,&quot; + std::to_string(xmax);
            result += &quot;,&quot; + std::to_string(ymax);
            
            /** Drawing only objects with &gt;50% probability **/
            classes[image_id].push_back(label);
            boxes[image_id].push_back(xmin);
            boxes[image_id].push_back(ymin);
            boxes[image_id].push_back(xmax - xmin);
            boxes[image_id].push_back(ymax - ymin);
            //std::cout &lt;&lt; &quot; WILL BE PRINTED!&quot;;
            /*std::cout &lt;&lt; std::endl;*/
            //slog::info &lt;&lt; &quot; add prediction&quot; &lt;&lt; slog::endl;
        }
        result += &quot;;&quot;;
        std::cout &lt;&lt; std::endl;
    }
    
    data1.reset();
    originalData.reset();
    
    //dst.release();
    //image.release();
    

    length_of_out_result = (int)result.size();
    
    std::copy(result.begin(), result.end(), out_result);
    out_result[std::min(length_of_out_result - 1, (int)result.size())] = 0;
            std::cout &lt;&lt; &quot;end code&quot;&lt;&lt;std::endl;
    
}
</code></pre>
<p>}</p>
<p>in the code above i am trying to decode my byte array which i send from c# to opencv mat and run inference on mat object, so when i call this function from c# it works fine but after printing &quot;end code&quot; line it gives error in my c# application, below is my code in c#, my c# application can't print Console.WriteLine(&quot;after call&quot;); and in the visual studio output i can see last line printed by c++ function which is &quot;end code&quot;</p>
<pre><code>[DllImport(&quot;object_detection_sample_ssd.dll&quot;, CallingConvention = CallingConvention.Cdecl)]
    static extern void Classify_Image(byte[] img, long data_len, StringBuilder out_result, int out_result_length, int top_n_results = 2);

    private void button3_Click(object sender, EventArgs e)
    {
        byte[] result = new byte[200];
        Image img = Image.FromFile(@&quot;C:\Users\Sam\Desktop\la.jpg&quot;);
        ImageFormat fmt = new ImageFormat(img.RawFormat.Guid);
        var imageCodecInfo = ImageCodecInfo.GetImageEncoders().FirstOrDefault(codec =&gt; codec.FormatID == img.RawFormat.Guid);
        //this is for situations, where the image is not read from disk, and is stored in the memort(e.g. image comes from a camera or snapshot)
        if (imageCodecInfo == null)
        {
            fmt = ImageFormat.Jpeg;
        }
        //Image img = Image.FromFile(@&quot;&quot;);
        using (MemoryStream ms = new MemoryStream())
        {
            img.Save(ms, fmt);
            byte[] image_byte_array = ms.ToArray();
            int len_result=300;

            int STRING_MAX_LENGTH1 = 300;
            StringBuilder str1 = new StringBuilder(STRING_MAX_LENGTH1);

            Classify_Image(image_byte_array, ms.Length, str1, len_result, 2);
            Console.WriteLine(&quot;after call&quot;);
        }
        Console.WriteLine(&quot;even after using&quot;);
        Console.WriteLine(&quot;output =&quot;+ ASCIIEncoding.ASCII.GetString(result));
        
    }
</code></pre>
<p>i dont know what is wrong with that, in the original example of openvino toolkit there is a FormatReader.h file which is used to load image from image file path, i have tried with passing image file name and use FormatReader as original and it works. FormatReader <a href=""https://github.com/openvinotoolkit/openvino/tree/master/inference-engine/samples/common/format_reader"" rel=""nofollow noreferrer"">https://github.com/openvinotoolkit/openvino/tree/master/inference-engine/samples/common/format_reader</a></p>
<p>please help!</p>
","<p>I suggest you refer to Hello Classification C++ Sample, which loads image from Mat for inferencing. The main.cpp file is available at the following link:
<a href=""https://github.com/openvinotoolkit/openvino/blob/master/inference-engine/samples/hello_classification/main.cpp"" rel=""nofollow noreferrer"">https://github.com/openvinotoolkit/openvino/blob/master/inference-engine/samples/hello_classification/main.cpp</a></p>
","408","0","1","<c++><openvino>"
"60460765","Failure to process openvino dnn recognizer on NCS2 on Raspberry Pi","2020-02-28 23:57:29","<p>I am trying to run a face detector/face recognizer python script on my Raspberry Pi 4 with the Intel NCS2 device.<br>
I am running on the Pi 4 with OpenCV and Openvino installed per the excellent blogs at pyimagesearch.com.</p>

<p>Face detection runs with this code, set up to target the NCS2 (myriad) device:</p>

<pre><code>detector = cv2.dnn.readNetFromCaffe(""face_detection_model/deploy.prototxt"",              
                ""face_detection_model/res10_300x300_ssd_iter_140000.caffemodel"")
detector.setPreferableTarget(cv2.dnn.DNN_TARGET_MYRIAD)
</code></pre>

<p>The detector runs with </p>

<pre><code>     imageBlob = cv2.dnn.blobFromImage(
        cv2.resize(image, (300, 300)), 1.0, (300, 300),
        (104.0, 177.0, 123.0), swapRB=False, crop=False)
     detector.setInput(imageBlob)
     detections = detector.forward()
</code></pre>

<p>The face detector works great and runs smoothly.</p>

<p>But, then, I set up an image recognizer on the face that the detector finds.</p>

<pre><code>embedder = cv2.dnn.readNetFromTorch(""face_embedding_model/openface.nn4.small2.v1.t7""])
embedder.setPreferableTarget(cv2.dnn.DNN_TARGET_MYRIAD)
</code></pre>

<p>And then I invoke it with the image of the face, based on detection box</p>

<pre><code>        face = image[startY:endY, startX:endX]
        (fH, fW) = face.shape[:2]
        faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96),
            (0, 0, 0), swapRB=True, crop=False)
        embedder.setInput(faceBlob)
        vec = embedder.forward()
</code></pre>

<p>But, when it gets to vec=embedder.forward(), the following error gets thrown</p>

<pre><code>Failed to Initialize Inference Engine backend : Device with “CPU” name is not registered in the InferenceEngine in function ‘initPlugin’ 
</code></pre>

<p>I have no idea why the function is correctly targeted to the NCS2 for the detection, but fails to run for the recognizer. </p>

<p>From what I can tell, it is trying to run the recognizer code on the CPU, not the NCS2.  The Intel openvino libraries supposedly don't support the Raspberry Pi Arm processor, so it throws the error.</p>

<p>I have tried using various other setPreferableTarget and SetPreferrableBackend settings from the OpenCV documentation.  According to the documentation, the only allowable combo with the myriad is is a target of DNN_TARGET_MYRIAD and a backend of DNN_BACKEND_INFERENCE_ENGINE.  But no combination works.</p>

<p>Any suggestions?</p>
","<p>Artemy's comment got me thinking about versions, so I looked into a version warning that I thought was unrelated.  It turns out that it was a version conflict in sklearn that was somehow blocking the running of the recognizer on the NCS2. Installing the old 0.20.2 version of scikit-learn fixed it.</p>

<p>This really confuses me, since I wasn't expecting the MYRIAD calls to be dependent on a python library.  Maybe I accidentally changed another setting somewhere.</p>

<p>Interestingly enough, for my video application, the first pass of the recognizer takes ~30 seconds, but after that it takes only ~50 ms per face. </p>
","405","1","1","<opencv><raspberry-pi><face-recognition><openvino>"
"59545471","Intel OpenVINO: Error Verifying Installation","2019-12-31 14:40:27","<p>I'm new to Intel OpenVINO, I've downloaded the Installer and Followed their Installation Guide: <a href=""https://docs.openvinotoolkit.org/2019_R3.1/_docs_install_guides_installing_openvino_windows.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/2019_R3.1/_docs_install_guides_installing_openvino_windows.html</a></p>

<p>Note that I'm Using Windows 10 with Intel processor machine, </p>

<p>I followed all the steps and everything went smoothly, until the part that says verify the installation by 
Running a demo file that was implemented with the installation, When I ran that file I got error says: </p>

<pre><code>`[ ERROR ] Failed to find reference implementation for `377` Layer with `Quantize` Type on constant propagation
Error`
</code></pre>

<p>And I'm not sure what does this mean at all, So I tried to ask for help.</p>

<p>I asked at their Slack community, but no answer yet, So I thought about asking here, </p>

<p>Here is my Command Prompt that shows what I did:</p>

<pre><code>    Microsoft Windows [Version 10.0.17763.914]
(c) 2018 Microsoft Corporation. All rights reserved.

C:\Users\hreda&gt;cd C:\Program Files (x86)\IntelSWTools\openvino\bin\

C:\Program Files (x86)\IntelSWTools\openvino\bin&gt;setupvars.bat
Python 3.6.0
ECHO is off.
PYTHONPATH=C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\open_model_zoo\tools\accuracy_checker;C:\Program Files (x86)\IntelSWTools\openvino\python\python3.6;C:\Program Files (x86)\IntelSWTools\openvino\python\python3;
[setupvars.bat] OpenVINO environment initialized

C:\Program Files (x86)\IntelSWTools\openvino\bin&gt;cd C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer\install_prerequisites

C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer\install_prerequisites&gt;install_prerequisites.bat
Python 3.6.0
ECHO is off.
Requirement already satisfied: tensorflow&lt;2.0.0,&gt;=1.2.0 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from -r ..\requirements.txt (line 1)) (1.15.0)
Requirement already satisfied: mxnet&lt;=1.3.1,&gt;=1.0.0 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from -r ..\requirements.txt (line 2)) (1.3.1)
Requirement already satisfied: networkx&lt;2.4,&gt;=1.11 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from -r ..\requirements.txt (line 3)) (2.3)
Requirement already satisfied: numpy&gt;=1.12.0 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from -r ..\requirements.txt (line 4)) (1.17.4)
Requirement already satisfied: protobuf==3.6.1 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from -r ..\requirements.txt (line 5)) (3.6.1)
Requirement already satisfied: onnx&gt;=1.1.2 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from -r ..\requirements.txt (line 6)) (1.6.0)
Requirement already satisfied: defusedxml&gt;=0.5.0 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from -r ..\requirements.txt (line 7)) (0.6.0)
Requirement already satisfied: absl-py&gt;=0.7.0 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (0.9.0)
Requirement already satisfied: wheel&gt;=0.26 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (0.33.6)
Requirement already satisfied: keras-applications&gt;=1.0.8 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (1.0.8)
Requirement already satisfied: wrapt&gt;=1.11.1 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (1.11.2)
Requirement already satisfied: tensorboard&lt;1.16.0,&gt;=1.15.0 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (1.15.0)
Requirement already satisfied: google-pasta&gt;=0.1.6 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (0.1.8)
Requirement already satisfied: tensorflow-estimator==1.15.1 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (1.15.1)
Requirement already satisfied: keras-preprocessing&gt;=1.0.5 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (1.1.0)
Requirement already satisfied: termcolor&gt;=1.1.0 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (1.1.0)
Requirement already satisfied: opt-einsum&gt;=2.3.2 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (3.1.0)
Requirement already satisfied: grpcio&gt;=1.8.6 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (1.26.0)
Requirement already satisfied: six&gt;=1.10.0 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (1.13.0)
Requirement already satisfied: gast==0.2.2 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (0.2.2)
Requirement already satisfied: astor&gt;=0.6.0 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (0.8.1)
Requirement already satisfied: graphviz&lt;0.9.0,&gt;=0.8.1 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from mxnet&lt;=1.3.1,&gt;=1.0.0-&gt;-r ..\requirements.txt (line 2)) (0.8.4)
Requirement already satisfied: requests&lt;2.19.0,&gt;=2.18.4 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from mxnet&lt;=1.3.1,&gt;=1.0.0-&gt;-r ..\requirements.txt (line 2)) (2.18.4)
Requirement already satisfied: decorator&gt;=4.3.0 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from networkx&lt;2.4,&gt;=1.11-&gt;-r ..\requirements.txt (line 3)) (4.4.1)
Requirement already satisfied: setuptools in c:\users\hreda\appdata\local\programs\python\python36\lib\site-packages (from protobuf==3.6.1-&gt;-r ..\requirements.txt (line 5)) (28.8.0)
Requirement already satisfied: typing-extensions&gt;=3.6.2.1 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from onnx&gt;=1.1.2-&gt;-r ..\requirements.txt (line 6)) (3.7.4.1)
Requirement already satisfied: h5py in c:\users\hreda\appdata\roaming\python\python36\site-packages (from keras-applications&gt;=1.0.8-&gt;tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (2.10.0)
Requirement already satisfied: werkzeug&gt;=0.11.15 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorboard&lt;1.16.0,&gt;=1.15.0-&gt;tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (0.16.0)
Requirement already satisfied: markdown&gt;=2.6.8 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from tensorboard&lt;1.16.0,&gt;=1.15.0-&gt;tensorflow&lt;2.0.0,&gt;=1.2.0-&gt;-r ..\requirements.txt (line 1)) (3.1.1)
Requirement already satisfied: idna&lt;2.7,&gt;=2.5 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from requests&lt;2.19.0,&gt;=2.18.4-&gt;mxnet&lt;=1.3.1,&gt;=1.0.0-&gt;-r ..\requirements.txt (line 2)) (2.6)
Requirement already satisfied: certifi&gt;=2017.4.17 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from requests&lt;2.19.0,&gt;=2.18.4-&gt;mxnet&lt;=1.3.1,&gt;=1.0.0-&gt;-r ..\requirements.txt (line 2)) (2019.11.28)
Requirement already satisfied: urllib3&lt;1.23,&gt;=1.21.1 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from requests&lt;2.19.0,&gt;=2.18.4-&gt;mxnet&lt;=1.3.1,&gt;=1.0.0-&gt;-r ..\requirements.txt (line 2)) (1.22)
Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from requests&lt;2.19.0,&gt;=2.18.4-&gt;mxnet&lt;=1.3.1,&gt;=1.0.0-&gt;-r ..\requirements.txt (line 2)) (3.0.4)
*****************************************************************************************
Warning: please expect that Model Optimizer conversion might be slow.
You can boost conversion speed by installing protobuf-*.egg located in the
""model-optimizer\install_prerequisites"" folder or building protobuf library from sources.
For more information please refer to Model Optimizer FAQ, question #80.

C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer\install_prerequisites&gt;cd C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\

C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo&gt;demo_squeezenet_download_convert_run.bat
target_precision = FP16
Python 3.6.0
ECHO is off.
PYTHONPATH=C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\open_model_zoo\tools\accuracy_checker;C:\Program Files (x86)\IntelSWTools\openvino\python\python3.6;C:\Program Files (x86)\IntelSWTools\openvino\python\python3;C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\open_model_zoo\tools\accuracy_checker;C:\Program Files (x86)\IntelSWTools\openvino\python\python3.6;C:\Program Files (x86)\IntelSWTools\openvino\python\python3;
[setupvars.bat] OpenVINO environment initialized
INTEL_OPENVINO_DIR is set to C:\Program Files (x86)\IntelSWTools\openvino
Python 3.6.0
ECHO is off.
Requirement already satisfied: pyyaml in c:\users\hreda\appdata\roaming\python\python36\site-packages (from -r C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\\..\open_model_zoo\tools\downloader\requirements.in (line 1)) (5.2)
Requirement already satisfied: requests in c:\users\hreda\appdata\roaming\python\python36\site-packages (from -r C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\\..\open_model_zoo\tools\downloader\requirements.in (line 2)) (2.18.4)
Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from requests-&gt;-r C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\\..\open_model_zoo\tools\downloader\requirements.in (line 2)) (3.0.4)
Requirement already satisfied: certifi&gt;=2017.4.17 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from requests-&gt;-r C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\\..\open_model_zoo\tools\downloader\requirements.in (line 2)) (2019.11.28)
Requirement already satisfied: urllib3&lt;1.23,&gt;=1.21.1 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from requests-&gt;-r C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\\..\open_model_zoo\tools\downloader\requirements.in (line 2)) (1.22)
Requirement already satisfied: idna&lt;2.7,&gt;=2.5 in c:\users\hreda\appdata\roaming\python\python36\site-packages (from requests-&gt;-r C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\\..\open_model_zoo\tools\downloader\requirements.in (line 2)) (2.6)
Download public squeezenet1.1 model
python ""C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\open_model_zoo\tools\downloader\downloader.py"" --name squeezenet1.1 --output_dir C:\Users\hreda\Documents\Intel\OpenVINO\openvino_models\models --cache_dir C:\Users\hreda\Documents\Intel\OpenVINO\openvino_models\cache
################|| Downloading models ||################

========== Retrieving C:\Users\hreda\Documents\Intel\OpenVINO\openvino_models\models\public\squeezenet1.1\squeezenet1.1.prototxt from the cache

========== Retrieving C:\Users\hreda\Documents\Intel\OpenVINO\openvino_models\models\public\squeezenet1.1\squeezenet1.1.caffemodel from the cache

################|| Post-processing ||################

========== Replacing text in C:\Users\hreda\Documents\Intel\OpenVINO\openvino_models\models\public\squeezenet1.1\squeezenet1.1.prototxt
squeezenet1.1 model downloading completed

Waiting for 0 seconds, press a key to continue ...

Target folder C:\Users\hreda\Documents\Intel\OpenVINO\openvino_models\ir\public\squeezenet1.1\FP16 already exists. Skipping IR generation with Model Optimizer.
If you want to convert a model again, remove the entire C:\Users\hreda\Documents\Intel\OpenVINO\openvino_models\ir\public\squeezenet1.1\FP16 folder.

Waiting for 0 seconds, press a key to continue ...

###############|| Generate VS solution for Inference Engine samples using cmake ||###############


Waiting for 0 seconds, press a key to continue ...
Creating Visual Studio 16 2019 x64 files in C:\Users\hreda\Documents\Intel\OpenVINO\inference_engine_samples_build...
-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.17763.
-- The C compiler identification is MSVC 19.24.28314.0
-- The CXX compiler identification is MSVC 19.24.28314.0
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.24.28314/bin/Hostx64/x64/cl.exe
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.24.28314/bin/Hostx64/x64/cl.exe -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.24.28314/bin/Hostx64/x64/cl.exe
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.24.28314/bin/Hostx64/x64/cl.exe -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Check size of __int32
-- Check size of __int32 - done
-- Found InferenceEngine: C:/Program Files (x86)/IntelSWTools/openvino/deployment_tools/inference_engine/lib/intel64/Release/inference_engine.lib (Required is at least version ""2.1"")
-- Performing Test HAVE_CPUID_INFO
-- Performing Test HAVE_CPUID_INFO - Success
-- Host CPU features:
--   3DNOW not supported
--   3DNOWEXT not supported
--   ABM not supported
--   ADX supported
--   AES supported
--   AVX supported
--   AVX2 supported
--   AVX512CD not supported
--   AVX512F not supported
--   AVX512ER not supported
--   AVX512PF not supported
--   BMI1 supported
--   BMI2 supported
--   CLFSH supported
--   CMPXCHG16B supported
--   CX8 supported
--   ERMS supported
--   F16C supported
--   FMA supported
--   FSGSBASE supported
--   FXSR supported
--   HLE not supported
--   INVPCID supported
--   LAHF supported
--   LZCNT supported
--   MMX supported
--   MMXEXT not supported
--   MONITOR supported
--   MOVBE supported
--   MSR supported
--   OSXSAVE supported
--   PCLMULQDQ supported
--   POPCNT supported
--   PREFETCHWT1 not supported
--   RDRAND supported
--   RDSEED supported
--   RDTSCP supported
--   RTM not supported
--   SEP supported
--   SHA not supported
--   SSE supported
--   SSE2 supported
--   SSE3 supported
--   SSE4.1 supported
--   SSE4.2 supported
--   SSE4a not supported
--   SSSE3 supported
--   SYSCALL supported
--   TBM not supported
--   XOP not supported
--   XSAVE supported
-- TBB include: C:/Program Files (x86)/IntelSWTools/openvino/deployment_tools/inference_engine/external/tbb/include
-- TBB Release lib: C:/Program Files (x86)/IntelSWTools/openvino/deployment_tools/inference_engine/lib/intel64/Release/tbb.lib
-- TBB Debug lib: C:/Program Files (x86)/IntelSWTools/openvino/deployment_tools/inference_engine/lib/intel64/Debug/tbb_debug.lib
-- Looking for pthread.h
-- Looking for pthread.h - not found
-- Found Threads: TRUE
-- Configuring done
-- Generating done
-- Build files have been written to: C:/Users/hreda/Documents/Intel/OpenVINO/inference_engine_samples_build

Waiting for 0 seconds, press a key to continue ...

###############|| Build Inference Engine samples using MS Visual Studio (MSBuild.exe) ||###############


Waiting for 0 seconds, press a key to continue ...
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\Bin\MSBuild.exe"" Samples.sln /p:Configuration=Release /t:classification_sample_async /clp:ErrorsOnly /m
Microsoft (R) Build Engine version 16.4.0+e901037fe for .NET Framework
Copyright (C) Microsoft Corporation. All rights reserved.


Waiting for 0 seconds, press a key to continue ...

###############|| Run Inference Engine classification sample ||###############


Waiting for 0 seconds, press a key to continue ...
        1 file(s) copied.
classification_sample_async.exe -i ""C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\\car.png"" -m ""C:\Users\hreda\Documents\Intel\OpenVINO\openvino_models\ir\public\squeezenet1.1\FP16\squeezenet1.1.xml"" -d CPU
[ INFO ] InferenceEngine:
        API version ............ 2.1
        Build .................. 32974
        Description ....... API
[ INFO ] Parsing input parameters
[ INFO ] Parsing input parameters
[ INFO ] Files were added: 1
[ INFO ]     C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\\car.png
[ INFO ] Creating Inference Engine
        CPU
        MKLDNNPlugin version ......... 2.1
        Build ........... 32974

[ INFO ] Loading network files
[ INFO ] Preparing input blobs
[ WARNING ] Image is resized from (787, 259) to (224, 224)
[ INFO ] Batch size is 1
[ INFO ] Loading model to the device
[ ERROR ] Failed to find reference implementation for `377` Layer with `Quantize` Type on constant propagation
Error
</code></pre>
","<p>I just tested the demo_squeezenet_download_convert_run.bat on the latest OpenVINO Release (2021.4.1) and I did not see any issues. Could you try on the latest release and ensure your system meets the minimum <a href=""https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/system-requirements.html"" rel=""nofollow noreferrer"">system requirements</a>?</p>
<p><strong>Processors</strong></p>
<p>6th to 11th generation Intel® Core™ processors</p>
<p>1st to 3rd generation of Intel® Xeon® Scalable processors</p>
<pre><code>###############|| Run Inference Engine classification sample ||###############


Waiting for 0 seconds, press a key to continue ...
    1 file(s) copied.
classification_sample_async.exe -i &quot;C:\Program Files (x86)\Intel\openvino_2021\deployment_tools\demo\car.png&quot; -m &quot;C:\Users\jgespino\Documents\Intel\OpenVINO\openvino_models\ir\public\squeezenet1.1\FP16\squeezenet1.1.xml&quot; -d CPU
[ INFO ] InferenceEngine:
    IE version ......... 2021.4.1
    Build ........... 2021.4.1-3926-14e67d86634-releases/2021/4
[ INFO ] Parsing input parameters
[ INFO ] Files were added: 1
[ INFO ]     C:\Program Files (x86)\Intel\openvino_2021\deployment_tools\demo\car.png
[ INFO ] Loading Inference Engine
[ INFO ] Device info:
    CPU
    MKLDNNPlugin version ......... 2021.4.1
    Build ........... 2021.4.1-3926-14e67d86634-releases/2021/4

[ INFO ] Loading network files:
[ INFO ]     C:\Users\jgespino\Documents\Intel\OpenVINO\openvino_models\ir\public\squeezenet1.1\FP16\squeezenet1.1.xml
[ INFO ] Preparing input blobs
[ WARNING ] Image is resized from (787, 259) to (227, 227)
[ INFO ] Batch size is 1
[ INFO ] Loading model to the device
[ INFO ] Create infer request
[ INFO ] Start inference (10 asynchronous executions)
[ INFO ] Completed 1 async request execution
[ INFO ] Completed 2 async request execution
[ INFO ] Completed 3 async request execution
[ INFO ] Completed 4 async request execution
[ INFO ] Completed 5 async request execution
[ INFO ] Completed 6 async request execution
[ INFO ] Completed 7 async request execution
[ INFO ] Completed 8 async request execution
[ INFO ] Completed 9 async request execution
[ INFO ] Completed 10 async request execution
[ INFO ] Processing output blobs

Top 10 results:

Image C:\Program Files (x86)\Intel\openvino_2021\deployment_tools\demo\car.png

classid probability label
------- ----------- -----
817     0.6853030   sports car, sport car
479     0.1835197   car wheel
511     0.0917197   convertible
436     0.0200694   beach wagon, station wagon, wagon, estate car, beach waggon,         station waggon, waggon
751     0.0069604   racer, race car, racing car
656     0.0044177   minivan
717     0.0024739   pickup, pickup truck
581     0.0017788   grille, radiator grille
468     0.0013083   cab, hack, taxi, taxicab
661     0.0007443   Model T

[ INFO ] Execution successful

[ INFO ] This sample is an API example, for any performance measurements please use the dedicated benchmark_app tool

###############|| Classification demo completed successfully ||###############
</code></pre>
","403","1","1","<windows><intel><openvino>"
"56268851","How to optimize a trained Tensorflow graph for execution speedup?","2019-05-23 05:57:14","<p>in order to do fast CPU inference of a frozen Tensorflow graph (.pb) I am currently using Tensorflow's C API. The inference speed is already fairly good, however (compared to CPU-specific tools like Intel's OpenVINO) I have so far no possibility to somehow optimize the graph before running it. I am interested in any sort of optimization that is suitable:
- device-specific optimization for CPU
- graph-specific optimization (fusing operations, dropping out nodes, ...)
- ... and everything else lowering the time required for inference.</p>

<p>Therefore I am looking for a way to optimize graphs after training and before execution. As mentioned, Tools like Intel's OpenVINO (for CPUs) and NVIDIA's TensorRT (for GPUs) do stuff like that. I am also working with OpenVINO but currently waiting for a bug fix so that I would like to try an additional way.</p>

<p>I thought about trying Tensorflow XLA, but I have no experience using it. Moreover I have to make sure to either get a frozen graph (.pb) or something that I can convert to a frozen graph (e.g. .h5) in the end.</p>

<p>I would be grateful for recommendations!</p>

<p>Greets</p>
","<p>follow these steps:</p>

<ol>
<li>freeze tensorflow trained model (frozen_graph.pb) - for that you may required trained model .pb, checkpoints &amp; output node names</li>
<li>optimize your frozen model with Intel OpenVINO model optimizer - </li>
</ol>

<blockquote>
  <p>python3 mo.py --input_model frozen_graph.pb</p>
</blockquote>

<p>Additionally you may required <code>input_shape</code></p>

<ol start=""3"">
<li>you will get .xml &amp; .bin files as result. with the help of benchmark_app, you can check inference optimisation .</li>
</ol>
","399","1","1","<tensorflow><optimization><deep-learning><openvino><tensorflow-xla>"
"61979800","RuntimeError: Error reading network: cannot parse future versions: 10","2020-05-23 23:19:33","<p>I am trying to run the multi_camera_multi_person_tracking_demo.py from the PyCharm using the openvino library which I installed as follows:</p>

<pre><code>conda install openvino-ie4py -c openvino
</code></pre>

<p>Then I try to run the multi_camera_multi_person_tracking_demo.py script using the following command:</p>

<pre><code>python multi_camera_multi_person_tracking.py -i face-demographics-walking.mp4 --m_detector person-detection-retail-0013.xml --m_reid person-reidentification-retail-0103.xml --config config.py
</code></pre>

<p>After running this command, I get the following error:</p>

<pre><code>    INFO: 2020-05-23 19:08:19: Reading configuration file config.py
INFO: 2020-05-23 19:08:19: Opening file face-demographics-walking.mp4
INFO: 2020-05-23 19:08:19: Creating Inference Engine
INFO: 2020-05-23 19:08:19: Initializing Inference Engine plugin for CPU
INFO: 2020-05-23 19:08:19: Loading network files:
        person-detection-retail-0013.xml
        person-detection-retail-0013.bin
Traceback (most recent call last):
  File ""multi_camera_multi_person_tracking.py"", line 260, in &lt;module&gt;
    main()
  File ""multi_camera_multi_person_tracking.py"", line 248, in main
    capture.get_num_sources())
  File ""C:\Users\ShraddhaM\PycharmProjects\MultiPerson\utils\network_wrappers.py"", line 40, in __init__
    self.net = load_ie_model(ie, model_path, device, None, ext_path, num_reqs=max_num_frames)
  File ""C:\Users\ShraddhaM\PycharmProjects\MultiPerson\utils\ie_tools.py"", line 72, in load_ie_model
    net = IENetwork(model=model_xml, weights=model_bin)
  File ""ie_api.pyx"", line 415, in openvino.inference_engine.ie_api.IENetwork.__cinit__
RuntimeError: Error reading network: cannot parse future versions: 10
</code></pre>

<p>Please help me resolve this issue. Following are the details of my system:</p>

<pre><code>OS: Windows 10 Home, 64-bit 
Python version: `Python 3.7.7 [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32`
PyCharm Community Edition 2019.3.4
cv2 version: 4.2.0
openvino toolkit 2020.2.117.
CPU Device
</code></pre>
","<p>This is an incompatibility issue caused by using an older version with the latest IR v10. Upgrading to the latest OpenVINO version will fix issue.</p>
","391","0","1","<python><pycharm><openvino>"
"63498627","Incorrect freezing of weights maskrcnn Tensorflow 2 in object_detection_API","2020-08-20 05:05:25","<p>I am training the maskrcnn inception v2 model on the Tensorflow version for further work with OpenVino. After training the model, I freeze the model using a script in object_detection_API directory:</p>
<p><strong>python exporter_main_v2.py \
--trained_checkpoint_dir training <br />
--output_directory inference_graph <br />
--pipeline_config_path training/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8.config</strong></p>
<p>After this script, I get the saved model and pipeline files, which should be used in OpenVInO in the future
The following error occurs when uploading the received files to model optimizer:</p>
<p>Model Optimizer version:
2020-08-20 11:37:05.425293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
[ FRAMEWORK ERROR ]  Cannot load input model: TensorFlow cannot read the model file: &quot;C:\Users\Anna\Downloads\inference_graph\inference_graph\saved_model\saved_model.pb&quot; is incorrect TensorFlow model file.
The file should contain one of the following TensorFlow graphs:</p>
<ol>
<li>frozen graph in text or binary format</li>
<li>inference graph for freezing with checkpoint (--input_checkpoint) in text or binary format</li>
<li>meta graph</li>
</ol>
<p>Make sure that --input_model_is_text is provided for a model in text format. By default, a model is interpreted in binary format. Framework error details: Error parsing message.
For more information please refer to Model Optimizer FAQ (<a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html</a>), question #43.</p>
<p>I teach the model by following the example from the link article, using my own dataset: <a href=""https://gilberttanner.com/blog/train-a-mask-r-cnn-model-with-the-tensorflow-object-detection-api"" rel=""nofollow noreferrer"">https://gilberttanner.com/blog/train-a-mask-r-cnn-model-with-the-tensorflow-object-detection-api</a></p>
<p>On gpu, the model starts and works, but I need to get the converted model for OpenVINO</p>
","<p>Run the mo_tf.py script with a path to the SavedModel directory:</p>
<p>python3 mo_tf.py --saved_model_dir &lt;SAVED_MODEL_DIRECTORY&gt;</p>
","379","1","1","<tensorflow><openvino>"
"68181579","YOLOV5 with OPENVINO on CPU","2021-06-29 15:22:41","<p>I am trying to test:</p>
<p>only yolov5 without openvino : ~ 4 FPS <br>
yolov5 with openvino  : ~ 2 FPS <br></p>
<p>That is abnormal, can you give me any suggest or any comparison between two methods if you already compared between them.</p>
","<p>YOLOv5 is officially validated and supported on OpenVINO 2021.4. Might be the result that you obtained is based on a non-validated version. As such, I would recommend you to use the latest version of OpenVINO Toolkit.
In addition, YOLOv5 is available in four models, namely s, m, l, and x, each one of them offering different detection accuracy and performance. Refer to <a href=""https://github.com/ultralytics/yolov5#why-yolov5"" rel=""nofollow noreferrer"">YOLOv5</a> for more information.</p>
","362","0","1","<computer-vision><frame-rate><openvino><yolov4><yolov5>"
"65992940","Openvino MFX: Unsupported extension: rtsp://cris:pass@192.167.1.175:444/live/ch0 , Ubuntu","2021-02-01 12:44:27","<p>HI I'm running python Opencv motion detection script on Ubuntu 20.04.1 LTS  which is working fine, after running <code>source /opt/intel/openvino_2021/bin/setupvars.sh </code> and if I run it with OpenVINO 2021.2.185 then I got <code>Openvino MFX: Unsupported extension: rtsp://cris:pass@192.167.1.175:444/live/ch0</code> and If I change it from IP camera <code>src='rtsp://cris:pass@192.167.1.175:444/live/ch0'</code>  to  webcam  <code>src=0</code>  then everything works fine .</p>
<p>Can you please help me please ??</p>
<p>Below is the error</p>
<pre><code>[ WARN:0] global ../opencv/modules/videoio/src/cap_gstreamer.cpp (1766) handleMessage OpenCV | GStreamer warning: your GStreamer installation is missing a required plugin
[ WARN:0] global ../opencv/modules/videoio/src/cap_gstreamer.cpp (1781) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module uridecodebin0 reported: No URI handler implemented for &quot;rtsp&quot;.
[ WARN:0] global ../opencv/modules/videoio/src/cap_gstreamer.cpp (909) open OpenCV | GStreamer warning: unable to start pipeline
[ WARN:0] global ../opencv/modules/videoio/src/cap_gstreamer.cpp (501) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created
libva info: VA-API version 1.8.0
libva info: User environment variable requested driver 'iHD'
libva info: Trying to open /opt/intel/mediasdk/lib64/iHD_drv_video.so
libva info: Found init function __vaDriverInit_1_8
libva info: va_openDriver() returns 0
MFX: Unsupported extension: rtsp://cris:pass@192.167.1.175:444/live/ch0
[ WARN:0] global ../opencv/modules/videoio/src/cap_gstreamer.cpp (1601) open OpenCV | GStreamer warning: cannot link elements
libva info: VA-API version 1.8.0
libva info: User environment variable requested driver 'iHD'
libva info: Trying to open /opt/intel/mediasdk/lib64/iHD_drv_video.so
libva info: Found init function __vaDriverInit_1_8
libva info: va_openDriver() returns 0
MFX: Unsupported FourCC: XVID (0x44495658)
[ERROR:0] global ../opencv/modules/videoio/src/cap.cpp (561) open VIDEOIO(CV_IMAGES): raised OpenCV exception:

OpenCV(4.5.1-openvino) ../opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): video.avi in function 'icvExtractPattern'
</code></pre>
","<p>You may refer <a href=""https://github.com/alexa/avs-device-sdk/issues/524"" rel=""nofollow noreferrer"">here</a> to troubleshoot this[ WARN:0] global ../opencv/modules/videoio/src/cap_gstreamer.cpp (1766) handleMessage OpenCV | GStreamer warning: your <strong>GStreamer installation is missing a required plugin</strong></p>
<p>Next, for the error of Openvino MFX: Unsupported extension: rtsp://cris:pass@192.167.1.175:444/live/ch0, please refer to this <a href=""https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/OpenCV-4-1-2-openvino-CAP-IMAGES-can-t-find-starting-number-in/td-p/1129111#comment-1959910"" rel=""nofollow noreferrer"">thread</a></p>
","360","-1","1","<python><opencv><ubuntu><ip-camera><openvino>"
"62573318","How to find the exact name of the output node in .pb file?","2020-06-25 10:31:59","<p>I was trying to freeze a pb file for using in the OpenVino. For freezing, i need to know the output node name. For that, i tried loading the pb file and reading the output names, but, it got an error. Then i tried to get the output name from the model summary and it was dense_7 (Dense).</p>
<p>I followed the command in <a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html#freeze-the-tensorflow-model"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html#freeze-the-tensorflow-model</a> to freeze the model. But when i try the output node name as dense_7 (Dense), I'm getting the following error:</p>
<blockquote>
<p>AssertionError: dense_7 (Dense) is not in graph</p>
</blockquote>
<p>I'm attaching the model summary along with this.<a href=""https://i.stack.imgur.com/a6Z8y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a6Z8y.png"" alt=""enter image description here"" /></a></p>
<p>Is there any way to read the correct output name?</p>
","<ul>
<li><p>Question 26 in the ‘Model Optimizer Frequently Asked Questions’ page addresses the error that you are facing. See <a href=""https://docs.openvinotoolkit.org/2020.3/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/2020.3/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html</a></p>
</li>
<li><p>I would suggest you try the following potential methods that have been shared in Github, available at the following links:</p>
<p><a href=""https://github.com/tensorflow/tensorflow/issues/3986#issuecomment-304619868"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/3986#issuecomment-304619868</a>
<a href=""https://github.com/tensorflow/tensorflow/issues/3986#issuecomment-368282326"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/3986#issuecomment-368282326</a>
<a href=""https://github.com/tensorflow/tensorflow/issues/3986#issuecomment-470083664"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/3986#issuecomment-470083664</a>
<a href=""https://github.com/tensorflow/tensorflow/issues/3986#issuecomment-508897579"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/3986#issuecomment-508897579</a></p>
</li>
<li><p>Do let us know if any of these works for you.</p>
</li>
</ul>
","359","1","1","<tensorflow><tensorflow2.0><openvino>"
"64539024","Fail to run OpenVINO demo","2020-10-26 14:23:48","<p>I am trying to install OpenVINO in a custom location (Windows 10). So far installation occurs without error. However, when I try to run any of the samples, I get this output:</p>
<pre><code>.\demo_benchmark_app.bat
target_precision = FP16
Python 3.7.0
[setupvars.bat] OpenVINO environment initialized
INTEL_OPENVINO_DIR is set to C:\&lt;user_dir&gt;\openvino\install
Python 3.7.0
ECHO is off.
ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'C:\\&lt;user_dir&gt;\\openvino\\install\\deployment_tools\\demo\\..\\open_model_zoo\\tools\\downloader\\requirements.in'
Error
</code></pre>
<p>I do not have a folder <code>..\\open_model_zoo</code>. Was this supposed to be created during installation? Can I download and/or create the folder post installation?</p>
<p>Below are my installation commands:</p>
<pre><code>cd &lt;user_dir&gt;
mkdir openvino  
cd openvino/  
git clone https://github.com/openvinotoolkit/openvino.git .  
git submodule update --init --recursive  
mkdir build  
mkdir install  
cd build  
cmake -G &quot;Visual Studio 16 2019&quot; -A x64 -DCMAKE_BUILD_TYPE=Release -DNGRAPH_ONNX_IMPORT_ENABLE=ON -DCMAKE_INSTALL_PREFIX:PATH=&quot;C:\&lt;user_dir&gt;\openvino\install&quot; ..
cmake --build . --config Release  
cmake --install . --config Release  
</code></pre>
","<p>Open Model Zoo is a separate folder. It is not created during 'build from source' installation. You can clone the Open Model Zoo repo from this GitHub link: <a href=""https://github.com/openvinotoolkit/open_model_zoo.git"" rel=""nofollow noreferrer"">https://github.com/openvinotoolkit/open_model_zoo.git</a>.</p>
<p>Then, place the cloned repo in the <em>deployment_tools</em> directory.</p>
","358","1","1","<openvino>"
"61300750","TypeError cv2.rect","2020-04-19 07:05:34","<p>Am trying to implement an object detection model on an image using <strong>ssd_inception_v2_coco_2018_01_28</strong> model.<br> Am trying to get the output with a bounding box on top. <br></p>

<p>1) ssd_inception_v2_coco_2018_01_28 object detection model. Successfully converted to IR.</p>

<p>2) Taken an input image and preprocessed it according to the input layer information of the model.</p>

<p>3) Successfully extracted the model outputs.</p>

<p>4) Added bounding boxes to the output image.</p>

<p>But am getting the following error, please help!</p>

<p><br><br><strong>Source Code</strong></p>

<pre><code>#Preprocessing image

img = r'D:\SriSailam.jpg'
img = cv2.imread(img)
img = cv2.resize(img,(300,300))
img = img.transpose((2,0,1))
img = img.reshape(1,3,300,300)



exec_net = engine.load_network(net,'cpu')


#Asynchronous request

infer_request_handle = exec_net.start_async(request_id=0, inputs={'image_tensor': img}) 
infer_status = infer_request_handle.wait()
output = next(iter(net.outputs))


n,c,h,w = img.shape

result = infer_request_handle.outputs[output]  #output layer shape


'''
Draw bounding boxes onto the image.
'''
for box in result[0][0]: # Output shape is 1x1x100x7
        conf = box[2]  #box[2] = result[0][0][2] is a probability value
        print(""probability value : "",conf)
        print(""box[0] : "",box[0])
        print(""box[1] : "",box[1])
        print(""xmin : "", box[3])
        print(""ymin : "", box[4])
        print(""xmax : "", box[5])
        print(""ymax : "", box[6])

        if conf &gt;= 0:
            xmin = int(box[3] * w)  #box[3] - xmin
            ymin = int(box[4] * h) #box[4] - ymin
            xmax = int(box[5] * w)  #box[5] - xmax
            ymax = int(box[6] * h) #box[6] - ymax
            rect = cv2.rectangle(img, ((xmin), (ymin)), ((xmax), (ymax)), (0, 0, 255), 1)
cv2.imshow(rect,'face_detection')
cv2.waitKey(0)
</code></pre>

<p><br><br><strong>Output</strong></p>

<pre><code>C:\Users\adity\Desktop\openvino&gt;py check_infer.py                                                                                                                        
probability value :  0.6344592                                                                                                                                           
box[0] :  0.0                                                                                                                                                            
box[1] :  1.0                                                                                                                                                            
xmin :  0.1831626                                                                                                                                                        
ymin :  0.10697469                                                                                                                                                       
xmax :  0.86960804                                                                                                                                                       
ymax :  1.0                                                                                                                                                              
Traceback (most recent call last):                                                                                                                                       
  File ""check_infer.py"", line 68, in &lt;module&gt;                                                                                                                            
    rect = cv2.rectangle(img, ((xmin), (ymin)), ((xmax), (ymax)), (0, 0, 255), 1)                                                                                        
TypeError: an integer is required (got type tuple) 
</code></pre>
","<p>Reproduced your error.
The error message is pretty confusing in this version of OpenCV (4.2.0). </p>

<p>If you do the same in OpenCV 4.3.0 you will get a more intelligible error like</p>

<pre><code>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: Expected Ptr&lt;cv::UMat&gt; for argument 'img'
</code></pre>

<p>Which basically says that <code>img</code> argument you passed into <code>cv2.rectangle</code> has a wrong type.
The problem is that after you call </p>

<pre><code>img = img.transpose((2,0,1))
img = img.reshape(1,3,300,300)

</code></pre>

<p><code>img</code> cannot be interpreted as an image by OpenCV.
The issue can be solved, for example, this way:<br>
1. Copy <code>img</code> before the transformation to another variable <code>imgForDisplaying</code><br>
2. Use <code>imgForDisplaying</code> to draw rectangle.</p>

<pre><code>img = r'D:\SriSailam.jpg'
img = cv2.imread(img)
img = cv2.resize(img,(300,300))
imgToDisplay = img
img = img.transpose((2,0,1))
img = img.reshape(1,3,300,300)
...
rect = cv2.rectangle(imgToDisplay, (xmin, ymin), (xmax, ymax), (0, 0, 255), 1)
</code></pre>
","357","1","1","<python><opencv><object-detection-api><openvino>"
"61565285","OpenVino on MacOS using Anaconda","2020-05-02 19:40:56","<p>How do you go about installing OpenVino using Anaconda. Will this be possible with prerequisites like OpenCV and CMake that are required for installation.</p>

<pre><code>(IntelEnv) ChrNinja@ChrNinja demo % ./demo_squeezenet_download_convert_run.sh
target_precision = FP16
[setupvars.sh] OpenVINO environment initialized


###################################################



Downloading the Caffe model and the prototxt
Installing dependencies
DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support
Processing /Users/christianh/Library/Caches/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd/PyYAML-5.3.1-cp27-cp27m-macosx_10_15_x86_64.whl
Collecting requests
  Using cached https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl
Collecting idna&lt;3,&gt;=2.5
 Using cached https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl
Collecting chardet&lt;4,&gt;=3.0.2
  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl
Collecting certifi&gt;=2017.4.17
  Using cached https://files.pythonhosted.org/packages/57/2b/26e37a4b034800c960a00c4e1b3d9ca5d7014e983e6e729e33ea2f36426c/certifi-2020.4.5.1-py2.py3-none-any.whl
Collecting urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1
  Using cached https://files.pythonhosted.org/packages/e1/e5/df302e8017440f111c11cc41a6b432838672f5a70aa29227bf58149dc72f/urllib3-1.25.9-py2.py3-none-any.whl
Installing collected packages: pyyaml, idna, chardet, certifi, urllib3, requests
ERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/PyYAML-5.3.1.dist-info'
Consider using the `--user` option or check the permissions.

Error on or near line 150; exiting with status 1
</code></pre>
","<h1>1. Update conda to latest version:</h1>
<p>conda update --all</p>
<h1>2. Install the Intel® Distribution of OpenVINO™ Toolkit:</h1>
<p>conda install openvino-ie4py -c openvino</p>
<h1>3. Install OpenCV. This is required to run a sample:</h1>
<p>conda install opencv</p>
<h1>4. Verify the package installed:</h1>
<p>python -c “import openvino”</p>
","352","0","1","<macos><conda><openvino>"
"62254080","How to use multi Intel NCS2 on one task?","2020-06-08 02:25:18","<p>I saw some docs on Intel openvino website. And there is some docs about how to use only one NCS2, the performance is great. Now I have two NCS2, I want to test both of them on a platform, but there is no reference that how to use multi ncs2 to work on one task.</p>
","<p>The OpenVINO™ toolkit (>=2019 R2) introduced a Multi-Device Plugin that automatically assigns inference requests to available devices in order to execute the requests in parallel. What this does is allow you to use the Multi-Device Plugin with multiple Intel® Neural Compute Stick 2 devices. </p>

<p>The benchmark_app in <a href=""https://docs.openvinotoolkit.org/latest/_inference_engine_samples_benchmark_app_README.html"" rel=""nofollow noreferrer"">C++</a>/<a href=""https://docs.openvinotoolkit.org/latest/_inference_engine_tools_benchmark_tool_README.html"" rel=""nofollow noreferrer"">Python</a> is a good starting point to check how such plugin works. If you'd like to test drive this Multi-Device plugin, my recommendation would be to follow the article <a href=""https://www.intel.com/content/www/us/en/support/articles/000055294/boards-and-kits/neural-compute-sticks.html"" rel=""nofollow noreferrer"">here</a> as it contains a comprehensive and detailed walk-through of testing this feature on both Windows and Linux environments.</p>

<p>The typical ""setup"" of multi-device can be described in three major steps:</p>

<ol>
<li>Configuration of each device as usual (e.g. via conventional
SetConfig method)</li>
<li>Loading of a network to the Multi-Device plugin created on top of
(prioritized) list of the configured devices. </li>
<li>Just like with any other ExecutableNetwork (resulted from
LoadNetwork) you just create as many requests as needed to saturate
the devices.</li>
</ol>

<p>For this and more detailed information check <a href=""https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_MULTI.html#defining_and_configuring_the_multi_device"" rel=""nofollow noreferrer"">Multi-Device Plugin documentation</a>. </p>
","351","-1","1","<openvino>"
"63794856","Cannot import OpenCv module in Python","2020-09-08 13:21:53","<p>I try to run the python script using OpenVINO, so I write <code>&quot;setupvars&quot;</code> and run a script by command line as &quot;python main.py&quot;.</p>
<p><strong>It tells me this:</strong></p>
<blockquote>
<p>Traceback (most recent call last): File &quot;main.py&quot;, line 1, in import
cv2 ImportError: DLL load failed while importing cv2: Module not
found. If I run the script not initializing OpenVINO</p>
</blockquote>
<p>It works properly. What is my problem, guys?</p>
<p>Thank you for your answers</p>
","<p>You need to install OpenCV module at python terminal
go to the window termianal and command</p>
<pre><code>'''
pip install opencv-python
'''
</code></pre>
<p>then you can try your script again</p>
","350","0","2","<python><opencv><openvino>"
"63794856","Cannot import OpenCv module in Python","2020-09-08 13:21:53","<p>I try to run the python script using OpenVINO, so I write <code>&quot;setupvars&quot;</code> and run a script by command line as &quot;python main.py&quot;.</p>
<p><strong>It tells me this:</strong></p>
<blockquote>
<p>Traceback (most recent call last): File &quot;main.py&quot;, line 1, in import
cv2 ImportError: DLL load failed while importing cv2: Module not
found. If I run the script not initializing OpenVINO</p>
</blockquote>
<p>It works properly. What is my problem, guys?</p>
<p>Thank you for your answers</p>
","<p>Can you show us your python setup?</p>
<p>try:</p>
<pre><code>python3 --version
</code></pre>
<p>and then try</p>
<pre><code>pip3 freeze
</code></pre>
<p>If you do not see opencv in that list, it means it was not installed properly.</p>
<p>I had the same issue as I was using Python 3.8+ and apparently they don't support that. Right now OpenVINO supports only from 3.5-3.7.</p>
<p>Also, you should not need to install the opencv again using the pip install command. If anything, you should reinstall the OpenVINO toolkit again after you got Python 3.7 or so installed. (You can reinstall by re-running the toolkit installer again).</p>
","350","0","2","<python><opencv><openvino>"
"56100546","How to add batch file in windows environment variable","2019-05-12 15:08:43","<p>I am following the <a href=""https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_windows.html"" rel=""nofollow noreferrer"">installation guide</a> for openvino on Windows. After a successful installation, we need to run <code>setupvars.bat</code> file in order to initialize the openvino environment. We can also <a href=""https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_windows.html#set-the-environment-variables"" rel=""nofollow noreferrer"">add it permanently in environment variable</a> so that it is initialized automatically. But the instructions are not given on how to add it.</p>

<p>I wanted to know if its possible to add the <code>setupvars.bat</code> so the it runs automatically. Also I need to run the visual studio from the same environment. </p>

<p>I always do this step manually which takes a lot time. I first open a <code>cmd</code>, navigate to the desired folder and then run <code>setupvars.bat</code>. After that from the same <code>cmd</code>, I navigate to the visual studio installed directory and then start the visual studio from the <code>cmd</code> so that visual studio is launched under same openvino environment. Is is possible to automate all this task. Thanks</p>
","<p><strong>Solution 1</strong>: You can set the environment variables for the visual studio as mentioned here - <a href=""https://stackoverflow.com/questions/100543/how-do-i-set-specific-environment-variables-when-debugging-in-visual-studio"">How do I set specific environment variables when debugging in Visual Studio?</a></p>

<p><strong>Solution 2</strong>: Write one more batch file which will 1st call setupvars.bat &amp; then open the visual studio. Then you can run the new bat file. </p>

<p><strong>Solution 3:</strong> You can create a cmd shortcut like this - <a href=""https://stackoverflow.com/questions/9738434/run-a-command-prompt-command-from-desktop-shortcut"">Run a Command Prompt command from Desktop Shortcut</a> 
In this, you can 1st call setupvars.bat and then cmd to open Visual Studio. Once you click the new shortcut both the things should happen automatically. You can even add this step along with the installer. </p>
","347","0","1","<windows><environment-variables><openvino>"
"58427555","OpenCV using Movidius NCS2 gives error when using Darknet neural network","2019-10-17 07:52:14","<p>I am trying to perform OCR using the Intel Movidius Neural Compute Stick 2. The OCR network that I am using is based on YOLO, and therefore has the graph structure saved as a <code>.cfg</code> file and the weights as a <code>.weights</code> file. I use OpenCV which was installed with Openvino on a Raspberry Pi 3b+. My code is given below:</p>

<pre class=""lang-py prettyprint-override""><code>import cv2

# load network
ocr_net  = cv2.dnn.readNetFromDarknet('ocr-net.cfg', 'ocr-net.weights')
ocr_net.setPreferableTarget(cv2.dnn.DNN_TARGET_MYRIAD)
ocr_net.setPreferableBackend(cv2.dnn.DNN_BACKEND_INFERENCE_ENGINE)

# perform inference
im = cv2.imread('img_path.jpg')
blob = cv2.dnn.blobFromImage(im, swapRB=False)
ocr_net.setInput(blob)
result = ocr_net.forward()

</code></pre>

<p>On running this code, I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""test.py"", line 12, in &lt;module&gt;
    result = ocr_net.forward()
cv2.error: OpenCV(4.1.2-openvino) /home/jenkins/workspace/OpenCV/OpenVINO/build/opencv/modules/dnn/src/op_inf_engine.cpp:704: error: (-215:Assertion failed) Failed to initialize Inference Engine backend: Device with ""CPU"" name is not registered in the InferenceEngine in function 'initPlugin'

</code></pre>

<p>If I change the inference target to <code>cv2.dnn.DNN_TARGET_CPU</code> and the inference backend to <code>cv2.dnn.DNN_BACKEND_OPENCV</code>, the model works fine and is able to correctly perform OCR. My openvino/openCV setup is also correctly installed since I can correctly run other Tensorflow/Caffe models using openCV (using <code>readNetFromTensorflow()</code>/<code>readNetFromCaffe()</code>).</p>

<p>The OCR network that I am attempting to use can be found here: <a href=""https://www.inf.ufrgs.br/~smsilva/alpr-unconstrained/data/ocr/ocr-net.cfg"" rel=""nofollow noreferrer"">cfg</a> , <a href=""https://www.inf.ufrgs.br/~smsilva/alpr-unconstrained/data/ocr/ocr-net.weights"" rel=""nofollow noreferrer"">weights</a> .</p>

<p>Any help is appreciated!</p>
","<p>From what I remember of Intel's documentation, the openVINO distribution of opencv.dnn cannot handle darknet models. On <a href=""https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_Supported_Frameworks_Layers.html"" rel=""nofollow noreferrer"">this page</a> you can find a list of what I assume can be interpreted as all supported frameworks.</p>

<p>I'm currently trying to convert my darknet custom models to a tf.pb representation, no luck so far.</p>
","343","0","1","<python><opencv><intel><openvino><movidius>"
"63023826","Converting a saved tensorflow model to IR using openvino model optimizer","2020-07-21 22:08:55","<p>I trained a custom CNN model using keras and tensorflow 2.2.0 as background. After that, I saved the model as .ckpt file having assets, variables, .pb file as subfolders init. After that to convert it into IR in openvino documentation it is given that we have use the following command:</p>
<p>**To convert such TensorFlow model:
Go to the &lt;INSTALL_DIR&gt;/deployment_tools/model_optimizer directory</p>
<p>Run the mo_tf.py script with a path to the SavedModel directory to convert a model:</p>
<p>python3 mo_tf.py --saved_model_dir &lt;SAVED_MODEL_DIRECTORY&gt;**</p>
<p>so, I went to the following directory as mentioned and tired the following command:</p>
<p><strong>python3 mo_tf.py --saved_model_dir C:\Users\vyas\Desktop\saved_model\cp.ckpt</strong></p>
<p>There is no output or anything. There is no error also.
Also, I tried the following command:</p>
<p><strong>python3 mo_tf.py --saved_model_dir C:\Users\vyas\Desktop\saved_model\cp.ckpt --output_dir C:\Users\vyas\Desktop\out</strong></p>
<p>Still there is no output.</p>
<p>Can someone please help.</p>
<p><strong>I am using tensorflow 2.2.0</strong></p>
<p>Can someone please help me with this</p>
","<p>--saved_model_dir must provide a path to the SavedModel directory.</p>
<p>Modify your command as follows:</p>
<p>python3 mo_tf.py --saved_model_dir C:\Users\vyas\Desktop\saved_model</p>
","342","0","1","<tensorflow><openvino>"
"59930994","Get parameters/weights for each layer of the model using the c++ API on OpenVINO","2020-01-27 12:15:30","<p>I have been looking for a way to get the tensor of weights/parameters and biases for each layer of the network using the C++ API on the OpenVINO framework. I can't find anything on the documentation nor any example on the samples. How could I extract these tensors?</p>

<p>Thanks,
César.</p>

<p>EDIT:
Code for getting weights and biases separately:</p>

<pre><code>for (auto&amp;&amp; layer : this-&gt;pImplementation-&gt;network) {
        weightsbuf &lt;&lt; ""Layer name: "" &lt;&lt; layer-&gt;name &lt;&lt; std::endl;
        weightsbuf &lt;&lt; ""Parameters:"" &lt;&lt; std::endl;

        for (auto&amp;&amp; param : layer-&gt;params) {

            weightsbuf &lt;&lt; '\t' &lt;&lt; param.first &lt;&lt; "": "" &lt;&lt; param.second &lt;&lt; std::endl;
        }

        std::vector&lt;int&gt; kernelvect;
        auto kernelsize = layer-&gt;params.at(""kernel"");

        std::stringstream ss(kernelsize);

        // split by comma kernel size
        for (int i; ss &gt;&gt; i;) {
            kernelvect.push_back(i);
            if (ss.peek() == ',')
                ss.ignore();
        }
        int noutputs = std::stoi(layer-&gt;params.at(""output""));
        int nweights = kernelvect[0] * kernelvect[1] * noutputs;
        int nbias = noutputs;

        for (auto&amp;&amp; blob : layer-&gt;blobs) {
            weightsbuf &lt;&lt; '\t' &lt;&lt; blob.first &lt;&lt; "": "";
            for (size_t w = 0; w &lt; nweights; ++w) {
                weightsbuf &lt;&lt; blob.second-&gt;buffer().as&lt;float*&gt;()[w] &lt;&lt; "" "";
            }
            weightsbuf &lt;&lt; std::endl;
            weightsbuf &lt;&lt; '\t' &lt;&lt; ""biases:"";
            for (size_t b = 0; b &lt; nbias; ++b) {
                weightsbuf &lt;&lt; blob.second-&gt;buffer().as&lt;float*&gt;()[nweights + b] &lt;&lt; "" "";
            }
        }
        weightsbuf &lt;&lt; std::endl;
    }
</code></pre>
","<p>Looks like there is no official example to show that functionality. I haven't found anything like that as well.</p>

<p>I implemented a basic sample which prints information about each layer of a network. Please take a look: <a href=""https://github.com/ArtemSkrebkov/dldt/blob/askrebko/iterate-through-network/inference-engine/samples/cnn_network_parser/main.cpp"" rel=""nofollow noreferrer"">https://github.com/ArtemSkrebkov/dldt/blob/askrebko/iterate-through-network/inference-engine/samples/cnn_network_parser/main.cpp</a></p>

<p>I believe the idea how to use API is clear.</p>

<p>The sample is based on the current state of the dldt repo (branch '2019', it corresponds to the release 2019 R3.1) </p>

<p>Another link, which might be useful, is the documentation on CNNLayer class:
<a href=""https://docs.openvinotoolkit.org/latest/classInferenceEngine_1_1CNNLayer.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/classInferenceEngine_1_1CNNLayer.html</a> </p>
","342","1","1","<c++><neural-network><intel><openvino>"
"60316210","squeezenet1.1.caffemodel"" is not existing file","2020-02-20 09:04:26","<p>I have installed all the required steps for openvino. but while running the demos, to make sure that every thing is working fine , I am recieving this error : <strong>The ""C:\Users\OGNGHAFF\Documents\Intel\OpenVINO\openvino_models\models\public\squeezenet1.1/squeezenet1.1.caffemodel"" is not existing file</strong></p>

<p>I couldn't find any solution. </p>

<pre><code>cd C:\Program Files (x86)\IntelSWTools\openvino\bin\
setupvars.bat
</code></pre>

<p>This is working fine. Then I am running initilazation: </p>

<pre><code>cd C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer\install_prerequisites
install_prerequisites.bat
</code></pre>

<p>This is also working fine, but then when I want to run demo :</p>

<pre><code>cd C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\
demo_squeezenet_download_convert_run.bat
</code></pre>

<p>I am recieving this error : </p>

<pre><code>

**Download public squeezenet1.1 model
python ""C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\open_model_zoo\tools\downloader\downloader.py"" --name ""squeezenet1.1"" --output_dir ""C:\Users\OGNGHAFF\Documents\Intel\OpenVINO\openvino_models\models"" --cache_dir ""C:\Users\OGNGHAFF\Documents\Intel\OpenVINO\openvino_models\cache""
################|| Downloading models ||################

========== Downloading C:\Users\OGNGHAFF\Documents\Intel\OpenVINO\openvino_models\models\public\squeezenet1.1\squeezenet1.1.prototxt
Traceback (most recent call last):
  File ""C:\Users\OGNGHAFF\AppData\Roaming\Python\Python36\site-packages\urllib3\connectionpool.py"", line 601, in urlopen
    chunked=chunked)
  File ""C:\Users\OGNGHAFF\AppData\Roaming\Python\Python36\site-packages\urllib3\connectionpool.py"", line 346, in _make_request
    self._validate_conn(conn)
  File ""C:\Users\OGNGHAFF\AppData\Roaming\Python\Python36\site-packages\urllib3\connectionpool.py"", line 850, in _validate_conn
    conn.connect()
  File ""C:\Users\OGNGHAFF\AppData\Roaming\Python\Python36\site-packages\urllib3\connection.py"", line 326, in connect
    ssl_context=context)
  File ""C:\Users\OGNGHAFF\AppData\Roaming\Python\Python36\site-packages\urllib3\util\ssl_.py"", line 329, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File ""C:\Users\OGNGHAFF\AppData\Local\Programs\Python\Python36\lib\ssl.py"", line 407, in wrap_socket
    _context=self, _session=session)
  File ""C:\Users\OGNGHAFF\AppData\Local\Programs\Python\Python36\lib\ssl.py"", line 814, in __init__
    self.do_handshake()
  File ""C:\Users\OGNGHAFF\AppData\Local\Programs\Python\Python36\lib\ssl.py"", line 1068, in do_handshake
    self._sslobj.do_handshake()
  File ""C:\Users\OGNGHAFF\AppData\Local\Programs\Python\Python36\lib\ssl.py"", line 689, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\OGNGHAFF\AppData\Roaming\Python\Python36\site-packages\requests\adapters.py"", line 440, in send
    timeout=timeout
  File ""C:\Users\OGNGHAFF\AppData\Roaming\Python\Python36\site-packages\urllib3\connectionpool.py"", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""C:\Users\OGNGHAFF\AppData\Roaming\Python\Python36\site-packages\urllib3\util\retry.py"", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /DeepScale/SqueezeNet/a47b6f13d30985279789d08053d37013d67d131b/SqueezeNet_v1.1/deploy.prototxt (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)'),))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\open_model_zoo\tools\downloader\downloader.py"", line 71, in try_download
    chunk_iterable = start_download()
  File ""C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\open_model_zoo\tools\downloader\downloader.py"", line 261, in &lt;lambda&gt;
    lambda: model_file.source.start_download(session, CHUNK_SIZE)):
  File ""C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\open_model_zoo\tools\downloader\common.py"", line 185, in start_download
    response = session.get(self.url, stream=True, timeout=DOWNLOAD_TIMEOUT)
  File ""C:\Users\OGNGHAFF\AppData\Roaming\Python\Python36\site-packages\requests\sessions.py"", line 521, in get
    return self.request('GET', url, **kwargs)
  File ""C:\Users\OGNGHAFF\AppData\Roaming\Python\Python36\site-packages\requests\sessions.py"", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File ""C:\Users\OGNGHAFF\AppData\Roaming\Python\Python36\site-packages\requests\sessions.py"", line 618, in send
    r = adapter.send(request, **kwargs)
  File ""C:\Users\OGNGHAFF\AppData\Roaming\Python\Python36\site-packages\requests\adapters.py"", line 506, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /DeepScale/SqueezeNet/a47b6f13d30985279789d08053d37013d67d131b/SqueezeNet_v1.1/deploy.prototxt (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)'),))
########## Error: Download failed**

</code></pre>
","<p>As suggested in the comments above, make sure to configure your proxy settings for pip if you are behind a proxy server.</p>
<p>Try configuring your <code>http_proxy</code> and <code>https_proxy</code> variables as follows:</p>
<pre><code>set http_proxy=http://[username:password@]proxyserver:port
set https_proxy=https://[username:password@]proxyserver:port
</code></pre>
","336","-1","1","<python><neural-network><conv-neural-network><intel><openvino>"
"63064803","Cannot CMake NGraph on Raspberry Pi due to NGRAPH_VERSION","2020-07-23 23:55:25","<p>The full build log is below</p>
<p>I am building following NGraph instructions on <a href=""https://github.com/NervanaSystems/ngraph"" rel=""nofollow noreferrer"">https://github.com/NervanaSystems/ngraph</a></p>
<p>Looking at these I am assuming these variables</p>
<pre><code>NGRAPH_VERSION
NGRAPH_VERSION_SHORT
NGRAPH_API_VERSION
</code></pre>
<p>did not get set for some reason, even though the instructions never specifically require them to be set</p>
<p>My installation steps are:</p>
<pre><code>git clone https://github.com/csullivan/ngraph.git
cd ngraph
mkdir build
cd build
cmake .. -DCMAKE_C_COMPILER=arm-linux-gnueabihf-gcc -DCMAKE_CXX_COMPILER=arm-linux-gnueabihf-g++ -DCMAKE_INSTALL_PREFIX=/home/pi/openvino_files/ngraph/install
make -j install
</code></pre>
<p>I tried setting them manually in the command line and in the cmake file, adding</p>
<pre><code>SET(NGRAPH_VERSION 1)
SET(NGRAPH_VERSION_SHORT 1)
SET(NGRAPH_API_VERSION 1)
</code></pre>
<p>but the error still did not go away. And i don't want to have to set the version manually every time.</p>
<p>Something else is wrong here, what am I doing wrong?
What command do I need to type in to make it build correctly?</p>
<p>Thanks,</p>
<pre><code>pi@raspberrypi:~/openvino_files/ngraph/build $ cmake .. -DCMAKE_C_COMPILER=arm-linux-gnueabihf-gcc -DCMAKE_CXX_COMPILER=arm-linux-gnueabihf-g++ -DCMAKE_INSTALL_PREFIX=/home/pi/openvino_files/ngraph/install
-- Found Git: /usr/bin/git (found version &quot;2.20.1&quot;)
From https://github.com/csullivan/ngraph.git
CMake Error at cmake/Modules/git_tags.cmake:39 (string):
  string sub-command REGEX, mode MATCH needs at least 5 arguments total to
  command.
Call Stack (most recent call first):
  cmake/Modules/git_tags.cmake:60 (NGRAPH_GET_TAG_OF_CURRENT_HASH)
  CMakeLists.txt:21 (NGRAPH_GET_VERSION_LABEL)


CMake Error at CMakeLists.txt:27 (list):
  list GET given empty list


CMake Error at CMakeLists.txt:28 (list):
  list GET given empty list


CMake Error at CMakeLists.txt:29 (list):
  list GET given empty list


-- NGRAPH_VERSION
-- NGRAPH_VERSION_SHORT
-- NGRAPH_API_VERSION
CMake Deprecation Warning at CMakeLists.txt:41 (cmake_policy):
  The OLD behavior for policy CMP0042 will be removed from a future version
  of CMake.

  The cmake-policies(7) manual explains that the OLD behaviors of all
  policies are deprecated and that a policy should be set to OLD only under
  specific short-term circumstances.  Projects should be ported to the NEW
  behavior and not rely on setting a policy to OLD.


-- The C compiler identification is GNU 8.3.0
-- The CXX compiler identification is GNU 8.3.0
-- Check for working C compiler: /usr/bin/arm-linux-gnueabihf-gcc
-- Check for working C compiler: /usr/bin/arm-linux-gnueabihf-gcc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/arm-linux-gnueabihf-g++
-- Check for working CXX compiler: /usr/bin/arm-linux-gnueabihf-g++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Installation directory: /home/pi/openvino_files/ngraph/install
-- Configuring done
-- Generating done
-- Build files have been written to: /home/pi/openvino_files/ngraph/build/tbb
Scanning dependencies of target ext_tbb
[ 12%] Creating directories for 'ext_tbb'
[ 25%] Performing download step (git clone) for 'ext_tbb'
Cloning into 'tbb-src'...
Switched to a new branch 'tbb_2018'
Branch 'tbb_2018' set up to track remote branch 'tbb_2018' from 'origin'.
[ 37%] No patch step for 'ext_tbb'
[ 50%] No update step for 'ext_tbb'
[ 62%] No configure step for 'ext_tbb'
[ 75%] No build step for 'ext_tbb'
[ 87%] No install step for 'ext_tbb'
[100%] Completed 'ext_tbb'
[100%] Built target ext_tbb
CMake Error at src/ngraph/runtime/interpreter/CMakeLists.txt:27 (set_target_properties):
  set_target_properties called with incorrect number of arguments.


-- Found OpenMP_C: -fopenmp (found version &quot;4.5&quot;)
-- Found OpenMP_CXX: -fopenmp (found version &quot;4.5&quot;)
-- Found OpenMP: TRUE (found version &quot;4.5&quot;)
-- Building Intel TBB: /usr/bin/make -j4 compiler=gcc tbb_build_dir=/home/pi/openvino_files/ngraph/build/src/ngraph/runtime/cpu/tbb_build tbb_build_prefix=tbb
CMake Warning (dev) at src/ngraph/runtime/cpu/CMakeLists.txt:125 (find_package):
  Policy CMP0074 is not set: find_package uses &lt;PackageName&gt;_ROOT variables.
  Run &quot;cmake --help-policy CMP0074&quot; for policy details.  Use the cmake_policy
  command to set the policy and suppress this warning.

  CMake variable TBB_ROOT is set to:

    /home/pi/openvino_files/ngraph/build/tbb/tbb-src

  For compatibility, CMake is ignoring the variable.
This warning is for project developers.  Use -Wno-dev to suppress it.

-- Found TBB and imported target TBB::tbb
-- tools enabled
-- Adding unit test for backend INTERPRETER
-- Adding unit test for backend CPU
-- unit tests enabled
-- Configuring incomplete, errors occurred!
See also &quot;/home/pi/openvino_files/ngraph/build/CMakeFiles/CMakeOutput.log&quot;.
</code></pre>
","<p>I suggest you use a validated OpenVINO toolkit release with all the components (including ngraph). You can download the latest Raspberry Pi OpenVINO version from <a href=""https://download.01.org/opencv/2020/openvinotoolkit/2020.4/"" rel=""nofollow noreferrer"">https://download.01.org/opencv/2020/openvinotoolkit/2020.4/</a></p>
<p>The OpenVINO Raspberry Pi installation guide is available at
<a href=""https://docs.openvinotoolkit.org/2020.4/openvino_docs_install_guides_installing_openvino_raspbian.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/2020.4/openvino_docs_install_guides_installing_openvino_raspbian.html</a></p>
","336","1","1","<linux><tensorflow><cmake><openvino>"
"64444307","Error (expected: 'inputShapeLimitation.size() == blobShape.size()') when using opencv dnn readNetFromModelOptimizer","2020-10-20 11:35:03","<p>I'm attempting to use transfer learning to train a model for object detection to use with the Intel Neural Compute Stick 2 (NCS2)</p>
<p>Steps so far.</p>
<ol>
<li>Using transfer learning train faster_rcnn_inception_v2_coco_2018_01_28 model on my custom dataset using tensorflow 1.15 on google COLAB.</li>
<li>Verified saved tensorflow model works with opencv-python for object detection with tensorflow.saved_model.load</li>
<li>freeze model and use the openvino model optimizer command shown below to create the IR .bin and .xml to use with opencv-python dnn function.</li>
</ol>
<pre><code>python mo_tf.py --input_model frozen_inference_graph.pb --tensorflow_object_detection_api_pipeline_config pipeline.config  --transformations_config extensions/front/tf/faster_rcnn_support_api_v1.15.json --reverse_input_channels --data_type FP16 --input_shape [1,600,600,3] --input image_tensor --output=detection_scores,detection_boxes,num_detections
</code></pre>
<p>output as follows</p>
<pre><code>Model Optimizer arguments:
Common parameters:
- Path to the Input Model:  frozen_inference_graph.pb
- Path for generated IR:    /.
- IR output name:   frozen_inference_graph
- Log level:    ERROR
- Batch:    Not specified, inherited from the model
- Input layers:     image_tensor
- Output layers:    detection_scores,detection_boxes,num_detections
- Input shapes:     [1,600,600,3]
- Mean values:  Not specified
- Scale values:     Not specified
- Scale factor:     Not specified
- Precision of IR:  FP16
- Enable fusing:    True
- Enable grouped convolutions fusing:   True
- Move mean values to preprocess section:   False
- Reverse input channels:   True

TensorFlow specific parameters:
- Input model in text protobuf format:  False
- Path to model dump for TensorBoard:   None
- List of shared libraries with TensorFlow custom layers implementation:    None
- Update the configuration file with input/output node names:   None
- Use configuration file used to generate the model with Object Detection API:  pipeline.config
- Use the config file:  None

Model Optimizer version:    
[ WARNING ] Model Optimizer removes pre-processing block of the model which resizes image
keeping aspect ratio. The Inference Engine does not support dynamic image size so the
Intermediate Representation file is generated with the input image size of a fixed size.
The Preprocessor block has been removed. Only nodes performing mean value subtraction and
scaling (if applicable) are kept.
The graph output nodes &quot;num_detections&quot;, &quot;detection_boxes&quot;, &quot;detection_classes&quot;,
&quot;detection_scores&quot; have been replaced with a single layer of type &quot;Detection Output&quot;.
Refer to IR catalogue in the documentation for information about this layer.

[ WARNING ]  Network has 2 inputs overall, but only 1 of them are suitable for input
channels reversing.
Suitable for input channel reversing inputs are 4-dimensional with 3 channels
All inputs: {'image_tensor': [1, 3, 600, 600], 'image_info': [1, 3]}
Suitable inputs {'image_tensor': [1, 3, 600, 600]}

[ SUCCESS ] Generated IR version 10 model.
[ SUCCESS ] XML file: /./frozen_inference_graph.xml
[ SUCCESS ] BIN file: /./frozen_inference_graph.bin
[ SUCCESS ] Total execution time: 26.84 seconds. 
[ SUCCESS ] Memory consumed: 617 MB. 
</code></pre>
<ol start=""4"">
<li>Load the converted model with opencv-python dnn
Using the openvino ubuntu_dev docker image openvino/ubuntu18_dev:latest
I run a python script containing the following.</li>
</ol>
<pre><code>net = cv2.dnn.readNetFromModelOptimizer('frozen_inference_graph.xml',
        'frozen_inference_graph.bin') 
blob = cv2.dnn.blobFromImage(image_from_file)
net.setInput(blob)
</code></pre>
<p>The following error gets reported</p>
<pre><code>Traceback (most recent call last):
  File &quot;xxxxxxxxxxxxxx-dnn.py&quot;, line 49, in &lt;module&gt;
  
net.setInput(blob)
cv2.error: OpenCV(4.4.0-openvino) ../opencv/modules/dnn/src/dnn.cpp:4017: error:
    (-2:Unspecified error) in function 'void   cv::dnn::dnn4_v20200609::Net::setInput(cv::InputArray, const String&amp;, double, const Scalar&amp;)'
    (expected: 'inputShapeLimitation.size() == blobShape.size()'), where 'inputShapeLimitation.size()' is 2 must be equal to 'blobShape.size()' is 4
</code></pre>
<p>Can anyone shed some light on how to resolve this error please?</p>
","<p>I suggest that you try to load your model into Openvino's sample as in here: <a href=""https://docs.openvinotoolkit.org/2018_R5/_samples_object_detection_demo_README.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/2018_R5/_samples_object_detection_demo_README.html</a></p>
<p>It seems like there are incompatible sizes are used which related with the blob size. Your python script might not associated with dynamic shaping.</p>
<p>This might be useful for you: <a href=""https://www.youtube.com/watch?v=Ga8j0lgi-OQ"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=Ga8j0lgi-OQ</a></p>
","325","0","1","<python><tensorflow><opencv><object-detection><openvino>"
"58287316","How to convert segmentation model model to openvino int8 model?","2019-10-08 13:25:42","<p>I am trying convert my tensorflow segmentation model to openvino with quantization. I convert my .pb model to intermediate representation with openvino model optimizer. But how quantize model. In official documentation write that to do it with DL workbench. But in workbench i have only detection and classification dataset. <a href=""https://i.stack.imgur.com/o3xXh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o3xXh.png"" alt=""enter image description here""></a></p>

<p>Can i convert my model to int8 without dataset or can i create dataset to segmentation?</p>
","<p>The overall flow for converting a model from FP32 to INT8 is:</p>

<ul>
<li><p>Select an FP32 model</p></li>
<li><p>Select an appropriate dataset</p></li>
<li><p>Run a baseline inference</p></li>
<li><p>Configure INT8 calibration settings</p></li>
<li><p>Configure inference settings for a calibrated model</p></li>
<li><p>View INT8 calibration</p></li>
<li><p>View inference results</p></li>
<li><p>Compare the calibrated model with the original FP32 model</p></li>
</ul>

<p>Only some convolution models in the FP32 format can be quantized to INT8. If your model is incompatible, you will receive an error message.</p>

<p>The second stage of creating a configuration is adding a sample dataset. You can import a dataset, automatically generate a test dataset consisting of Gaussian distributed noise, or select a previously uploaded dataset.</p>

<p>You can find more details in the below link:
<a href=""http://docs.openvinotoolkit.org/latest/_docs_Workbench_DG_Select_Datasets.html"" rel=""nofollow noreferrer"">http://docs.openvinotoolkit.org/latest/_docs_Workbench_DG_Select_Datasets.html</a></p>
","325","0","2","<python><tensorflow><openvino>"
"58287316","How to convert segmentation model model to openvino int8 model?","2019-10-08 13:25:42","<p>I am trying convert my tensorflow segmentation model to openvino with quantization. I convert my .pb model to intermediate representation with openvino model optimizer. But how quantize model. In official documentation write that to do it with DL workbench. But in workbench i have only detection and classification dataset. <a href=""https://i.stack.imgur.com/o3xXh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o3xXh.png"" alt=""enter image description here""></a></p>

<p>Can i convert my model to int8 without dataset or can i create dataset to segmentation?</p>
","<p>You can find additional information about low precision inference in OpenVINO here:</p>
<ol>
<li>General approach: <a href=""https://docs.openvino.ai/latest/openvino_docs_IE_DG_Int8Inference.html"" rel=""nofollow noreferrer"">https://docs.openvino.ai/latest/openvino_docs_IE_DG_Int8Inference.html</a></li>
<li>Post-Training Optimisation Tool (POT) with default algorithm: <a href=""https://docs.openvino.ai/latest/pot_docs_LowPrecisionOptimizationGuide.html#doxid-pot-docs-low-precision-optimization-guide"" rel=""nofollow noreferrer"">https://docs.openvino.ai/latest/pot_docs_LowPrecisionOptimizationGuide.html#doxid-pot-docs-low-precision-optimization-guide</a></li>
</ol>
<p>Let me know if you still have questions.</p>
","325","0","2","<python><tensorflow><openvino>"
"59101507","How to detect openvino devices in cpp code","2019-11-29 08:49:49","<p>I am using OpenVINO 2019 and I need to detect the CPU and VPU for my project. Previously in 2018 version I had used some API, but now they are missing in new version.</p>

<p>So what is the proper way to detect openvino devices in a cpp code. </p>
","<pre><code>ExecutableNetworkInternal::Ptr clDNNEngine::LoadExeNetworkImpl(InferenceEngine::ICNNNetwork &amp;network,
                                                               const std::map&lt;std::string, std::string&gt; &amp;config) {
    auto specifiedDevice = network.getTargetDevice();
    auto supportedDevice = InferenceEngine::TargetDevice::eGPU;
    if (specifiedDevice != InferenceEngine::TargetDevice::eDefault &amp;&amp; specifiedDevice != supportedDevice) {
        THROW_IE_EXCEPTION &lt;&lt; ""The plugin doesn't support target device: "" &lt;&lt; getDeviceName(specifiedDevice) &lt;&lt; "".\n"" &lt;&lt;
                           ""Supported target device: "" &lt;&lt; getDeviceName(supportedDevice);
    }
</code></pre>
","323","0","2","<c++><openvino>"
"57496462","cannot get the same output as the pytorch model with openvino","2019-08-14 14:13:56","<p>I have a strange problem in trying to use OpenVino.</p>

<p>I have exported my pytorch model to onnx and then imported it to OpenVino using the following command:</p>

<pre><code>python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model ~/Downloads/unet2d.onnx --disable_resnet_optimization --disable_fusing --disable_gfusing --data_type=FP32
</code></pre>

<p>So for the test case, I have disabled the optimizations.</p>

<p>Now, using the sample python applications, I run inference using the model as follows:</p>

<pre><code>from openvino.inference_engine import IENetwork, IECore
import numpy as np

model_xml = path.expanduser('model.xml')
model_bin = path.expanduser('model.bin')
ie = IECore()
net = IENetwork(model=model_xml, weights=model_bin)
input_blob = next(iter(net.inputs))
out_blob = next(iter(net.outputs))
net.batch_size = 1

exec_net = ie.load_network(network=net, device_name='CPU')
np.random.seed(0)
x = np.random.randn(1, 2, 256, 256) # expected input shape
res = exec_net.infer(inputs={input_blob: x})
res = res[out_blob]
</code></pre>

<p>The problem is that this seems to output something completely different from my onnx or the pytorch model.</p>

<p>Additionally, I realized that I do not even have to pass an input, so if I do something like:</p>

<pre><code>x = None
res = exec_net.infer(inputs={input_blob: x})
</code></pre>

<p><strong>This still returns me the same output!</strong> So it seems to suggest that somehow my input is getting ignored or something like that?</p>
","<p>Could you try without --disable_resnet_optimization --disable_fusing --disable_gfusing 
with leaving the optimizations in.</p>
","323","3","1","<openvino>"
"59101507","How to detect openvino devices in cpp code","2019-11-29 08:49:49","<p>I am using OpenVINO 2019 and I need to detect the CPU and VPU for my project. Previously in 2018 version I had used some API, but now they are missing in new version.</p>

<p>So what is the proper way to detect openvino devices in a cpp code. </p>
","<p>There is a hello query device sample in the below path.</p>

<p><strong>C:\Program Files (x86)\IntelSWTools\openvino_\inference_engine\samples\hello_query_device</strong></p>

<p>It queries Inference Engine devices and prints their metrics and default configuration values. The sample shows how to use Query Device API feature.</p>

<blockquote>
  <p><strong>NOTE:</strong> This topic describes usage of C++ implementation of the Query Device Sample. 
  For the Python* implementation, refer to Hello Query Device Python* Sample</p>
</blockquote>

<h2>Running</h2>

<p>To see quired information, run the following:</p>

<pre class=""lang-sh prettyprint-override""><code>./hello_query_device
</code></pre>

<h2>Sample Output</h2>

<p>The application prints all available devices with their supported metrics and default values for configuration parameters:</p>

<pre><code>Available devices: 
    Device: CPU
    Metrics: 
        AVAILABLE_DEVICES : [ 0 ]
        SUPPORTED_METRICS : [ AVAILABLE_DEVICES SUPPORTED_METRICS FULL_DEVICE_NAME OPTIMIZATION_CAPABILITIES SUPPORTED_CONFIG_KEYS RANGE_FOR_ASYNC_INFER_REQUESTS RANGE_FOR_STREAMS ]
        FULL_DEVICE_NAME : Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz
        OPTIMIZATION_CAPABILITIES : [ WINOGRAD FP32 INT8 BIN ]
        SUPPORTED_CONFIG_KEYS : [ CPU_BIND_THREAD CPU_THREADS_NUM CPU_THROUGHPUT_STREAMS DUMP_EXEC_GRAPH_AS_DOT DYN_BATCH_ENABLED DYN_BATCH_LIMIT EXCLUSIVE_ASYNC_REQUESTS PERF_COUNT ]
        ...
    Default values for device configuration keys: 
        CPU_BIND_THREAD : YES
        CPU_THREADS_NUM : 0
        CPU_THROUGHPUT_STREAMS : 1
        DUMP_EXEC_GRAPH_AS_DOT : """"
        DYN_BATCH_ENABLED : NO
        DYN_BATCH_LIMIT : 0
        EXCLUSIVE_ASYNC_REQUESTS : NO
        PERF_COUNT : NO

    Device: FPGA
    Metrics: 
        AVAILABLE_DEVICES : [ 0 ]
        SUPPORTED_METRICS : [ AVAILABLE_DEVICES SUPPORTED_METRICS SUPPORTED_CONFIG_KEYS FULL_DEVICE_NAME OPTIMIZATION_CAPABILITIES RANGE_FOR_ASYNC_INFER_REQUESTS ]
        SUPPORTED_CONFIG_KEYS : [ DEVICE_ID PERF_COUNT EXCLUSIVE_ASYNC_REQUESTS DLIA_IO_TRANSFORMATIONS_NATIVE DLIA_ARCH_ROOT_DIR DLIA_PERF_ESTIMATION ]
        FULL_DEVICE_NAME : a10gx_2ddr : Intel Vision Accelerator Design with Intel Arria 10 FPGA (acla10_1150_sg10)
        OPTIMIZATION_CAPABILITIES : [ FP16 ]
        RANGE_FOR_ASYNC_INFER_REQUESTS : { 2, 5, 1 }
    Default values for device configuration keys: 
        DEVICE_ID : [ 0 ]
        PERF_COUNT : true
        EXCLUSIVE_ASYNC_REQUESTS : false
        DLIA_IO_TRANSFORMATIONS_NATIVE : false
        DLIA_PERF_ESTIMATION : true

</code></pre>
","323","0","2","<c++><openvino>"
"65698234","OpenVINO(NCS2): How to build OpenCV with Inference Engine","2021-01-13 08:34:41","<p>I have Intel NCS2 and i want run my program on it, but i have some problem</p>
<p>Code:</p>
<pre><code>import json
import cv2


def decode_out(out):
    detections = []

    for data in out[0, 0, :, :]:
        if float(data[2]) &gt; 0.3:
            detections.append({
                &quot;bbox&quot;: [float(x) for x in data[3:]],
                &quot;score&quot;: float(data[2]),
                &quot;class_id&quot;: int(data[1])
            })

    return sorted(detections, key=lambda x: x['score'], reverse=True)


image = cv2.imread(r&quot;C:\Users\06442\PycharmProjects\OpenVino\33.jpg&quot;)
image = cv2.resize(image, (300, 300))
input_blob = cv2.dnn.blobFromImage(image, 1.0 / 127.5, (300, 300), 127.5)

model = r&quot;C:\Users\06442\PycharmProjects\OpenVino\MobileNetSSD_deploy.caffemodel&quot;
prototxt = r&quot;C:\Users\06442\PycharmProjects\OpenVino\MobileNetSSD_deploy.prototxt&quot;

net = cv2.dnn.readNetFromCaffe(prototxt, model)

# with CPU
net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
net.setInput(input_blob)
out1 = net.forward()
print(json.dumps(decode_out(out1), indent=2))

# with NCS2
net.setPreferableTarget(cv2.dnn.DNN_TARGET_MYRIAD)
net.setPreferableBackend(cv2.dnn.DNN_BACKEND_INFERENCE_ENGINE)
net.setInput(input_blob)
out2 = net.forward()
print(json.dumps(decode_out(out2), indent=2))

</code></pre>
<p>Error in &quot;out2 = net.forward()&quot;:</p>
<pre><code>Unknown backend identifier in function
</code></pre>
<p>On CPU all work but on NCS2 not.
In my another code i have error:</p>
<pre><code>Build OpenCV with Inference Engine to enable loading models from Model Optimizer
</code></pre>
<p>Maybe it's help</p>
","<p>As the error suggests this is probably due to the OpenCV version used in your environment, which may not have Intel's Deep Learning Inference Engine (DL IE).</p>
<blockquote>
<p>Build OpenCV with Inference Engine to enable loading models from Model Optimizer</p>
</blockquote>
<p>Assuming you are on Windows (based this assumption on paths used in your program), you can choose one of the following options:</p>
<ul>
<li>Download and Install <a href=""https://software.seek.intel.com/openvino-toolkit"" rel=""nofollow noreferrer"">Intel® OpenVINO™ toolkit</a> - includes ready to use build of OpenCV</li>
<li><a href=""https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend#opencvdldt-windows-package-community-version"" rel=""nofollow noreferrer"">OpenCV+DLDT Windows package</a> (community version)</li>
<li>Build OpenCV from source for <a href=""https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend#microsoft-windows"" rel=""nofollow noreferrer"">Microsoft Windows</a></li>
</ul>
<p>For this and additional information check <a href=""https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend"" rel=""nofollow noreferrer"">Intel's Deep Learning Inference Engine backend</a></p>
","316","0","1","<python><opencv><intel><openvino>"
"63299117","DLDT (OpenVINO) install on Ubuntu 18.04 on Raspberry Pi 4 - Cython code error","2020-08-07 09:38:03","<p>I am trying to install the DLDT package on Ubuntu 18.04 running on the Raspberry Pi 4. The 2019 branch of DLDT seems to install correctly with some issues but can be rectified. However, the later version (i.e. 2020.3) is giving me the below error:</p>
<pre><code>Error compiling Cython file:
------------------------------------------------------------
...
    #  Usage example:\n
    #  ```python
    #  ie = IECore()
    #  net = ie.read_network(model=path_to_xml_file, weights=path_to_bin_file)
    #  ```
    cpdef IENetwork read_network(self, model: [str, bytes], weights: [str, bytes] = &quot;&quot;, init_from_buffer: bool = &quot;False&quot;):
         ^
------------------------------------------------------------

/home/ubuntu/dldt/inference-engine/ie_bridges/python/src/openvino/inference_engine/ie_api.pyx:136:10: Signature not compatible with previous declaration

Error compiling Cython file:
------------------------------------------------------------
...
cdef class LayersStatsMap(dict):
    cdef C.IENetwork net_impl

cdef class IECore:
    cdef C.IECore impl
    cpdef IENetwork read_network(self, model : [str, bytes], weights : [str, bytes] = ?, bool init_from_buffer = ?)
                               ^
------------------------------------------------------------

</code></pre>
<p>The CMAKE command I use is:</p>
<pre><code>sudo cmake -DCMAKE_BUILD_TYPE=Release -DENABLE_MKL_DNN=OFF -DENABLE_CLDNN=OFF -DENABLE_GNA=OFF -DENABLE_SSE42=OFF -DTHREADING=SEQ -DENABLE_OPENCV=OFF -DENABLE_PYTHON=ON -DPYTHON_EXECUTABLE=/usr/bin/python3.6 -DPYTHON_LIBRARY=/usr/lib/aarch64-linux-gnu/libpython3.6m.so -DPYTHON_INCLUDE_DIR=/usr/include/python3.6 ..
</code></pre>
<p>It seems like there is an issue with the declaration of the function or the usage of it. Is there any advice on this from anybody?</p>
<p>Is this a compatibility issue? Is this related to some Cython version issues? The one I have is: 0.29.21</p>
<p>Would appreciate some help on this. Thanks in advance!</p>
","<p>Another option to install and build openvino on Raspberry pi 4,</p>
<p>Download raspian installer 2019 version from <a href=""https://download.01.org/opencv/2019/openvinotoolkit/R3/"" rel=""nofollow noreferrer"">https://download.01.org/opencv/2019/openvinotoolkit/R3/</a></p>
<p>Follow the step given in this link - <a href=""https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_raspbian.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_raspbian.html</a></p>
","313","0","2","<cython><ubuntu-18.04><raspberry-pi4><openvino>"
"63299117","DLDT (OpenVINO) install on Ubuntu 18.04 on Raspberry Pi 4 - Cython code error","2020-08-07 09:38:03","<p>I am trying to install the DLDT package on Ubuntu 18.04 running on the Raspberry Pi 4. The 2019 branch of DLDT seems to install correctly with some issues but can be rectified. However, the later version (i.e. 2020.3) is giving me the below error:</p>
<pre><code>Error compiling Cython file:
------------------------------------------------------------
...
    #  Usage example:\n
    #  ```python
    #  ie = IECore()
    #  net = ie.read_network(model=path_to_xml_file, weights=path_to_bin_file)
    #  ```
    cpdef IENetwork read_network(self, model: [str, bytes], weights: [str, bytes] = &quot;&quot;, init_from_buffer: bool = &quot;False&quot;):
         ^
------------------------------------------------------------

/home/ubuntu/dldt/inference-engine/ie_bridges/python/src/openvino/inference_engine/ie_api.pyx:136:10: Signature not compatible with previous declaration

Error compiling Cython file:
------------------------------------------------------------
...
cdef class LayersStatsMap(dict):
    cdef C.IENetwork net_impl

cdef class IECore:
    cdef C.IECore impl
    cpdef IENetwork read_network(self, model : [str, bytes], weights : [str, bytes] = ?, bool init_from_buffer = ?)
                               ^
------------------------------------------------------------

</code></pre>
<p>The CMAKE command I use is:</p>
<pre><code>sudo cmake -DCMAKE_BUILD_TYPE=Release -DENABLE_MKL_DNN=OFF -DENABLE_CLDNN=OFF -DENABLE_GNA=OFF -DENABLE_SSE42=OFF -DTHREADING=SEQ -DENABLE_OPENCV=OFF -DENABLE_PYTHON=ON -DPYTHON_EXECUTABLE=/usr/bin/python3.6 -DPYTHON_LIBRARY=/usr/lib/aarch64-linux-gnu/libpython3.6m.so -DPYTHON_INCLUDE_DIR=/usr/include/python3.6 ..
</code></pre>
<p>It seems like there is an issue with the declaration of the function or the usage of it. Is there any advice on this from anybody?</p>
<p>Is this a compatibility issue? Is this related to some Cython version issues? The one I have is: 0.29.21</p>
<p>Would appreciate some help on this. Thanks in advance!</p>
","<p>Well it turns out that I had two versions of cython on my RPi (i.e. 0.26 and 0.29) and the cmake was using the older version. Once I updated the cmake to use the 0.29 version everything was fine.</p>
<p>I also downloaded the latest version of DLDT (v 2020.4) and used the same cmake command as earlier. This version of DLDT checks for the minimum required Cython version which is 0.29 and this led me to the answer.</p>
","313","0","2","<cython><ubuntu-18.04><raspberry-pi4><openvino>"
"63071884","Exe created for OpenVino using PyInstaller is not working","2020-07-24 10:37:50","<p>I have a running face recognition demo using openvino, now I want to create an exe for the same. The exe generated after running the command pyinstaller --onefile &quot;filename.py&quot; is not opening. I could find solve few errors after drag and drop to the cmd. Now I am getting a new error which I am not able to solve.</p>
<pre><code>Traceback (most recent call last):
  File &quot;face_recognition_demo.py&quot;, line 31, in &lt;module&gt;
    from faces_database import FacesDatabase
  File &quot;c:\users\gainthehouse\appdata\local\programs\python\python37\lib\site- 
packages\PyInstaller\loader\pyimod03_importers.py&quot;, line 627, in exec_module
    exec(bytecode, module.__dict__)
  File &quot;faces_database.py&quot;, line 23, in &lt;module&gt;
    from scipy.optimize import linear_sum_assignment
  File &quot;c:\users\gainthehouse\appdata\local\programs\python\python37\lib\site- 
packages\PyInstaller\loader\pyimod03_importers.py&quot;, line 627, in exec_module
    exec(bytecode, module.__dict__)
  File &quot;site-packages\scipy\optimize\__init__.py&quot;, line 389, in &lt;module&gt;
  File &quot;c:\users\gainthehouse\appdata\local\programs\python\python37\lib\site- 
packages\PyInstaller\loader\pyimod03_importers.py&quot;, line 627, in exec_module
    exec(bytecode, module.__dict__)
  File &quot;site-packages\scipy\optimize\optimize.py&quot;, line 37, in &lt;module&gt;
  File &quot;c:\users\gainthehouse\appdata\local\programs\python\python37\lib\site- 
packages\PyInstaller\loader\pyimod03_importers.py&quot;, line 627, in exec_module
    exec(bytecode, module.__dict__)
  File &quot;site-packages\scipy\optimize\linesearch.py&quot;, line 18, in &lt;module&gt;
ImportError: DLL load failed: The specified module could not be found.
[19168] Failed to execute script face_recognition_demo
</code></pre>
<p>Anybody who had come across this kind of error please help me to get out of this.</p>
","<p>I would suggest you try any of the following potential solutions:</p>
<ul>
<li>Reinstall missing packages/folders using pip install.</li>
<li>If you’re using Anaconda, try reinstalling Anaconda.</li>
<li>Install Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019.</li>
</ul>
<p>Additionally, I would suggest you add the --add-data option to your command line, to include the underlying dependencies for the smooth running of the .exe file.</p>
<p>pyinstaller --onefile --add-data &quot;&lt;install_directory&gt;\deployment_tools\inference_engine\bin\intel64\Release\plugins.xml;.&quot; filename.py</p>
<p>Do let us know if any of these works for you.</p>
","309","1","1","<python><exe><pyinstaller><openvino>"
"55451601","Im trying to follow an Openvino tutorial online, and am stuck at cmakelist not found","2019-04-01 09:11:41","<p>So basically I am trying to follow an openvino tutorial on youtube.
they are using ubuntu while im on windows 10, so i assume some of the codes typed in the terminal is just slightly change. it reached a point where it tells me to:</p>

<pre><code>mkdir build

cd build

cmake ..
</code></pre>

<p>so I did that in cmd</p>

<pre><code>C:\&gt;mkdir build
A subdirectory or file build already exists.


C:\&gt;cd build


C:\build&gt;cmake ..

CMake Error: The source directory ""C:/"" does not appear to contain `CMakeLists.txt.
Specify --help for usage, or press the help button on the CMake GUI.`
</code></pre>

<p>May I know what is going on and how to solve this?
im following this tutorial</p>

<p><a href=""https://www.youtube.com/watch?v=6Ww_zLDGfII&amp;list=PLDKCjIU5YH6jMzcTV5_cxX9aPHsborbXQ&amp;index=5"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=6Ww_zLDGfII&amp;list=PLDKCjIU5YH6jMzcTV5_cxX9aPHsborbXQ&amp;index=5</a></p>

<p>terribly sorry for any formatting issue. im new here</p>
","<p>If you want to run cmake under the same folder with CMakeLists.txt, please use ' <strong>.</strong> ' instead of  ' <strong>..</strong> '</p>

<p>In your case you used ' <strong>..</strong> ' after cmake, it will jump up and up (just like <strong>cd ..</strong>) in the directory.</p>
","302","0","1","<openvino>"
"62678255","How to install tensorflow or tensorflow-lite on raspberry Pi4?","2020-07-01 13:43:09","<p>I have a raspberry Pi4 containning ubuntu 18.04. My ubuntu has been installed from a dockerfile and push thanks to balena OS in my raspberry.</p>
<p>Then I tried to install tensorflow for openvino project on it by following this tuto in an environment with python3.7.5:
<a href=""https://qengineering.eu/install-tensorflow-2.1.0-on-raspberry-pi-4.html"" rel=""nofollow noreferrer"">https://qengineering.eu/install-tensorflow-2.1.0-on-raspberry-pi-4.html</a></p>
<p>But I get the following error: ERROR: tensorflow-2.1.0-cp37-cp37m-linux_armv7l.whl is not a supported wheel on this platform.</p>
<p>Is it possible that balena OS stop the detection of the raspberryPi4 architecture and then the installation ?</p>
<p>If not do you have a good version of tensorflow for raspberrypi4 with ARMv8 architecture.</p>
<p>I don't understand where this problem come from so if you have any idea, thanks a lot.</p>
","<p>If I understand your question correctly, you want to install TensorFlow on Docker running Ubuntu 18.04 on RPI4 with balena OS as host.</p>
<p>You could try to build TensorFlow from source as described in their <a href=""https://www.tensorflow.org/install/source"" rel=""nofollow noreferrer"">Build from source instructions</a>. Based on latest OpenVINO™ toolkit release (v2020.3), the TensorFlow version that gets installed is v1.14, just keep that in mind when you decide which TensorFlow version to build from source. Also the build process may take a while.</p>
","298","0","1","<tensorflow><tensorflow-lite><raspberry-pi4><openvino><balena>"
"60606297","Cannot convert tf.keras.layers.ConvLSTM2D layer to open vino intermediate representation","2020-03-09 18:22:12","<p>I am trying to convert a trained model in tensorflow to <a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html#tensorflow_specific_conversion_params"" rel=""nofollow noreferrer"">Open VINO</a> Intermediate Representation.</p>

<p>I have a model of the form given below</p>

<pre><code>class Conv3DModel(tf.keras.Model):
    def __init__(self):
        super(Conv3DModel, self).__init__()
        # Convolutions
        self.conv1 = tf.compat.v2.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', name=""conv1"", data_format='channels_last')
        self.pool1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')
        self.conv2 = tf.compat.v2.keras.layers.Conv3D(64, (3, 3, 3), activation='relu', name=""conv1"", data_format='channels_last')
        self.pool2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2,2), data_format='channels_last')

        # LSTM &amp; Flatten
        self.convLSTM =tf.keras.layers.ConvLSTM2D(40, (3, 3))
        self.flatten =  tf.keras.layers.Flatten(name=""flatten"")

        # Dense layers
        self.d1 = tf.keras.layers.Dense(128, activation='relu', name=""d1"")
        self.out = tf.keras.layers.Dense(6, activation='softmax', name=""output"")


    def call(self, x):
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.pool2(x)
        x = self.convLSTM(x)
        x = self.flatten(x)
        x = self.d1(x)
        return self.out(x)
</code></pre>

<p>I tried to convert the model into IR. The model is <a href=""https://drive.google.com/file/d/1OdzRQRYVB6KOZYeTxmu-SkwkqMrOCUK7/view?usp=sharing"" rel=""nofollow noreferrer"">here</a> .</p>

<p>I have trained this model in tensorflow 1.15. Tensorflow 2.0 is currently not supported.</p>

<p>Now I tried to run the command</p>

<p>python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --saved_model_dir jester_trained_models/3dcnn-basic/  --output_dir /home/deepanshu/open_vino/udacity_project_custom_model/</p>

<p>Now i got the following error</p>

<p><strong>Model Optimizer arguments:</strong></p>

<p>Common parameters: </p>

<ul>
<li><p>Path to the Input Model: None</p></li>
<li><p>Path for generated IR: /home/deepanshu/open_vino/udacity_project_custom_model/</p></li>
<li><p>IR output name: saved_model</p></li>
<li><p>Log level: ERROR</p></li>
<li><p>Batch: Not specified, inherited from the model</p></li>
<li><p>Input layers: Not specified, inherited from the model</p></li>
<li><p>Output layers: Not specified, inherited from the model</p></li>
<li><p>Input shapes: Not specified, inherited from the model</p></li>
<li><p>Mean values: Not specified</p></li>
<li><p>Scale values: Not specified</p></li>
<li><p>Scale factor: Not specified</p></li>
<li><p>Precision of IR: FP32</p></li>
<li><p>Enable fusing: True</p></li>
<li><p>Enable grouped convolutions fusing: True</p></li>
<li><p>Move mean values to preprocess section: False</p></li>
<li><p>Reverse input channels: False</p></li>
</ul>

<p>TensorFlow specific parameters:</p>

<ul>
<li><p>Input model in text protobuf format: False</p></li>
<li><p>Path to model dump for TensorBoard: None</p></li>
<li><p>List of shared libraries with TensorFlow custom layers implementation: None</p></li>
<li><p>Update the configuration file with input/output node names: None</p></li>
<li><p>Use configuration file used to generate the model with Object Detection API: None</p></li>
<li><p>Operations to offload: None</p></li>
<li><p>Patterns to offload: None</p></li>
<li><p>Use the config file: None</p></li>
</ul>

<p>Model Optimizer version: 2020.1.0-61-gd349c3ba4a</p>

<p><strong>[ ERROR ] Unexpected exception happened during extracting attributes for node conv3d_model/conv_lst_m2d/bias/Read/ReadVariableOp. Original exception message: 'ascii' codec can't decode byte 0xc9 in position 1: ordinal not in range(128)</strong></p>

<p>As far as  I can see it is the tf.keras.layers.ConvLSTM2D(40, (3, 3)) causing problems . I am kind of stuck here . Can anyone tell me where can I proceed further ?</p>

<p>Thanks</p>

<h1>Edit to the question</h1>

<p>Now I rejected the above tensorflow implementation and used keras . My h5 model developed was converted into .pb format using this <a href=""https://software.intel.com/en-us/forums/intel-distribution-of-openvino-toolkit/topic/820599"" rel=""nofollow noreferrer"">post</a>.</p>

<p>Now I ran the model optimizer on this .pb file. Using the command </p>

<p><code>python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_model /home/deepanshu/ml_playground/jester_freezed/tf_model.pb  --output_dir /home/deepanshu/open_vino/udacity_project_custom_model/  --input_shape=[1,30,64,64,1] --data_type FP32
</code></p>

<p>Now i am facing another issue . The issue here is point no. <strong>97</strong> on this <a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html"" rel=""nofollow noreferrer"">post</a>.</p>

<p>So my model contains a cycle and model optimizer does not know a way to convert it. Has anybody faced this issue before ?</p>

<p>Please help.</p>

<p>Here is the <a href=""https://drive.google.com/file/d/16RbBRBctz8BvxAQW3Qju2R6gErz-mu5i/view?usp=sharing"" rel=""nofollow noreferrer"">model</a> .</p>

<p>Here is the defination of the model in keras</p>

<pre class=""lang-py prettyprint-override""><code>
from keras.models import Sequential

from keras.layers import Conv3D , MaxPool3D,Flatten ,Dense

from keras.layers.convolutional_recurrent import ConvLSTM2D

import keras


model = Sequential()

model.add(Conv3D(32, (3, 3, 3), 

         name=""conv1"" , input_shape=(30, 64, 64,1) ,  data_format='channels_last',

        activation='relu') )

model.add(MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last'))

model.add(Conv3D(64, (3, 3, 3), activation='relu', name=""conv2"", data_format='channels_last'))

model.add(MaxPool3D(pool_size=(2, 2,2), data_format='channels_last'))

model.add(ConvLSTM2D(40, (3, 3)))

model.add(Flatten(name=""flatten""))

model.add(Dense(128, activation='relu', name=""d1""))

model.add(Dense(6, activation='softmax', name=""output""))
</code></pre>
","<p>Actually the script to convert from h5 to .pb suggested by intel was not good enough. Always use the code from <a href=""https://stackoverflow.com/questions/45466020/how-to-export-keras-h5-to-tensorflow-pb"">here</a> to convert your keras model to <strong>.pb</strong>. </p>

<p>Once you obtain your <strong>.pb</strong> file now convert your model to IR using</p>

<pre><code>python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_model ml_playground/try_directory/tf_model.pb   --output_dir /home/deepanshu/open_vino/udacity_project_custom_model/  --input_shape=[1,30,64,64,1] --data_type FP32
</code></pre>

<p>After the execution of this script we can obtain the intermediate representation of the keras model. </p>
","295","1","1","<tensorflow><keras><lstm><intel><openvino>"
"62275274","Unable to convert VGG-16 to IR","2020-06-09 04:54:31","<p>I have truncated version of vgg16 in .pb format. I am unable to convert to IR using OpenVino Model Optimizer getting following error:</p>

<p>[ ANALYSIS INFO ]  It looks like there is IteratorGetNext as input
Run the Model Optimizer with:
                --input ""IteratorGetNext:0[-1 224 224 3]""
And replace all negative values with positive values
[ ERROR ]  Exception occurred during running replacer ""REPLACEMENT_ID"" (): Graph contains 0 node after executing . It considered as error because resulting IR will be empty which is not usual</p>

<pre><code>python3 /opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/mo_tf.py --input_model model.pb

With *.meta

python3 /opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/mo_tf.py --input_meta_graph model.meta --log_level DEBUG


[ 2020-06-11 10:59:34,182 ] [ DEBUG ] [ main:213 ]  Placeholder shapes : None
'extensions.back.ScalarConstNormalize.RangeInputNormalize'&gt;
|  310 |  True   | &lt;class 'extensions.back.AvgPool.AvgPool'&gt;
|  311 |  True   | &lt;class 'extensions.back.ReverseInputChannels.ApplyReverseChannels'&gt;
|  312 |  True   | &lt;class 'extensions.back.split_normalizer.SplitNormalizer'&gt;
|  313 |  True   | &lt;class 'extensions.back.ParameterToPlaceholder.ParameterToInput'&gt;
|  314 |  True   | &lt;class 'extensions.back.GroupedConvWeightsNormalize.GroupedConvWeightsNormalize'&gt;
|  315 |  True   | &lt;class 'extensions.back.ConvolutionNormalizer.DeconvolutionNormalizer'&gt;
|  316 |  True   | &lt;class 'extensions.back.StridedSliceMasksNormalizer.StridedSliceMasksNormalizer'&gt;
|  317 |  True   | &lt;class 'extensions.back.ConvolutionNormalizer.ConvolutionWithGroupsResolver'&gt;
|  318 |  True   | &lt;class 'extensions.back.ReshapeMutation.ReshapeMutation'&gt;
|  319 |  True   | &lt;class 'extensions.back.ForceStrictPrecision.ForceStrictPrecision'&gt;
|  320 |  True   | &lt;class 'extensions.back.I64ToI32.I64ToI32'&gt;
|  321 |  True   | &lt;class 'extensions.back.ReshapeMutation.DisableReshapeMutationInTensorIterator'&gt;
|  322 |  True   | &lt;class 'extensions.back.ActivationsNormalizer.ActivationsNormalizer'&gt;
|  323 |  True   | &lt;class 'extensions.back.pass_separator.BackFinish'&gt;
|  324 |  False  | &lt;class 'extensions.back.SpecialNodesFinalization.RemoveConstOps'&gt;
|  325 |  False  | &lt;class 'extensions.back.SpecialNodesFinalization.CreateConstNodesReplacement'&gt;
|  326 |  True   | &lt;class 'extensions.back.kaldi_remove_memory_output.KaldiRemoveMemoryOutputBackReplacementPattern'&gt;
|  327 |  False  | &lt;class 'extensions.back.SpecialNodesFinalization.RemoveOutputOps'&gt;
|  328 |  True   | &lt;class 'extensions.back.blob_normalizer.BlobNormalizer'&gt;
|  329 |  False  | &lt;class 'extensions.middle.MulFakeQuantizeFuse.MulFakeQuantizeFuse'&gt;
|  330 |  False  | &lt;class 'extensions.middle.AddFakeQuantizeFuse.AddFakeQuantizeFuse'&gt;
[ 2020-06-11 10:59:34,900 ] [ DEBUG ] [ class_registration:282 ]  Run replacer &lt;class 'extensions.load.tf.loader.TFLoader'&gt;
[ INFO ]  Restoring parameters from %s
[ WARNING ]  From %s: %s (from %s) is deprecated and will be removed %s.
Instructions for updating:
%s
[ WARNING ]  From %s: %s (from %s) is deprecated and will be removed %s.
Instructions for updating:
%s
[ FRAMEWORK ERROR ]  Cannot load input model: Attempting to use uninitialized value metrics/accuracy/total
     [[{{node _retval_metrics/accuracy/total_0_54}}]]
[ 2020-06-11 10:59:35,760 ] [ DEBUG ] [ main:328 ]  Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn
    target_list, run_metadata)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value metrics/accuracy/total
     [[{{node _retval_metrics/accuracy/total_0_54}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/mo/front/tf/loader.py"", line 220, in load_tf_graph_def
    outputs)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py"", line 330, in convert_variables_to_constants
    returned_variables = sess.run(variable_names)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value metrics/accuracy/total
     [[{{node _retval_metrics/accuracy/total_0_54}}]]

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/mo/utils/class_registration.py"", line 288, in apply_transform
    for_graph_and_each_sub_graph_recursively(graph, replacer.find_and_replace_pattern)
  File ""/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/mo/middle/pattern_match.py"", line 58, in for_graph_and_each_sub_graph_recursively
    func(graph)
  File ""/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/extensions/load/loader.py"", line 27, in find_and_replace_pattern
    self.load(graph)
  File ""/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/extensions/load/tf/loader.py"", line 58, in load
    saved_model_tags=argv.saved_model_tags)
  File ""/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/mo/front/tf/loader.py"", line 231, in load_tf_graph_def
    raise FrameworkError('Cannot load input model: {}', e) from e
mo.utils.error.FrameworkError: Cannot load input model: Attempting to use uninitialized value metrics/accuracy/total
     [[{{node _retval_metrics/accuracy/total_0_54}}]]

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/mo/main.py"", line 312, in main
    ret_code = driver(argv)
  File ""/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/mo/main.py"", line 273, in driver
    ret_res = emit_ir(prepare_ir(argv), argv)
  File ""/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/mo/main.py"", line 238, in prepare_ir
    graph = unified_pipeline(argv)
  File ""/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/mo/pipeline/unified.py"", line 29, in unified_pipeline
    class_registration.ClassType.BACK_REPLACER
  File ""/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/mo/utils/class_registration.py"", line 334, in apply_replacements
    apply_replacements_list(graph, replacers_order)
  File ""/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/mo/utils/class_registration.py"", line 324, in apply_replacements_list
    num_transforms=len(replacers_order))
  File ""/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/mo/utils/logger.py"", line 124, in wrapper
    function(*args, **kwargs)
  File ""/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/mo/utils/class_registration.py"", line 306, in apply_transform
    raise FrameworkError('{}'.format(str(err))) from err
mo.utils.error.FrameworkError: Cannot load input model: Attempting to use uninitialized value metrics/accuracy/total
     [[{{node _retval_metrics/accuracy/total_0_54}}]]
</code></pre>
","<p>The problem is that models trained in TensorFlow have some shapes undefined. In your case, it looks like batch of the input is not defined. To fix it, please add an additional argument to the command line: <code>-b 1</code>. The option sets batch to 1. It should fix this particular issue.</p>

<p>After that, I guess, you may encounter other issues so I would leave the following link: <a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html"" rel=""nofollow noreferrer"">Converting a TensorFlow Model</a>.
There are some tips about how to convert TensorFlow model to IR.</p>
","293","0","1","<tensorflow><intel><openvino>"
"64164029","How to get OpenVino toolkit to work with Anaconda Environment on Windows 10","2020-10-01 22:34:56","<p>I am trying to get the Intel OpenVino toolkit to work on an Anaconda environment. I know Anaconda has a package to get the Openvino tool kit. However, I have already installed the pre-requisite installation for Windows 10. How do I go about setting up the Openvino toolkit to work with the already existing Anaconda Environment?
Any advice would be appreciated.</p>
<p>Thanks in advance!</p>
","<p>I recommend you to get the pre-requisite in your system first:</p>
<ol>
<li>Microsoft Visual Studio* with C++ 2019 or 2017 with MSBuild ( I recommend 2019)</li>
<li>CMake 2.8.12 or higher 64-bit (ensure that this added to your system's path)</li>
<li>Python 3.5 - 3.7 64-bit ( I recommend 3.6)</li>
<li>Intel® Distribution of OpenVINO™ toolkit core components (2020.4 is the latest)</li>
</ol>
<p>Since you already done that, you can proceed to next steps.</p>
<p>create a virtual env : conda create --name </p>
<p>conda activate </p>
<p>(then proceed the next steps of installation here, as in official Openvino documentation provided below)</p>
<p>conda cheat sheet: <a href=""https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-chea.."" rel=""nofollow noreferrer"">https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-chea..</a>.</p>
<p>This is Openvino official documentation that shows step by step installation process: <a href=""https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_windows.htm.."" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_windows.htm..</a>.</p>
","289","0","1","<python-3.x><windows><anaconda><intel><openvino>"
"58524693","How to run the cpp samples in openvino","2019-10-23 14:05:31","<p>I am using openvino_2019.2.242 and trying to run the cpp samples in the same. Please guide me with the complete steps to build the sample applications. As an example can you provide the command to run classification sample async . Thanks in advance.</p>
","<p>To build the sample applications , run the build_samples_msvc.bat batch file:</p>

<blockquote>
  <p>&lt; INSTALL_DIR >\inference_engine\samples\build_samples_msvc.bat</p>
</blockquote>

<p>Open terminal</p>

<pre><code> cd C:\Program Files (x86)\IntelSWTools\openvino_2019.2.242\inference_engine\samples
 build_samples_msvc.bat
</code></pre>

<p>The sample applications binaries are in the </p>

<pre><code> C:\Users\&lt;username&gt;\Documents\Intel\OpenVINO\inference_engine_samples_build\intel64\Release
</code></pre>

<p>directory.</p>

<hr>

<p><strong><em>The below 3 steps are optional</em></strong></p>

<pre><code> cd C:\Users\&lt;username&gt;\Documents\Intel\OpenVINO\inference_engine_samples_build

 select samples.sln

 Build -&gt; Build Solution
</code></pre>

<hr>

<p>This will create the executable files at the below location</p>

<pre><code> C:\Users\&lt;username&gt;\Documents\Intel\OpenVINO\inference_engine_samples_build\intel64\Release
</code></pre>

<p>To run </p>

<p>(for eg: classification_sample_async)</p>

<pre><code> cd C:\Users\&lt;username&gt;\Documents\Intel\OpenVINO\inference_engine_samples_build\intel64\Release

 classification_sample_async.exe -m C:\Users\&lt;username&gt;\Documents\Intel\OpenVINO\openvino_models\ir\FP32\classification\squeezenet\1.1\caffe\squeezenet1.1.xml -i ""C:\Program Files (x86)\IntelSWTools\openvino_2019.1.122\deployment_tools\demo\car.png"" -labels &lt;PATH_TO_LABEL_FILE&gt;\squeezenet1.1.labels -d CPU
</code></pre>
","284","1","1","<c++><intel><openvino>"
"55674762","OpenVINO serving - Servable not found for request","2019-04-14 11:29:03","<p>I am trying to serve a OpenVINO model using procedure mentioned in <a href=""https://github.com/IntelAI/OpenVINO-model-server/tree/master/example_client"" rel=""nofollow noreferrer"">OpenVINO model server repo</a> but encountering the below issue when trying to get the metadata for the model.<br>
Command I am executing is  </p>

<pre><code>python get_serving_meta.py --grpc_address 0.0.0.0 --grpc_port 9001 \  
--model_name my_model --model_version 1  
</code></pre>

<p>Error I am receiving is:</p>

<pre><code>    Getting model metadata for model: my_model  
    Traceback (most recent call last):
      File ""get_serving_meta.py"", line 97, in &lt;module&gt;  
    result = stub.GetModelMetadata(request, 10.0) # result includes a dictionary with all model outputs


    File ""/root/.pyenv/versions/3.6.8/lib/python3.6/site-packages/grpc/_channel.py"", line 549, in __call__  
    return _end_unary_response_blocking(state, call, False, None) 
File ""/root/.pyenv/versions/3.6.8/lib/python3.6/site-packages/grpc/_channel.py"", line 466, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)  
grpc._channel._Rendezvous: &lt;_Rendezvous of RPC that terminated with:  
        status = StatusCode.NOT_FOUND  
        details = ""Servable not found for request:Specific(my_model, 1)""  
        debug_error_string = ""{""created"":""@1555239621.319103888"",""description"":""Error received from peer"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Servable not found for request: Specific(my_model, 1)"",""grpc_status"":5}""  
</code></pre>

<p>To get the docker container up, cmd I am using is:  </p>

<pre><code>docker run --rm -d -v /home/rachit/models/:/opt/ml:ro -p 9001:9001  \
docker.io/intelaipg/openvino-model-server:latest \
/ie-serving-py/start_server.sh ie_serving model \
--model_path /opt/ml/model1 --model_name my_model --port 9001  
</code></pre>

<p>The directory tree structure for /home/rachit/models is  </p>

<pre><code>models/
|-- model1/
|---|---1/
|---|---|--- frozen_inference_graph.bin
|---|---|--- frozen_inference_graph.xml  
</code></pre>

<p>Docker Logs:  </p>

<blockquote>
  <p>2019-04-14 10:36:03,862 - ie_serving.main - INFO - Log level set: INFO<br>
  2019-04-14 10:36:03,863 - ie_serving.models.model - INFO - Server start loading model: my_model<br>
  2019-04-14 10:36:03,865 - ie_serving.models.model - INFO - List of available versions for my_model model: []<br>
  2019-04-14 10:36:03,865 - ie_serving.models.model - INFO - Default version for my_model model is -1<br>
  2019-04-14 10:36:03,876 - ie_serving.server.start - INFO - Server listens on port 9001 and will be serving models: ['my_model']</p>
</blockquote>

<p>I am a newbie so any help would be really helpful. Thanks</p>
","<p>Based on the problem description there is incorrect --grpc_address parameter.
It should be the IP address of the service exposed in the openvino model server docker container. If you are connecting from the same host, where the docker is started, just use localhost value:
<code>python get_serving_meta.py --grpc_address localhost --grpc_port 9001 --model_name my_model --model_version 1</code></p>

<p>Beside that you need to mind the client proxy settings. In your case <code>http_proxy</code> should be unset or <code>localhost</code> should be included in <code>no_proxy</code> variable.</p>
","283","0","1","<tensorflow-serving><grpc-python><openvino>"
"55852212","How to ensure static shapes in Tensorflow model for easy OpenVINO conversion?","2019-04-25 14:54:42","<p>I'm trying to optimize and convert a tensorflow model to OpenVINO IR. It hasn't been very successful because of the problems I'm facing with input shapes. So I'm planning to remodel the whole model with static shapes. The model I'm trying to work on is Tacotron by keithito.</p>

<p>How do I ensure all the nodes in my model will have static shapes?</p>

<p>Will just setting the input placeholder nodes to a fixed shape allow tensorflow to infer and fix the shapes of all other nodes?</p>
","<p>You can have dynamic shapes in TF model and provide static shape while cnverting model with ModelOptimizer. Example for input data of size 256x256 with 3 channels.</p>

<p>python mo_tf.py --input_shape [1,256,256,3] --input_model model.pb</p>
","274","0","1","<python><tensorflow><speech><openvino>"
"56427609","Convert posenet model with OpenVINO toolkit?","2019-06-03 12:41:21","<p>I am trying to convert posenet model (of MobileNetV1 architecture) using OpenVINO model optimizer. But it is throwing an error as below.<br>
I am using the following command to convert.  </p>

<pre><code>python3 mo_tf.py --input_model /vino/models/posenet/_models/model-mobilenet_v1_101.pb --input_meta_graph /vino/models/posenet/_models/checkpoints/model-mobilenet_v1_101.ckpt.meta  --output_dir /vino/models/posenet/
</code></pre>

<p>Error I am receiving is:</p>

<pre><code>Model Optimizer arguments:
Common parameters:
    - Path to the Input Model:  /vino/models/posenet/_models/model-mobilenet_v1_101.pb
    - Path for generated IR:    /vino/models/posenet/
    - IR output name:   model-mobilenet_v1_101
    - Log level:    ERROR
    - Batch:    Not specified, inherited from the model
    - Input layers:     Not specified, inherited from the model
    - Output layers:    Not specified, inherited from the model
    - Input shapes:     Not specified, inherited from the model
    - Mean values:  Not specified
    - Scale values:     Not specified
    - Scale factor:     Not specified
    - Precision of IR:  FP32
    - Enable fusing:    True
    - Enable grouped convolutions fusing:   True
    - Move mean values to preprocess section:   False
    - Reverse input channels:   False
TensorFlow specific parameters:
    - Input model in text protobuf format:  False
    - Path to model dump for TensorBoard:   None
    - List of shared libraries with TensorFlow custom layers implementation:    None
    - Update the configuration file with input/output node names:   None
    - Use configuration file used to generate the model with Object Detection API:  None
    - Operations to offload:    None
    - Patterns to offload:  None
    - Use the config file:  None
Model Optimizer version:    2019.1.0-341-gc9b66a2
[ ERROR ]  Unknown configuration of input model parameters
</code></pre>

<p>I am suspecting that, I am using the wrong command altogher but couldn't figure it out even after going through its documentation. Any leads on what I am doing wrong would be really helpful.</p>
","<p>Try to provide just input_meta_graph, without input_model. </p>

<p><a href=""https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html</a> </p>
","269","0","1","<openvino><mobilenet>"
"61063772","OpenVino model outputs zeroes","2020-04-06 15:53:48","<p>I have an acoustic model that successfully converted from ONNX to OpenVino. However, in OpenVino this model outputs tensor that consists of zeroes from some position.</p>

<pre class=""lang-cpp prettyprint-override""><code>#include &lt;iostream&gt;
#include &lt;fstream&gt;
#include &lt;iterator&gt;
#include &lt;inference_engine.hpp&gt;

typedef struct {
    float* data;
    size_t size;
    size_t timeLen;
} Fbank;

using namespace InferenceEngine;
using std::cout;
using std::endl;

void print_arr(std::string text, const float* arr, int l, int r) {
    cout &lt;&lt; text &lt;&lt; endl;
    for (int i = l; i &lt; r; i++) {
        cout &lt;&lt; arr[i] &lt;&lt; "" "";
    }
    cout &lt;&lt; endl;
}

void doInference(ExecutableNetwork&amp; executable_network, const std::string&amp; input_name, const std::string&amp; output_name, Fbank* fbank) {

    InferRequest infer_request = executable_network.CreateInferRequest();

    InferenceEngine::TensorDesc tDesc(InferenceEngine::Precision::FP32,
        {fbank-&gt;size, fbank-&gt;timeLen}, InferenceEngine::Layout::HW);
    Blob::Ptr blob = InferenceEngine::make_shared_blob&lt;float&gt;(tDesc, fbank-&gt;data);

    infer_request.SetBlob(input_name, blob);

    infer_request.Infer();
    Blob::Ptr output_blob = infer_request.GetBlob(output_name);

    auto dims = output_blob-&gt;getTensorDesc().getDims();

    size_t batchSize = dims[0];
    size_t T = dims[1];
    size_t D = dims[2];

    MemoryBlob::CPtr moutput = as&lt;MemoryBlob&gt;(output_blob);
    if (!moutput) {
        return;
    }

    auto moutputHolder = moutput-&gt;rmap();
    const float *pred = moutputHolder.as&lt;const float*&gt;();

    print_arr(""AM output:"", pred, D*29, D*31);
}


int main() {
    Fbank* fbank = new Fbank;
    fbank-&gt;size = 64;
    fbank-&gt;timeLen = 2000;
    fbank-&gt;data = new float[64*2000];

    Core ie;
    CNNNetwork network = ie.ReadNetwork(""quartznet_random.xml"", ""quartznet_random.bin"");

    std::string input_name = network.getInputsInfo().begin()-&gt;first;
    std::string output_name = network.getOutputsInfo().begin()-&gt;first;

    network.getOutputsInfo().begin()-&gt;second-&gt;setPrecision(Precision::FP32);

    ExecutableNetwork executable_network = ie.LoadNetwork(network, ""cpu"");
    doInference(executable_network, input_name, output_name, fbank);
    return 0;
}
</code></pre>

<p>Outputs:</p>

<pre><code>AM output:
0.138650 -5.833140 -8.023724 -7.637482 -8.001101 -9.033963 -8.029905 -8.132050 -9.186495 -8.537528 -8.788505 -9.240234 -8.547676 -8.673388 0.000000 0.000000 -0.000000 0.000000 -0.000000 0.000000 0.000000 -0.000000 -0.000000 0.000000 -0.000000 0.000000 0.000000 -0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 -0.000000 0.000000 0.000000 -0.000000 0.000000 0.000000 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000 0.000000 -0.000000 -0.000000 -0.000000 0.000000 -0.000000 -0.000000 -0.000000 0.000000 0.000000 -0.000000 -0.000000 0.000000 -0.000000 0.000000 -0.000000 -0.000000 -0.000000 0.000000 -0.000000 -0.000000 0.000000 -0.000000 0.000000
</code></pre>

<p>If I run <a href=""https://transfer.vkpartner.ru/b1f50972c59c"" rel=""nofollow noreferrer"">ONNX model</a> in Python using <code>onnxruntime</code>, the output will be correct. (<a href=""https://gist.github.com/yutkin/50f38d56a682ed5a51d3aa3ef09bd52e"" rel=""nofollow noreferrer"">Example</a>).</p>

<p>Is it possible to fix it?</p>

<p>P.S. Command to convert the model from ONNX: <code>python3 mo_onnx.py —input_model model.onnx —output=""output"" —input=""fbanks[64 2000]""</code></p>
","<p>Tested provided ONNX model in OpenVINO for Linux, couple of findings while testing OpenVINO 2020.1 and new 2020.2 version (released today 4/14/2020, <a href=""https://software.intel.com/en-us/articles/OpenVINO-RelNotes"" rel=""nofollow noreferrer"">release notes</a>).</p>

<p>Using same command to convert from ONNX. Although its unclear what would be the expected output (probability between 0.0 and 1.0?), OpenVINO 2020.2 seems to affect the output results. </p>

<ol>
<li>On 2020.1, observed similar results to yours (one can assume this is the OpenVINO version you used).</li>
</ol>

<blockquote>
  <p>AM output: -3.55062 -3.5114 -3.50925 -3.52013 -3.51791 -3.54656 -3.53908 -3.54239 -3.53626 -3.50982 -3.54193 -3.55593 -3.52877 -3.53786 -1.546e-07 -6.14673e-08 -8.56817e-08 -1.41561e-07 -6.14673e-08 -1.16415e-07 -9.30158e-08 -9.12696e-08 -1.29454e-07 -1.04774e-07 -6.14673e-08 -5.58794e-08 -1.71363e-07 -1.02445e-07 -5.7742e-08 -1.35042e-07 -9.26666e-08 -1.00583e-07 -1.04308e-07 -1.2666e-07 -1.39698e-07 -7.26432e-08 -9.68575e-08 -1.47149e-07 -9.40636e-08 -9.77889e-08 -9.49949e-08 -1.16415e-07 -9.54606e-08 -8.3819e-08 -1.28523e-07 -1.35973e-07 -7.66013e-08 -1.12224e-07 -1.546e-07 -6.14673e-08 -8.56817e-08 -1.41561e-07 -6.14673e-08 -1.16415e-07 -9.30158e-08 -9.12696e-08 -1.29454e-07 -1.04774e-07 -6.14673e-08 -5.58794e-08 -1.71363e-07 -1.02445e-07 -5.7742e-08 -1.35042e-07 -9.26666e-08 -1.00583e-07 -1.04308e-07 -1.2666e-07</p>
</blockquote>

<ol start=""2"">
<li>On OpenVINO 2020.2 had to change <code>ExecutableNetwork executable_network = ie.LoadNetwork(network, ""cpu"");</code> to <code>ExecutableNetwork executable_network = ie.LoadNetwork(network, ""CPU"");</code> as Inference Engine didnt't recognize lowercase CPU device, error was ""<em>terminate called after throwing an instance of 'InferenceEngine::details::InferenceEngineException'
what():  Device with ""cpu"" name is not registered in the InferenceEngine
Aborted (core dumped)</em>""</li>
<li>On OpenVINO 2020.2, the results differ and are not close to zero (although all seem negative).</li>
</ol>

<blockquote>
  <p>AM output: -3.55062 -3.5114 -3.50925 -3.52013 -3.51791 -3.54656 -3.53908 -3.54239 -3.53626 -3.50982 -3.54193 -3.55593 -3.52877 -3.53786 -3.52153 -3.52563 -3.51142 -3.54885 -3.52137 -3.54384 -3.53411 -3.55188 -3.5477 -3.52514 -3.51171 -3.5022 -3.5138 -3.50823 -3.50125 -3.51817 -3.53914 -3.50173 -3.50603 -3.51917 -3.55062 -3.5114 -3.50925 -3.52013 -3.51791 -3.54656 -3.53908 -3.54239 -3.53626 -3.50982 -3.54193 -3.55593 -3.52877 -3.53786 -3.52153 -3.52563 -3.51142 -3.54885 -3.52137 -3.54384 -3.53411 -3.55188 -3.5477 -3.52514 -3.51171 -3.5022 -3.5138 -3.50823 -3.50125 -3.51817 -3.53914 -3.50173 -3.50603 -3.51917</p>
</blockquote>

<p>It's uncertain if the output results of OpenVINO 2020.2 are expected/correct. I am unable to test Python example with the ONNX model using <code>onnxruntime</code>, script expects <code>/kek/fbank.out</code> file. Clarify/share what output is expected, i.e. correct AM output.</p>
","266","0","2","<c++><openvino>"
"61063772","OpenVino model outputs zeroes","2020-04-06 15:53:48","<p>I have an acoustic model that successfully converted from ONNX to OpenVino. However, in OpenVino this model outputs tensor that consists of zeroes from some position.</p>

<pre class=""lang-cpp prettyprint-override""><code>#include &lt;iostream&gt;
#include &lt;fstream&gt;
#include &lt;iterator&gt;
#include &lt;inference_engine.hpp&gt;

typedef struct {
    float* data;
    size_t size;
    size_t timeLen;
} Fbank;

using namespace InferenceEngine;
using std::cout;
using std::endl;

void print_arr(std::string text, const float* arr, int l, int r) {
    cout &lt;&lt; text &lt;&lt; endl;
    for (int i = l; i &lt; r; i++) {
        cout &lt;&lt; arr[i] &lt;&lt; "" "";
    }
    cout &lt;&lt; endl;
}

void doInference(ExecutableNetwork&amp; executable_network, const std::string&amp; input_name, const std::string&amp; output_name, Fbank* fbank) {

    InferRequest infer_request = executable_network.CreateInferRequest();

    InferenceEngine::TensorDesc tDesc(InferenceEngine::Precision::FP32,
        {fbank-&gt;size, fbank-&gt;timeLen}, InferenceEngine::Layout::HW);
    Blob::Ptr blob = InferenceEngine::make_shared_blob&lt;float&gt;(tDesc, fbank-&gt;data);

    infer_request.SetBlob(input_name, blob);

    infer_request.Infer();
    Blob::Ptr output_blob = infer_request.GetBlob(output_name);

    auto dims = output_blob-&gt;getTensorDesc().getDims();

    size_t batchSize = dims[0];
    size_t T = dims[1];
    size_t D = dims[2];

    MemoryBlob::CPtr moutput = as&lt;MemoryBlob&gt;(output_blob);
    if (!moutput) {
        return;
    }

    auto moutputHolder = moutput-&gt;rmap();
    const float *pred = moutputHolder.as&lt;const float*&gt;();

    print_arr(""AM output:"", pred, D*29, D*31);
}


int main() {
    Fbank* fbank = new Fbank;
    fbank-&gt;size = 64;
    fbank-&gt;timeLen = 2000;
    fbank-&gt;data = new float[64*2000];

    Core ie;
    CNNNetwork network = ie.ReadNetwork(""quartznet_random.xml"", ""quartznet_random.bin"");

    std::string input_name = network.getInputsInfo().begin()-&gt;first;
    std::string output_name = network.getOutputsInfo().begin()-&gt;first;

    network.getOutputsInfo().begin()-&gt;second-&gt;setPrecision(Precision::FP32);

    ExecutableNetwork executable_network = ie.LoadNetwork(network, ""cpu"");
    doInference(executable_network, input_name, output_name, fbank);
    return 0;
}
</code></pre>

<p>Outputs:</p>

<pre><code>AM output:
0.138650 -5.833140 -8.023724 -7.637482 -8.001101 -9.033963 -8.029905 -8.132050 -9.186495 -8.537528 -8.788505 -9.240234 -8.547676 -8.673388 0.000000 0.000000 -0.000000 0.000000 -0.000000 0.000000 0.000000 -0.000000 -0.000000 0.000000 -0.000000 0.000000 0.000000 -0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 -0.000000 0.000000 0.000000 -0.000000 0.000000 0.000000 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000 0.000000 -0.000000 -0.000000 -0.000000 0.000000 -0.000000 -0.000000 -0.000000 0.000000 0.000000 -0.000000 -0.000000 0.000000 -0.000000 0.000000 -0.000000 -0.000000 -0.000000 0.000000 -0.000000 -0.000000 0.000000 -0.000000 0.000000
</code></pre>

<p>If I run <a href=""https://transfer.vkpartner.ru/b1f50972c59c"" rel=""nofollow noreferrer"">ONNX model</a> in Python using <code>onnxruntime</code>, the output will be correct. (<a href=""https://gist.github.com/yutkin/50f38d56a682ed5a51d3aa3ef09bd52e"" rel=""nofollow noreferrer"">Example</a>).</p>

<p>Is it possible to fix it?</p>

<p>P.S. Command to convert the model from ONNX: <code>python3 mo_onnx.py —input_model model.onnx —output=""output"" —input=""fbanks[64 2000]""</code></p>
","<p>The problem was due to numerical instability in our implementation of LogSoftmax. Without log of softmax all works fine.</p>
","266","0","2","<c++><openvino>"
"60227602","Dynamic batch is not supported on Intel NCS2 vpu","2020-02-14 13:47:28","<p>I'm trying to run FP16 person-detection-retail-0013 and person-reidentification-retail-0079 on Intel Neural Compute Stick hardware, but once I run the application to load the nets on the device I get this exception: </p>

<pre><code>[INFERENCE ENGINE EXCEPTION] Dynamic batch is not supported
</code></pre>

<p>I've load the net with setting of the max batch size to 1 and I've started my project from the pedestrian tracker demo into the OpenVINO toolkit: </p>

<p><strong>main.cpp --> CreatePedestrianTracker</strong></p>

<pre><code>    CnnConfig reid_config(reid_model, reid_weights);
    reid_config.max_batch_size = 16;

    try {
        if (ie.GetConfig(deviceName, CONFIG_KEY(DYN_BATCH_ENABLED)).as&lt;std::string&gt;() != 
            PluginConfigParams::YES) {
            reid_config.max_batch_size = 1;
            std::cerr &lt;&lt; ""[DEBUG] Dynamic batch is not supported for "" &lt;&lt; deviceName &lt;&lt; "". Fall back 
            to batch 1."" &lt;&lt; std::endl;
        }
    }
    catch (const InferenceEngine::details::InferenceEngineException&amp; e) {
        reid_config.max_batch_size = 1;
        std::cerr &lt;&lt; e.what() &lt;&lt; "" for "" &lt;&lt; deviceName &lt;&lt; "". Fall back to batch 1."" &lt;&lt; std::endl;
    }
</code></pre>

<p><strong>Cnn.cpp --> void CnnBase::InferBatch</strong></p>

<pre><code>void CnnBase::InferBatch(
const std::vector&lt;cv::Mat&gt;&amp; frames,
std::function&lt;void(const InferenceEngine::BlobMap&amp;, size_t)&gt; fetch_results) const {
const size_t batch_size = input_blob_-&gt;getTensorDesc().getDims()[0];

size_t num_imgs = frames.size();
for (size_t batch_i = 0; batch_i &lt; num_imgs; batch_i += batch_size) {

    const size_t current_batch_size = std::min(batch_size, num_imgs - batch_i);

    for (size_t b = 0; b &lt; current_batch_size; b++) {
        matU8ToBlob&lt;uint8_t&gt;(frames[batch_i + b], input_blob_, b); 
    }

    if ((deviceName_.find(""MYRIAD"") == std::string::npos) &amp;&amp; (deviceName_.find(""HDDL"") == 
        std::string::npos)) {
        infer_request_.SetBatch(current_batch_size); 
    }

    infer_request_.Infer();

    fetch_results(outputs_, current_batch_size);
 }
}
</code></pre>

<p>I suppose that the problem could be the topology of the detection net, but I ask if anyone has had the same problem and solved the issue.<br>
Thank's. </p>
","<p>I am afraid, myriad plugin does not support dynamic batch. Please try an updated version of the demo. You can find it, for example, here: <a href=""https://github.com/opencv/open_model_zoo/tree/master/demos/pedestrian_tracker_demo"" rel=""nofollow noreferrer"">https://github.com/opencv/open_model_zoo/tree/master/demos/pedestrian_tracker_demo</a>
The demo is updated not to use dynamic batch at all.</p>
","265","1","1","<c++><opencv><computer-vision><artificial-intelligence><openvino>"
"62945678","How can I write a Dockerfile to merge two or more docker images into one?","2020-07-17 01:18:19","<p>I am trying to integrate three things into one docker image.</p>
<p>The first image is: <a href=""https://github.com/opencv/gst-video-analytics"" rel=""nofollow noreferrer"">https://github.com/opencv/gst-video-analytics</a></p>
<p>The second image is: <a href=""https://github.com/domoritz/streamlit-docker"" rel=""nofollow noreferrer"">https://github.com/domoritz/streamlit-docker</a></p>
<p>The third image is: <a href=""http://blog.feabhas.com/2020/02/running-the-eclipse-mosquitto-mqtt-broker-in-a-docker-container/"" rel=""nofollow noreferrer"">http://blog.feabhas.com/2020/02/running-the-eclipse-mosquitto-mqtt-broker-in-a-docker-container/</a></p>
<p>Can someone guide me on how can I integrate these three images into one on docker in Windows 10?</p>
<pre><code>Operating system: Windows 10 

Docker version 19.03.1
</code></pre>
","<p>I did something similar with <a href=""https://github.com/kichik/docker-combo"" rel=""nofollow noreferrer"">docker-combos</a>. You basically just include all the lines from all three <code>Dockerfile</code> files, but only one <code>FROM</code> line. Then start correcting errors as they come up. You may have a bunch of errors because the <code>FROM</code> lines aren't the same (one is Ubuntu, one is Alpine so you have to change <code>apk add</code> to <code>apt-get install</code>).</p>
<p>You can also consider not using a single Docker container for all these and go with <code>docker-compose</code>. It lets you bring up multiple containers all together and link their network for easy access.</p>
","257","0","1","<windows><docker><openvino><streamlit>"
"67510651","Openvino: Failed to create plugin libclDNNPlugin.so for device GPU","2021-05-12 20:31:22","<p>I would like to run OpenVINO on an integrated GPU Intel HD 400.</p>
<p>When I run it I have the following error:</p>
<pre><code>12.05.21 16:02:27 (-0400)       self._engine = self._ie.load_network(**openvino_config)
12.05.21 16:02:27 (-0400)     File &quot;ie_api.pyx&quot;, line 311, in openvino.inference_engine.ie_api.IECore.load_network
12.05.21 16:02:27 (-0400)     File &quot;ie_api.pyx&quot;, line 320, in openvino.inference_engine.ie_api.IECore.load_network
12.05.21 16:02:27 (-0400)   RuntimeError: Failed to create plugin /opt/intel/openvino_2021.1.110/deployment_tools/inference_engine/lib/intel64/libclDNNPlugin.so for device GPU
 I followed the instructions to install the GPU plugin here Steps for Intel® Processor Graphics (GPU) and I can confirm that libclDNNPlugin.so exists.
I am running the code within a docker container and I am not sure if the host os has the proper drivers installed. 
I run lsmod on host os and I got the following output

Module                  Size  Used by
bnep                   20480  2
ip6t_REJECT            16384  2
nf_reject_ipv6         16384  1 ip6t_REJECT
ip6table_filter        16384  1
xt_state               16384  0
ipt_REJECT             16384  2
nf_reject_ipv4         16384  1 ipt_REJECT
ip6_tables             28672  1 ip6table_filter
xt_MASQUERADE          16384  12
nf_conntrack_netlink    32768  0
nfnetlink              16384  2 nf_conntrack_netlink
xfrm_user              36864  1
xt_owner               16384  0
snd_hda_codec_hdmi     53248  1
snd_hda_codec_realtek    98304  1
snd_hda_codec_generic    65536  1 snd_hda_codec_realtek
snd_hda_intel          32768  0
coretemp               16384  0
snd_intel_dspcfg       16384  1 snd_hda_intel
snd_hda_codec          94208  4 snd_hda_codec_generic,snd_hda_codec_hdmi,snd_hda_intel,snd_hda_codec_realtek
i915                 1802240  5
at24                   20480  0
r8169                  73728  0
snd_hda_core           65536  5 snd_hda_codec_generic,snd_hda_codec_hdmi,snd_hda_intel,snd_hda_codec,snd_hda_codec_realtek
regmap_i2c             16384  1 at24
snd_pcm                86016  4 snd_hda_codec_hdmi,snd_hda_intel,snd_hda_codec,snd_hda_core
efivars                20480  0
realtek                20480  2
libphy                 81920  2 r8169,realtek
snd_timer              28672  1 snd_pcm
video                  40960  1 i915
backlight              16384  2 video,i915
sch_fq_codel           20480  4
</code></pre>
<p>The error specifies: <code>[CLDNN ERROR]. No GPU device was found.</code></p>
<p>Also, <code>clinfo</code> reported 0 platform available.
I run the following commands:</p>
<pre><code>sudo apt install ocl-icd-libopencl1
sudo apt install opencl-headers
sudo apt install clinfo
sudo apt install ocl-icd-opencl-dev
sudo apt install beignet
</code></pre>
<p>and now the <code>clinfo</code> output is:</p>
<pre><code>Number of platforms                               1
  Platform Name                                   Intel Gen OCL Driver
  Platform Vendor                                 Intel
  Platform Version                                OpenCL 2.0 beignet 1.3
  Platform Profile                                FULL_PROFILE
  Platform Extensions                             cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_3d_image_writes cl_khr_image2d_from_buffer cl_khr_depth_images cl_khr_spir cl_khr_icd cl_intel_accelerator cl_intel_subgroups cl_intel_subgroups_short cl_khr_gl_sharing
  Platform Extensions function suffix             Intel

  Platform Name                                   Intel Gen OCL Driver
Number of devices                                 1
  Device Name                                     Intel(R) HD Graphics Cherryview
  Device Vendor                                   Intel
  Device Vendor ID                                0x8086
  Device Version                                  OpenCL 1.2 beignet 1.3
  Driver Version                                  1.3
  Device OpenCL C Version                         OpenCL C 1.2 beignet 1.3
  Device Type                                     GPU
  Device Profile                                  FULL_PROFILE
  Device Available                                Yes
  Compiler Available                              Yes
  Linker Available                                Yes
  Max compute units                               12
  Max clock frequency                             1000MHz
  Device Partition                                (core)
    Max number of sub-devices                     1
    Supported partition types                     None, None, None
  Max work item dimensions                        3
  Max work item sizes                             512x512x512
  Max work group size                             512
  Preferred work group size multiple              16
  Preferred / native vector sizes                 
    char                                                16 / 8       
    short                                                8 / 8       
    int                                                  4 / 4       
    long                                                 2 / 2       
    half                                                 0 / 8        (cl_khr_fp16)
    float                                                4 / 4       
    double                                               0 / 2        (n/a)
  Half-precision Floating-point support           (cl_khr_fp16)
    Denormals                                     No
    Infinity and NANs                             Yes
    Round to nearest                              Yes
    Round to zero                                 No
    Round to infinity                             No
    IEEE754-2008 fused multiply-add               No
    Support is emulated in software               No
  Single-precision Floating-point support         (core)
    Denormals                                     No
    Infinity and NANs                             Yes
    Round to nearest                              Yes
    Round to zero                                 No
    Round to infinity                             No
    IEEE754-2008 fused multiply-add               No
    Support is emulated in software               No
    Correctly-rounded divide and sqrt operations  No
  Double-precision Floating-point support         (n/a)
  Address bits                                    32, Little-Endian
  Global memory size                              2147483648 (2GiB)
  Error Correction support                        No
  Max memory allocation                           1610612736 (1.5GiB)
  Unified memory for Host and Device              Yes
  Minimum alignment for any data type             128 bytes
  Alignment of base address                       1024 bits (128 bytes)
  Global Memory cache type                        Read/Write
  Global Memory cache size                        8192 (8KiB)
  Global Memory cache line size                   64 bytes
  Image support                                   Yes
    Max number of samplers per kernel             16
    Max size for 1D images from buffer            65536 pixels
    Max 1D or 2D image array size                 2048 images
    Base address alignment for 2D image buffers   4096 bytes
    Pitch alignment for 2D image buffers          1 pixels
    Max 2D image size                             8192x8192 pixels
    Max 3D image size                             8192x8192x2048 pixels
    Max number of read image args                 128
    Max number of write image args                8
  Local memory type                               Local
  Local memory size                               65536 (64KiB)
  Max number of constant args                     8
  Max constant buffer size                        134217728 (128MiB)
  Max size of kernel argument                     1024
  Queue properties                                
    Out-of-order execution                        No
    Profiling                                     Yes
  Prefer user sync for interop                    Yes
  Profiling timer resolution                      80ns
  Execution capabilities                          
    Run OpenCL kernels                            Yes
    Run native kernels                            Yes
    SPIR versions                                 1.2
  printf() buffer size                            1048576 (1024KiB)
  Built-in kernels                                __cl_copy_region_align4;__cl_copy_region_align16;__cl_cpy_region_unalign_same_offset;__cl_copy_region_unalign_dst_offset;__cl_copy_region_unalign_src_offset;__cl_copy_buffer_rect;__cl_copy_image_1d_to_1d;__cl_copy_image_2d_to_2d;__cl_copy_image_3d_to_2d;__cl_copy_image_2d_to_3d;__cl_copy_image_3d_to_3d;__cl_copy_image_2d_to_buffer;__cl_copy_image_3d_to_buffer;__cl_copy_buffer_to_image_2d;__cl_copy_buffer_to_image_3d;__cl_fill_region_unalign;__cl_fill_region_align2;__cl_fill_region_align4;__cl_fill_region_align8_2;__cl_fill_region_align8_4;__cl_fill_region_align8_8;__cl_fill_region_align8_16;__cl_fill_region_align128;__cl_fill_image_1d;__cl_fill_image_1d_array;__cl_fill_image_2d;__cl_fill_image_2d_array;__cl_fill_image_3d;
  Device Extensions                               cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_3d_image_writes cl_khr_image2d_from_buffer cl_khr_depth_images cl_khr_spir cl_khr_icd cl_intel_accelerator cl_intel_subgroups cl_intel_subgroups_short cl_khr_gl_sharing cl_khr_fp16

NULL platform behavior
  clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...)  Intel Gen OCL Driver
  clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...)   Success [Intel]
  clCreateContext(NULL, ...) [default]            Success [Intel]
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_DEFAULT)  Success (1)
    Platform Name                                 Intel Gen OCL Driver
    Device Name                                   Intel(R) HD Graphics Cherryview
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU)  Success (1)
    Platform Name                                 Intel Gen OCL Driver
    Device Name                                   Intel(R) HD Graphics Cherryview
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL)  Success (1)
    Platform Name                                 Intel Gen OCL Driver
    Device Name                                   Intel(R) HD Graphics Cherryview

ICD loader properties
  ICD loader Name                                 OpenCL ICD Loader
  ICD loader Vendor                               OCL Icd free software
  ICD loader Version                              2.2.11
  ICD loader Profile                              OpenCL 2.1
root@406231114801:/src# clinfo
Number of platforms                               1
  Platform Name                                   Intel Gen OCL Driver
  Platform Vendor                                 Intel
  Platform Version                                OpenCL 2.0 beignet 1.3
  Platform Profile                                FULL_PROFILE
  Platform Extensions                             cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_3d_image_writes cl_khr_image2d_from_buffer cl_khr_depth_images cl_khr_spir cl_khr_icd cl_intel_accelerator cl_intel_subgroups cl_intel_subgroups_short cl_khr_gl_sharing
  Platform Extensions function suffix             Intel

  Platform Name                                   Intel Gen OCL Driver
Number of devices                                 1
  Device Name                                     Intel(R) HD Graphics Cherryview
  Device Vendor                                   Intel
  Device Vendor ID                                0x8086
  Device Version                                  OpenCL 1.2 beignet 1.3
  Driver Version                                  1.3
  Device OpenCL C Version                         OpenCL C 1.2 beignet 1.3
  Device Type                                     GPU
  Device Profile                                  FULL_PROFILE
  Device Available                                Yes
  Compiler Available                              Yes
  Linker Available                                Yes
  Max compute units                               12
  Max clock frequency                             1000MHz
  Device Partition                                (core)
    Max number of sub-devices                     1
    Supported partition types                     None, None, None
  Max work item dimensions                        3
  Max work item sizes                             512x512x512
  Max work group size                             512
  Preferred work group size multiple              16
  Preferred / native vector sizes                 
    char                                                16 / 8       
    short                                                8 / 8       
    int                                                  4 / 4       
    long                                                 2 / 2       
    half                                                 0 / 8        (cl_khr_fp16)
    float                                                4 / 4       
    double                                               0 / 2        (n/a)
  Half-precision Floating-point support           (cl_khr_fp16)
    Denormals                                     No
    Infinity and NANs                             Yes
    Round to nearest                              Yes
    Round to zero                                 No
    Round to infinity                             No
    IEEE754-2008 fused multiply-add               No
    Support is emulated in software               No
  Single-precision Floating-point support         (core)
    Denormals                                     No
    Infinity and NANs                             Yes
    Round to nearest                              Yes
    Round to zero                                 No
    Round to infinity                             No
    IEEE754-2008 fused multiply-add               No
    Support is emulated in software               No
    Correctly-rounded divide and sqrt operations  No
  Double-precision Floating-point support         (n/a)
  Address bits                                    32, Little-Endian
  Global memory size                              2147483648 (2GiB)
  Error Correction support                        No
  Max memory allocation                           1610612736 (1.5GiB)
  Unified memory for Host and Device              Yes
  Minimum alignment for any data type             128 bytes
  Alignment of base address                       1024 bits (128 bytes)
  Global Memory cache type                        Read/Write
  Global Memory cache size                        8192 (8KiB)
  Global Memory cache line size                   64 bytes
  Image support                                   Yes
    Max number of samplers per kernel             16
    Max size for 1D images from buffer            65536 pixels
    Max 1D or 2D image array size                 2048 images
    Base address alignment for 2D image buffers   4096 bytes
    Pitch alignment for 2D image buffers          1 pixels
    Max 2D image size                             8192x8192 pixels
    Max 3D image size                             8192x8192x2048 pixels
    Max number of read image args                 128
    Max number of write image args                8
  Local memory type                               Local
  Local memory size                               65536 (64KiB)
  Max number of constant args                     8
  Max constant buffer size                        134217728 (128MiB)
  Max size of kernel argument                     1024
  Queue properties                                
    Out-of-order execution                        No
    Profiling                                     Yes
  Prefer user sync for interop                    Yes
  Profiling timer resolution                      80ns
  Execution capabilities                          
    Run OpenCL kernels                            Yes
    Run native kernels                            Yes
    SPIR versions                                 1.2
  printf() buffer size                            1048576 (1024KiB)
  Built-in kernels                                __cl_copy_region_align4;__cl_copy_region_align16;__cl_cpy_region_unalign_same_offset;__cl_copy_region_unalign_dst_offset;__cl_copy_region_unalign_src_offset;__cl_copy_buffer_rect;__cl_copy_image_1d_to_1d;__cl_copy_image_2d_to_2d;__cl_copy_image_3d_to_2d;__cl_copy_image_2d_to_3d;__cl_copy_image_3d_to_3d;__cl_copy_image_2d_to_buffer;__cl_copy_image_3d_to_buffer;__cl_copy_buffer_to_image_2d;__cl_copy_buffer_to_image_3d;__cl_fill_region_unalign;__cl_fill_region_align2;__cl_fill_region_align4;__cl_fill_region_align8_2;__cl_fill_region_align8_4;__cl_fill_region_align8_8;__cl_fill_region_align8_16;__cl_fill_region_align128;__cl_fill_image_1d;__cl_fill_image_1d_array;__cl_fill_image_2d;__cl_fill_image_2d_array;__cl_fill_image_3d;
  Device Extensions                               cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_3d_image_writes cl_khr_image2d_from_buffer cl_khr_depth_images cl_khr_spir cl_khr_icd cl_intel_accelerator cl_intel_subgroups cl_intel_subgroups_short cl_khr_gl_sharing cl_khr_fp16

NULL platform behavior
  clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...)  Intel Gen OCL Driver
  clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...)   Success [Intel]
  clCreateContext(NULL, ...) [default]            Success [Intel]
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_DEFAULT)  Success (1)
    Platform Name                                 Intel Gen OCL Driver
    Device Name                                   Intel(R) HD Graphics Cherryview
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU)  Success (1)
    Platform Name                                 Intel Gen OCL Driver
    Device Name                                   Intel(R) HD Graphics Cherryview
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL)  Success (1)
    Platform Name                                 Intel Gen OCL Driver
    Device Name                                   Intel(R) HD Graphics Cherryview

ICD loader properties
  ICD loader Name                                 OpenCL ICD Loader
  ICD loader Vendor                               OCL Icd free software
  ICD loader Version                              2.2.11
  ICD loader Profile                              OpenCL 2.1
</code></pre>
<p>How can I solve the issue?</p>
","<p>According to these <a href=""https://github.com/intel/compute-runtime#supported-platforms"" rel=""nofollow noreferrer"">supported platforms</a> information, your GPU currently might not supported. Could you run the programs on CPU? Try running this command &quot; lspci | grep -i vga &quot; and share the output here.</p>
","252","0","1","<gpu><openvino>"
"59872027","Can openVINO model optimiser be used to convert tensorflow ann models?","2020-01-23 05:25:39","<p>I trained an ANN model as saved it as .h5 file.Then I converted the model into tensorflow model and got <code>'savedmodel.pb'</code> and <code>'variables'</code> folder.
Then I used model optimiser openvino to generate IR files using:</p>

<pre><code>python3 mo_tf.py --input_model saved_model.pb
</code></pre>

<p>But I get the following error:</p>

<pre><code>[ FRAMEWORK ERROR ]  Error parsing message
TensorFlow cannot read the model file: ""/home/user/Downloads/OpenVino/dldt-2019/model-optimizer/saved_model.pb"" is incorrect TensorFlow model file
</code></pre>

<p>Can <code>openVINO</code> be used to convert <code>ANN</code> models in the first place?</p>
","<p>Unfortunately, OpenVINO does not support Keras model format.</p>

<p>I guess you got an error from model optimizer because your model is not frozen. </p>

<p>There are a lot of scripts which can be used to convert a Keras model to a frozen Tensorflow graph. I can recommend <a href=""https://github.com/amir-abdi/keras_to_tensorflow"" rel=""nofollow noreferrer"">this one</a> for example.</p>

<p>Hope it will help.</p>
","250","1","1","<tensorflow><openvino><inference-engine>"
"59852686","Unexpected exception happened during extracting attributes Open Vino","2020-01-22 04:36:41","<p>I was trying to convert a Caffe model using mo_caffe.py script. I always get errors like below, but in random nodes (all have the common ""BatchNorm"" op). Trained the model using Nvidia-Digits (<a href=""https://github.com/NVIDIA/DIGITS"" rel=""nofollow noreferrer"">https://github.com/NVIDIA/DIGITS</a>)</p>

<pre><code>Model Optimizer arguments:
Common parameters:
        - Path to the Input Model:      /home/deploy.caffemodel
        - Path for generated IR:        /dldt/model-optimizer/.
        - IR output name:       deploy
        - Log level:    INFO
        - Batch:        Not specified, inherited from the model
        - Input layers:         Not specified, inherited from the model
        - Output layers:        Not specified, inherited from the model
        - Input shapes:         Not specified, inherited from the model
        - Mean values:  Not specified
        - Scale values:         Not specified
        - Scale factor:         Not specified
        - Precision of IR:      FP32
        - Enable fusing:        True
        - Enable grouped convolutions fusing:   True
        - Move mean values to preprocess section:       False
        - Reverse input channels:       False
Caffe specific parameters:
        - Path to Python Caffe* parser generated from caffe.proto:      mo/front/caffe/proto
        - Enable resnet optimization:   True
        - Path to the Input prototxt:   /home/deploy.prototxt
        - Path to CustomLayersMapping.xml:      Default
        - Path to a mean file:  Not specified
        - Offsets for a mean file:      Not specified
Model Optimizer version:        unknown version
[ INFO ]  Importing extensions from: /dldt/model-optimizer/mo
[ INFO ]  New subclass: &lt;class 'mo.ops.crop.Crop'&gt;
[ INFO ]  Registered a new subclass with key: Crop
[ INFO ]  New subclass: &lt;class 'mo.ops.deformable_convolution.DeformableConvolution'&gt;
[ INFO ]  Registered a new subclass with key: DeformableConvolution
[ INFO ]  New subclass: &lt;class 'mo.ops.concat.Concat'&gt;
[ INFO ]  Registered a new subclass with key: Concat
[ INFO ]  New subclass: &lt;class 'mo.ops.split.Split'&gt;
...
Some log info. I don't think anything interesting here.
...
[ WARNING ]  Skipped &lt;class 'extensions.front.override_batch.OverrideBatch'&gt; registration because it was already registered or it was disabled.
[ WARNING ]  Skipped &lt;class 'extensions.front.TopKNormalize.TopKNormalize'&gt; registration because it was already registered or it was disabled.
[ WARNING ]  Skipped &lt;class 'extensions.front.reshape_dim_normalizer.ReshapeDimNormalizer'&gt; registration because it was already registered or it was
disabled.
[ WARNING ]  Skipped &lt;class 'extensions.front.ArgMaxSqueeze.ArgMaxSqueeze'&gt; registration because it was already registered or it was disabled.
[ WARNING ]  Skipped &lt;class 'extensions.front.standalone_const_eraser.StandaloneConstEraser'&gt; registration because it was already registered or it wa
s disabled.
[ WARNING ]  Skipped &lt;class 'extensions.front.TransposeOrderNormalizer.TransposeOrderNormalizer'&gt; registration because it was already registered or i
t was disabled.
[ WARNING ]  Skipped &lt;class 'mo.front.common.replacement.FrontReplacementOp'&gt; registration because it was already registered or it was disabled.
[ WARNING ]  Skipped &lt;class 'extensions.front.restore_ports.RestorePorts'&gt; registration because it was already registered or it was disabled.
[ WARNING ]  Skipped &lt;class 'extensions.front.softmax.SoftmaxFromKeras'&gt; registration because it was already registered or it was disabled.
[ WARNING ]  Skipped &lt;class 'extensions.front.reduce_axis_normalizer.ReduceAxisNormalizer'&gt; registration because it was already registered or it was
disabled.
[ WARNING ]  Skipped &lt;class 'extensions.front.freeze_placeholder_value.FreezePlaceholderValue'&gt; registration because it was already registered or it
was disabled.
[ WARNING ]  Skipped &lt;class 'extensions.front.no_op_eraser.NoOpEraser'&gt; registration because it was already registered or it was disabled.
[ WARNING ]  Node attributes: {'_in_ports': {}, 'model_pb': name: ""conv2_3_sep_bn_left""
type: ""BatchNorm""
bottom: ""conv2_3_sep_left""
top: ""conv2_3_sep_left""
param {
  lr_mult: 0.0
  decay_mult: 0.0
}
param {
  lr_mult: 0.0
  decay_mult: 0.0
}
param {
  lr_mult: 0.0
  decay_mult: 0.0
}
blobs {                                                                                                                                      [0/1963]
  shape {
    dim: 32
  }
}
blobs {
  shape {
    dim: 32
  }
}
blobs {
  shape {
    dim: 1
  }
}
phase: TRAIN
, 'kind': 'op', '_out_ports': {}, 'pb': name: ""conv2_3_sep_bn_left""
type: ""BatchNorm""
bottom: ""conv2_3_sep_left""
top: ""conv2_3_sep_left""
param {
  lr_mult: 0.0
  decay_mult: 0.0
}
param {
  lr_mult: 0.0
  decay_mult: 0.0
}
param {
  lr_mult: 0.0
  decay_mult: 0.0
}
, 'type': 'Parameter'}
[ ERROR ]  Unexpected exception happened during extracting attributes for node relu1.
Original exception message: list index (0) out of range
</code></pre>

<p>This is the error message I get. I also have created a GitHub <a href=""https://github.com/opencv/dldt/issues/371"" rel=""nofollow noreferrer"">issue</a></p>

<p><a href=""https://drive.google.com/drive/folders/1yjRrDRIpO0nzIENYbaD_IOuRXVCvbEpH?usp=sharing"" rel=""nofollow noreferrer"">Here</a> is the model that I want to convert from Caffe to OpenVino</p>
","<p>The problem was that Nvidia-Digits uses a customized <em>forked version of Caffe</em>. That's why the model weights were not read properly by <em>OpenVino</em>.</p>

<p>I had to use <a href=""https://gist.github.com/jkjung-avt/0d1e04d0a09c606328dc5e53a7f4ece8"" rel=""nofollow noreferrer"">this</a> script to convert the model before converting it with OpenVino.</p>
","250","0","1","<machine-learning><deep-learning><computer-vision><caffe><openvino>"
"64159152","Qt5 Openvino opencv cmake windows","2020-10-01 16:04:54","<p>I am trying to develop an application with qt that uses the openvino inference engine with opencv.<br />
I have been trying to create the project with qmake first but I couldn't manage then I switched to cmake an which led to some improvements but still no success.<br />
openvino: openvino_2020.04.287 <br />
opencv: the one included in openvino<br />
cmake: 3.14.7<br />
qt: qt 5_15_0\</p>
<p>As I have read openvino works with mscvc so that is what i am using instead of mingw.<br />
My CMakeLists.txt looks as follows:</p>
<pre><code>cmake_minimum_required(VERSION 3.14.7 FATAL_ERROR)

project(PortraitSegmentationWin LANGUAGES CXX)

set(CMAKE_INCLUDE_CURRENT_DIR ON)

find_package(QT5 REQUIRED COMPONENTS Core Widgets Gui)
find_package(InferenceEngine REQUIRED)
find_package(OpenCV REQUIRED)


set(project_ui
    mainwindow.ui)

set(project_headers
    mainwindow.h)

set(project_sources
    main.cpp
    mainwindow.cpp)

qt5_wrap_ui(project_headers_wrapped ${project_ui})
qt5_wrap_cpp(project_sources_moc ${project_headers})

add_executable(${PROJECT_NAME} ${project_headers} 
               ${project_sources} ${project_headers_wrapped} 
               ${project_sources_moc})

target_link_libraries(${PROJECT_NAME}
    PUBLIC
    ${QT5Widgets_LIBRARIES}
    ${Qt5Core_LIBRARIES}
    ${QT5Gui_LIBRARIES}
    ${InferenceEngine_LIBRARIES} 
    ${OpenCV_LIBS} )
</code></pre>
<p><br />
And when it gives me the following error:<br />
<em>CMake Error at PortraitSegmentationWin/CMakeLists.txt:22 (qt5_wrap_ui):
Unknown CMake command &quot;qt5_wrap_ui&quot;.</em>\</p>
<p>This I managed to solve if I use find_package(Qt5Widgets) and then I can proceed to the Generate option and the I can even open the project but then when I build it, it fails and gives me LINK2019 and LINK2001 error 144 one of them...</p>
<p>Another version of my CMakeLists.txt is:</p>
<pre><code>cmake_minimum_required(VERSION &quot;3.14.7&quot;)

project(PortraitSegmentation_openVINO_OpenCV LANGUAGES CXX)

set(CMAKE_INCLUDE_CURRENT_DIR ON)

set(CMAKE_AUTOUIC ON)
set(CMAKE_AUTOMOC ON)
set(CMAKE_AUTORCC ON)

set(CMAKE_CXX_STANDARD 11)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# QtCreator supports the following variables for Android, which are identical to qmake Android variables.
# Check http://doc.qt.io/qt-5/deployment-android.html for more information.
# They need to be set before the find_package(Qt5 ...) call.

#if(ANDROID)
#    set(ANDROID_PACKAGE_SOURCE_DIR &quot;${CMAKE_CURRENT_SOURCE_DIR}/android&quot;)
#    if (ANDROID_ABI STREQUAL &quot;armeabi-v7a&quot;)
#        set(ANDROID_EXTRA_LIBS
#            ${CMAKE_CURRENT_SOURCE_DIR}/path/to/libcrypto.so
#            ${CMAKE_CURRENT_SOURCE_DIR}/path/to/libssl.so)
#    endif()
#endif()

find_package(QT NAMES Qt6 Qt5 COMPONENTS Widgets REQUIRED)
find_package(Qt${QT_VERSION_MAJOR} COMPONENTS Widgets REQUIRED)
find_package(InferenceEngine REQUIRED)
find_package(ngraph REQUIRED)
find_package(OpenCV REQUIRED)

set(PortraitSegmentation_openVINO_OpenCV
    main.cpp
    mainwindow.cpp
    mainwindow.h
    mainwindow.ui
    )

add_executable(PortraitSegmentation_openVINO_OpenCV
  main.cpp
  mainwindow.cpp
  mainwindow.h
  mainwindow.ui
  )


target_link_libraries(PortraitSegmentation_openVINO_OpenCV PRIVATE Qt${QT_VERSION_MAJOR}::Widgets Qt5::Core Qt5::Gui  ${InferenceEngine_LIBRARIES} ${OpenCV_LIBRARIES} ${NGRAPH_LIBRARIES})

</code></pre>
<p>This on the other hand tells me that there is no qt-plugin detected.<br />
I am really clueless here I would appreciate some help of any sort!</p>
<p>Thanks in advance</p>
","<p>First and foremost, you should read these to get more infos on what's works:</p>
<ol>
<li><p><a href=""https://stackoverflow.com/questions/57727665/how-to-deploy-openvino-opencv-in-qt"">how to deploy openvino-opencv in Qt</a></p>
</li>
<li><p><a href=""https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/SOLVED-Develop-OpenVINO-with-QT/td-p/1147234"" rel=""nofollow noreferrer"">https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/SOLVED-Develop-OpenVINO-with-QT/td-p/1147234</a></p>
</li>
</ol>
<p>Next, the biggest concern is, which OS you are running the QT in.
If you are using Windows, <strong>you need to have all the prerequisite</strong> as in here:
<a href=""https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_windows.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_windows.html</a></p>
<p>Same thing applies for others
(Note: you can search for other OS on left corner &amp; ensure the correct version on top right dropdown menu )</p>
<p>Ensure to run the setupvars each time you open a new terminal.</p>
","248","0","2","<qt><cmake><windows-10><visual-studio-2019><openvino>"
"64159152","Qt5 Openvino opencv cmake windows","2020-10-01 16:04:54","<p>I am trying to develop an application with qt that uses the openvino inference engine with opencv.<br />
I have been trying to create the project with qmake first but I couldn't manage then I switched to cmake an which led to some improvements but still no success.<br />
openvino: openvino_2020.04.287 <br />
opencv: the one included in openvino<br />
cmake: 3.14.7<br />
qt: qt 5_15_0\</p>
<p>As I have read openvino works with mscvc so that is what i am using instead of mingw.<br />
My CMakeLists.txt looks as follows:</p>
<pre><code>cmake_minimum_required(VERSION 3.14.7 FATAL_ERROR)

project(PortraitSegmentationWin LANGUAGES CXX)

set(CMAKE_INCLUDE_CURRENT_DIR ON)

find_package(QT5 REQUIRED COMPONENTS Core Widgets Gui)
find_package(InferenceEngine REQUIRED)
find_package(OpenCV REQUIRED)


set(project_ui
    mainwindow.ui)

set(project_headers
    mainwindow.h)

set(project_sources
    main.cpp
    mainwindow.cpp)

qt5_wrap_ui(project_headers_wrapped ${project_ui})
qt5_wrap_cpp(project_sources_moc ${project_headers})

add_executable(${PROJECT_NAME} ${project_headers} 
               ${project_sources} ${project_headers_wrapped} 
               ${project_sources_moc})

target_link_libraries(${PROJECT_NAME}
    PUBLIC
    ${QT5Widgets_LIBRARIES}
    ${Qt5Core_LIBRARIES}
    ${QT5Gui_LIBRARIES}
    ${InferenceEngine_LIBRARIES} 
    ${OpenCV_LIBS} )
</code></pre>
<p><br />
And when it gives me the following error:<br />
<em>CMake Error at PortraitSegmentationWin/CMakeLists.txt:22 (qt5_wrap_ui):
Unknown CMake command &quot;qt5_wrap_ui&quot;.</em>\</p>
<p>This I managed to solve if I use find_package(Qt5Widgets) and then I can proceed to the Generate option and the I can even open the project but then when I build it, it fails and gives me LINK2019 and LINK2001 error 144 one of them...</p>
<p>Another version of my CMakeLists.txt is:</p>
<pre><code>cmake_minimum_required(VERSION &quot;3.14.7&quot;)

project(PortraitSegmentation_openVINO_OpenCV LANGUAGES CXX)

set(CMAKE_INCLUDE_CURRENT_DIR ON)

set(CMAKE_AUTOUIC ON)
set(CMAKE_AUTOMOC ON)
set(CMAKE_AUTORCC ON)

set(CMAKE_CXX_STANDARD 11)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# QtCreator supports the following variables for Android, which are identical to qmake Android variables.
# Check http://doc.qt.io/qt-5/deployment-android.html for more information.
# They need to be set before the find_package(Qt5 ...) call.

#if(ANDROID)
#    set(ANDROID_PACKAGE_SOURCE_DIR &quot;${CMAKE_CURRENT_SOURCE_DIR}/android&quot;)
#    if (ANDROID_ABI STREQUAL &quot;armeabi-v7a&quot;)
#        set(ANDROID_EXTRA_LIBS
#            ${CMAKE_CURRENT_SOURCE_DIR}/path/to/libcrypto.so
#            ${CMAKE_CURRENT_SOURCE_DIR}/path/to/libssl.so)
#    endif()
#endif()

find_package(QT NAMES Qt6 Qt5 COMPONENTS Widgets REQUIRED)
find_package(Qt${QT_VERSION_MAJOR} COMPONENTS Widgets REQUIRED)
find_package(InferenceEngine REQUIRED)
find_package(ngraph REQUIRED)
find_package(OpenCV REQUIRED)

set(PortraitSegmentation_openVINO_OpenCV
    main.cpp
    mainwindow.cpp
    mainwindow.h
    mainwindow.ui
    )

add_executable(PortraitSegmentation_openVINO_OpenCV
  main.cpp
  mainwindow.cpp
  mainwindow.h
  mainwindow.ui
  )


target_link_libraries(PortraitSegmentation_openVINO_OpenCV PRIVATE Qt${QT_VERSION_MAJOR}::Widgets Qt5::Core Qt5::Gui  ${InferenceEngine_LIBRARIES} ${OpenCV_LIBRARIES} ${NGRAPH_LIBRARIES})

</code></pre>
<p>This on the other hand tells me that there is no qt-plugin detected.<br />
I am really clueless here I would appreciate some help of any sort!</p>
<p>Thanks in advance</p>
","<p>if you set the setupvars permanently as in Linux, then you don't have to initiate it each time you open  a new terminal. However, most of the time in Windows, you required to do so as in the guide I provided above does not specify on doing permanently initiation of setupvars.</p>
<p>The error   No symbol loaded for inference_engined.dll  must related with LINK2019 and LINK2001 error which  might caused by MSbuild. I recommend for you to uninstall all version of Microsoft Visual Studio  with its MSBuild  and try to reinstall the version 2019 with MSBuild. This solved  my problem previously.</p>
","248","0","2","<qt><cmake><windows-10><visual-studio-2019><openvino>"
"64408396","Tensorflow ImportError (Unknown Location)","2020-10-17 23:46:43","<p>When trying to run a program on JupyterLab within an OpenVino environment on my Raspberry Pi 4, I get the following error when I try to import Tensorflow:</p>
<blockquote>
<p>ImportError: cannot import name 'context' from 'tensorflow.python.eager' (unknown location)</p>
</blockquote>
<p>This program was having no issue importing anything just the other day, so I don't know what went wrong, nor how to remedy this issue.</p>
<p>Other pertinent information is that I'm running Python 3.7 and Tensorflow 2.3</p>
<p>Any help with this is appreciated.</p>
","<p>Well I ended up resolving this issue. Turns out it was due to an issue in the cloning process. Created a new backup and restored from this new backup and the error went away.</p>
<p>Thanks for the help everyone.</p>
","244","2","1","<python><tensorflow><raspberry-pi><jupyter-lab><openvino>"
"54921958","Why does the intel openvino R5 precompiled binaries give ""not executable"" on my raspberry pi OS?","2019-02-28 09:09:34","<p>The precompiled OpenVINO R5 distribution supports ""raspbian 9"" and gives some precompiled libraries for interfacing with their ""movidius"" usb stick. I tried it, and for example their precompiled ""myriad_compile"" program runs on raspbian indeed. Now I am doing the same on a custom built OS made with OpenADK. It has the official raspberry pi kernel, and uses glibc 2.27 and I'm using gcc 7.3.0 too. If I run the exact same binary, then I get this message:</p>

<pre><code># ./myriad_compile
mksh: ./myriad_compile: not executable: 32-bit ELF file
</code></pre>

<p>As a test, I tried to run a random binary from raspbian on my OS, and it works ok. I also tried to run a random binary from my OS on raspbian and it also worked ok.</p>

<p>So now I'm a bit puzzled what else could be the cause of this.</p>

<p>I did a readelf of the intel binary:</p>

<pre><code>pi@raspberrypi:~/armv7l $ readelf -A ./myriad_compile
Attribute Section: aeabi
File Attributes
  Tag_CPU_name: ""7-A""
  Tag_CPU_arch: v7
  Tag_CPU_arch_profile: Application
  Tag_ARM_ISA_use: Yes
  Tag_THUMB_ISA_use: Thumb-2
  Tag_FP_arch: VFPv3-D16
  Tag_ABI_PCS_wchar_t: 4
  Tag_ABI_FP_rounding: Needed
  Tag_ABI_FP_denormal: Needed
  Tag_ABI_FP_exceptions: Needed
  Tag_ABI_FP_number_model: IEEE 754
  Tag_ABI_align_needed: 8-byte
  Tag_ABI_align_preserved: 8-byte, except leaf SP
  Tag_ABI_enum_size: int
  Tag_ABI_VFP_args: VFP registers
  Tag_CPU_unaligned_access: v6
</code></pre>

<p>And here is the random executable called ""watchdogctl"" compiled by my toolchain from my own OpenADK OS:</p>

<pre><code>pi@raspberrypi:~/armv7l $ readelf -A ./watchdogctl
Attribute Section: aeabi
File Attributes
  Tag_CPU_name: ""Cortex-A53""
  Tag_CPU_arch: v8
  Tag_CPU_arch_profile: Application
  Tag_ARM_ISA_use: Yes
  Tag_THUMB_ISA_use: Thumb-2
  Tag_FP_arch: FP for ARMv8
  Tag_Advanced_SIMD_arch: NEON for ARMv8
  Tag_ABI_PCS_wchar_t: 4
  Tag_ABI_FP_rounding: Needed
  Tag_ABI_FP_denormal: Needed
  Tag_ABI_FP_exceptions: Needed
  Tag_ABI_FP_number_model: IEEE 754
  Tag_ABI_align_needed: 8-byte
  Tag_ABI_align_preserved: 8-byte, except leaf SP
  Tag_ABI_enum_size: int
  Tag_ABI_VFP_args: VFP registers
  Tag_CPU_unaligned_access: v6
  Tag_MPextension_use: Allowed
  Tag_Virtualization_use: TrustZone and Virtualization Extensions
</code></pre>

<p>And this executable, coming from raspbian 9 runs on my OS without problems, and it's compiled for an older cpu version even:</p>

<pre><code>pi@raspberrypi:~/armv7l $ readelf -A /usr/bin/wpa_passphrase
Attribute Section: aeabi
File Attributes
  Tag_CPU_name: ""6""
  Tag_CPU_arch: v6
  Tag_ARM_ISA_use: Yes
  Tag_THUMB_ISA_use: Thumb-1
  Tag_FP_arch: VFPv2
  Tag_ABI_PCS_wchar_t: 4
  Tag_ABI_FP_rounding: Needed
  Tag_ABI_FP_denormal: Needed
  Tag_ABI_FP_exceptions: Needed
  Tag_ABI_FP_number_model: IEEE 754
  Tag_ABI_align_needed: 8-byte
  Tag_ABI_align_preserved: 8-byte, except leaf SP
  Tag_ABI_enum_size: int
  Tag_ABI_VFP_args: VFP registers
  Tag_CPU_unaligned_access: v6
</code></pre>

<p>So can I get some more info about what is missing in my OS that is present on raspbian 9? I see that they use glibc 2.24 but I have 2.27 so I should be OK I think, and also I have gcc 7 and they use gcc 6, but it's all with the new ABI so I also don't think there is a problem there.</p>

<p>Any suggestions are welcome!
Thank</p>
","<p>Sooooo, after some more searching and hair-pulling, it seems I have to enable the ""thumb"" compile options in my glibc compilation. I found out by compiling a subset of my own os again with that option, and then uploaded such a binary to my old OS, then saw the exact same message appear... so I then booted my thumb OS and it works!</p>
","240","1","1","<raspberry-pi3><raspbian><glibc><elf><openvino>"
"66084226","Why the input type changed while call _call_impl in pytorch","2021-02-07 03:18:05","<p>I'm working at converting pytorch model to openvino IR, the nn of my pytorch is vgg16, and the method is faster rcnn, there is a weird problem when I debug the code.</p>
<p>when I run the following code</p>
<p><code>fc7 = self.classifier(pool, abc=&quot;abc123&quot;)</code></p>
<p>the pool object is OpenVinoTensor, abc=&quot;abc123&quot; is the test parameters for debuging.
And the classifier is defined as following:</p>
<blockquote>
<p>Sequential(
(0): Linear(in_features=25088, out_features=4096, bias=True)
(1): ReLU(inplace=True)
(2): Linear(in_features=4096, out_features=4096, bias=True)
(3): ReLU(inplace=True)</p>
</blockquote>
<p>after self.classifier(pool, abc=&quot;abc123&quot;) is called, it calls torch.nn.Module._call_impl</p>
<p><code>def _call_impl(self, *input, **kwargs)</code></p>
<p>then I found the &quot;input&quot; here is '(ReLU_57,)', which is a tuple, I think it should be OpenVinoTensor, as well as &quot;kwargs&quot; is abc=&quot;abc123&quot;.</p>
<p>What happened after I call classifier?</p>
","<p>What happen when you call the classifier depends on what you had programmed the classifier function to do.</p>
<p>You may refer <a href=""https://towardsdatascience.com/torchvision-transfer-learning-1d4778b807cc"" rel=""nofollow noreferrer"">here</a> for further understanding.</p>
<p>Plus, this is how a <a href=""https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"" rel=""nofollow noreferrer"">classifier in PyTorch</a> works.</p>
","228","0","1","<python><pytorch><openvino>"
"66818039","openvino/ImportError: DLL load failed: Le module spécifié est introuvable","2021-03-26 13:39:08","<p>I'm working with intel's openvino 2020.
I get this error I searched and tried a lot of solutions online, but It didn't work.
and honestly, I'm new to working with python, openvino. So I don't really understand the error.
this is the error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;face_recognition_demo.py&quot;, line 27, in &lt;module&gt;
    from ie_module import InferenceContext
  File &quot;C:\Program Files (x86)\Intel\openvino_2020.3.194\deployment_tools\open_model_zoo\demos\python_demos\face_recognition_demo\ie_module.py&quot;, line 20, in &lt;module&gt;
    from openvino.inference_engine import IECore
  File &quot;C:\Program Files (x86)\Intel\openvino\python\python3.6\openvino\inference_engine\__init__.py&quot;, line 1, in &lt;module&gt;
    from .ie_api import *
ImportError: DLL load failed: Le module spécifié est introuvable.
</code></pre>
<p>my imports:</p>
<pre><code>import logging as log
import os.path as osp
import sys
import time
from argparse import ArgumentParser

import cv2
import numpy as np

from ie_module import InferenceContext
from landmarks_detector import LandmarksDetector
from face_detector import FaceDetector
from faces_database import FacesDatabase
from face_identifier import FaceIdentifier
</code></pre>
<p>I hope you can help me</p>
","<p>Have you verified your installation of OpenVINO by running the Image Classification Verification Script? Refer to the documentation from the link below and please verify your installation first since this error looks like a configuration mistake in the environment.
<a href=""https://docs.openvinotoolkit.org/2020.3/_docs_install_guides_installing_openvino_windows.html#Using-Demo-Scripts"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/2020.3/_docs_install_guides_installing_openvino_windows.html#Using-Demo-Scripts</a></p>
<p>Make sure to complete all steps given on the <a href=""https://docs.openvinotoolkit.org/2020.3/_docs_install_guides_installing_openvino_windows.html#installation_steps"" rel=""nofollow noreferrer"">Installation Guide for Windows</a>. Once verified done, then you can run the Interactive Face Recognition Demo. Here is the workaround to run the demo:</p>
<ol>
<li><p>Setup OpenVINO environment.</p>
<p>cd &lt;INSTALL_DIR&gt;\openvino_2020.3.194\deployment_tools\bin</p>
<p>Run setupvars.bat</p>
</li>
</ol>
<p>2.Navigate to the face recognition demo folder</p>
<p>cd &lt;INSTALL_DIR&gt;\openvino_2020.3.194\deployment_tools\open_model_zoo\demos\python_demos\face_recognition_demo</p>
<p>Run pip install -r requirement.txt</p>
<ol start=""3"">
<li>Run the demo
python./face_recognition_demo.py -m_fd &quot;&lt;INSTALL_DIR&gt;\openvino_2020.3.194\deployment_tools\tools
model_downloader\intel\face-detection-retails-0004\FP16\face-detection-retail-0004.xml&quot; -m_lm &quot;&lt;INSTALL_DIR&gt;\openvino_2020.3.194\deployment_tools\tools
model_downloader\intel\landmarks-regression-retail-0009\FP16\landmarks-regression-retail-0009.xml&quot; -m_reid &quot;&lt;INSTALL_DIR&gt;\openvino_2020.3.194\deployment_tools\tools
model_downloader\intel\face-reidentification-retail-0095\FP16\face-reidentification-retail-0095.xml&quot; -i 0 -fg &quot;&lt;IMAGE_DIR&gt;\Pictures\image_name&quot;</li>
</ol>
<p>Check out the <a href=""https://docs.openvinotoolkit.org/2020.3/_demos_python_demos_face_recognition_demo_README.html"" rel=""nofollow noreferrer"">Interactive Face Recognition Demo</a> documentation for more details.</p>
","227","0","3","<python><dll><import><intel><openvino>"
"66818039","openvino/ImportError: DLL load failed: Le module spécifié est introuvable","2021-03-26 13:39:08","<p>I'm working with intel's openvino 2020.
I get this error I searched and tried a lot of solutions online, but It didn't work.
and honestly, I'm new to working with python, openvino. So I don't really understand the error.
this is the error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;face_recognition_demo.py&quot;, line 27, in &lt;module&gt;
    from ie_module import InferenceContext
  File &quot;C:\Program Files (x86)\Intel\openvino_2020.3.194\deployment_tools\open_model_zoo\demos\python_demos\face_recognition_demo\ie_module.py&quot;, line 20, in &lt;module&gt;
    from openvino.inference_engine import IECore
  File &quot;C:\Program Files (x86)\Intel\openvino\python\python3.6\openvino\inference_engine\__init__.py&quot;, line 1, in &lt;module&gt;
    from .ie_api import *
ImportError: DLL load failed: Le module spécifié est introuvable.
</code></pre>
<p>my imports:</p>
<pre><code>import logging as log
import os.path as osp
import sys
import time
from argparse import ArgumentParser

import cv2
import numpy as np

from ie_module import InferenceContext
from landmarks_detector import LandmarksDetector
from face_detector import FaceDetector
from faces_database import FacesDatabase
from face_identifier import FaceIdentifier
</code></pre>
<p>I hope you can help me</p>
","<p>so yeah I followed these steps</p>
<ol>
<li><p>Setup OpenVINO environment.</p>
<p><code>cd &lt;INSTALL_DIR&gt;\openvino_2020.3.194\deployment_tools\bin</code></p>
<p><code>setupvars.bat</code></p>
</li>
</ol>
<p>result:
<a href=""https://i.stack.imgur.com/H6HeN.png"" rel=""nofollow noreferrer"">runnigsetupvars</a>
so I always get this warning about the libmmd.dll, again I researched for a solution, but I  couldn't find the source of this problem.</p>
<p>2.Navigate to the face recognition demo folder</p>
<pre><code>cd &lt;INSTALL_DIR&gt;\openvino_2020.3.194\deployment_tools\open_model_zoo\demos\python_demos\face_recognition_demo
    
`pip install -r requirements.txt`
</code></pre>
<p><a href=""https://i.stack.imgur.com/B8mg0.png"" rel=""nofollow noreferrer"">results</a>
and finally<a href=""https://i.stack.imgur.com/72MHf.png"" rel=""nofollow noreferrer"">error</a>
now I get this error, although OpenCV-python is downloaded, I tried to follow this solution I found:</p>
<pre><code>pip uninstall opencv-python
opencv-contrib-python
</code></pre>
<p>but the same cv2 error is showing</p>
","227","0","3","<python><dll><import><intel><openvino>"
"66818039","openvino/ImportError: DLL load failed: Le module spécifié est introuvable","2021-03-26 13:39:08","<p>I'm working with intel's openvino 2020.
I get this error I searched and tried a lot of solutions online, but It didn't work.
and honestly, I'm new to working with python, openvino. So I don't really understand the error.
this is the error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;face_recognition_demo.py&quot;, line 27, in &lt;module&gt;
    from ie_module import InferenceContext
  File &quot;C:\Program Files (x86)\Intel\openvino_2020.3.194\deployment_tools\open_model_zoo\demos\python_demos\face_recognition_demo\ie_module.py&quot;, line 20, in &lt;module&gt;
    from openvino.inference_engine import IECore
  File &quot;C:\Program Files (x86)\Intel\openvino\python\python3.6\openvino\inference_engine\__init__.py&quot;, line 1, in &lt;module&gt;
    from .ie_api import *
ImportError: DLL load failed: Le module spécifié est introuvable.
</code></pre>
<p>my imports:</p>
<pre><code>import logging as log
import os.path as osp
import sys
import time
from argparse import ArgumentParser

import cv2
import numpy as np

from ie_module import InferenceContext
from landmarks_detector import LandmarksDetector
from face_detector import FaceDetector
from faces_database import FacesDatabase
from face_identifier import FaceIdentifier
</code></pre>
<p>I hope you can help me</p>
","<p>Could you please reinstall the OpenVINO 2020.3 and run the verification script successfully. Then, follow again steps from the previous reply since I have validated this on my side and everything works fine. Make sure you restart your computer after the installation of OpenVINO and Microsoft Visual Studio.</p>
","227","0","3","<python><dll><import><intel><openvino>"
"58083755","Fully Connected Layer (dot product) using AVX","2019-09-24 15:34:30","<p>I have the following C++ code to perform the multiply and accumulate steps of a fully connected layer (without the bias). Basically I just do a dot product using a vector (inputs) and a matrix (weights). I used AVX vectors to speed up the operation.</p>

<pre><code>const float* src = inputs[0]-&gt;buffer();
const float* scl = weights-&gt;buffer();
float* dst = outputs[0]-&gt;buffer();

SizeVector in_dims = inputs[0]-&gt;getTensorDesc().getDims();
SizeVector out_dims = outputs[0]-&gt;getTensorDesc().getDims();

const int in_neurons = static_cast&lt;int&gt;(in_dims[1]);
const int out_neurons = static_cast&lt;int&gt;(out_dims[1]);    

for(size_t n = 0; n &lt; out_neurons; n++){
    float accum = 0.0;
    float temp[4] = {0,0,0,0};
    float *p = temp;

    __m128 in, ws, dp;

    for(size_t i = 0; i &lt; in_neurons; i+=4){

        // read and save the weights correctly by applying the mask
        temp[0] = scl[(i+0)*out_neurons + n];
        temp[1] = scl[(i+1)*out_neurons + n];
        temp[2] = scl[(i+2)*out_neurons + n];
        temp[3] = scl[(i+3)*out_neurons + n];

        // load input neurons sequentially
        in = _mm_load_ps(&amp;src[i]);

        // load weights
        ws = _mm_load_ps(p);

        // dot product
        dp = _mm_dp_ps(in, ws, 0xff);

        // accumulator
        accum += dp.m128_f32[0]; 
    }
    // save the final result
    dst[n] = accum.m128_f32[0];
}
</code></pre>

<p>It works but the speedup is far from what I expected. As you can see below a convolutional layer with x24 more operations than my custom dot product layer takes less time. This makes no sense and there should be much more room for improvements. What are my major mistakes when trying to use AVX? (I'm new to AVX programming so I don't fully understand from where I should start to look to fully optimize the code).</p>

<pre><code>**Convolutional Convolutional Layer Fully Optimized (AVX)**
Layer: CONV3-32 
Input: 28x28x32 = 25K   
Weights: (3*3*32)*32 = 9K   
Number of MACs: 3*3*27*27*32*32 = 7M    
Execution Time on OpenVINO framework: 0.049 ms

**My Custom Dot Product Layer Far From Optimized (AVX)**
Layer: FC
Inputs: 1x1x512
Outputs: 576    
Weights: 3*3*64*512 = 295K  
Number of MACs: 295K    
Execution Time on OpenVINO framework: 0.197 ms
</code></pre>

<p>Thanks for all help in advance!</p>
","<p>Addendum: What you are doing is actually a Matrix-Vector-product. It is well-understood how to implement this efficiently (although with caching and instruction-level parallelism it is not completely trivial). The rest of the answer just shows a very simple vectorized implementation.</p>

<hr>

<p>You can drastically simplify your implementation by incrementing <code>n+=8</code> and <code>i+=1</code> (assuming <code>out_neurons</code> is a multiple of 8, otherwise, some special handling needs to be done for the last elements), i.e., you can accumulate 8 <code>dst</code> values at once.</p>

<p>A very simple implementation assuming FMA is available (otherwise use multiplication and addition):</p>

<pre><code>void dot_product(const float* src, const float* scl, float* dst,
                 const int in_neurons, const int out_neurons)
{
    for(size_t n = 0; n &lt; out_neurons; n+=8){
        __m256 accum = _mm256_setzero_ps();

        for(size_t i = 0; i &lt; in_neurons; i++){
            accum = _mm256_fmadd_ps(_mm256_loadu_ps(&amp;scl[i*out_neurons+n]), _mm256_set1_ps(src[i]), accum);
        }
        // save the result
        _mm256_storeu_ps(dst+n ,accum);
    }
}
</code></pre>

<p>This could still be optimized e.g., by accumulating 2, 4, or 8 <code>dst</code> packets inside the inner loop, which would not only save some broadcast operations (the <code>_mm256_set1_ps</code> instruction), but also compensate latencies of the FMA instruction.</p>

<p>Godbolt-Link, if you want to play around with the code: <a href=""https://godbolt.org/z/mm-YHi"" rel=""nofollow noreferrer"">https://godbolt.org/z/mm-YHi</a></p>
","217","0","1","<c++><performance><avx><dot-product><openvino>"
"66156107","OpenVino and PyInstaller on Raspberry PI 3B+","2021-02-11 13:54:59","<p>I tried to build a standalone executable with PyInstaller for Python 3.5 using OpenVino 2020.4.287.</p>
<p>PyInstaller assembled a file successfully but I received the next error after launch:</p>
<pre><code>ImportError: No module named 'openvino'
</code></pre>
<p>I tried to include /opt/intel/openvino/deployment_tools/inference_engine/lib/armv7l/plugins.xml in data while building executable but it didn't help.</p>
<p>So the question is how to build a standalone executable in Raspbian with PyInstaller with OpenVino import?</p>
<p>Thanks.</p>
","<p>The biggest possibility is you didn't run the setupvars before that implementation.
Please note that you need to ensure the setupvars had been run and initialized(you should see the init message) in each cmd or terminal before proceeding to any further inferencing/etc.</p>
<p>This setupvars would keep the required packages together. Hence, if you didn't run it, the problem like what you are facing would persist.</p>
","217","1","2","<python><pyinstaller><raspberry-pi3><raspbian><openvino>"
"66156107","OpenVino and PyInstaller on Raspberry PI 3B+","2021-02-11 13:54:59","<p>I tried to build a standalone executable with PyInstaller for Python 3.5 using OpenVino 2020.4.287.</p>
<p>PyInstaller assembled a file successfully but I received the next error after launch:</p>
<pre><code>ImportError: No module named 'openvino'
</code></pre>
<p>I tried to include /opt/intel/openvino/deployment_tools/inference_engine/lib/armv7l/plugins.xml in data while building executable but it didn't help.</p>
<p>So the question is how to build a standalone executable in Raspbian with PyInstaller with OpenVino import?</p>
<p>Thanks.</p>
","<p>Okay, the issue was resolved.</p>
<p><strong>Initial conditions:</strong></p>
<p>Raspbian Stretch with Python 3.5.</p>
<p><strong>Considerations:</strong></p>
<p>The last OpenVino supporting Python 3.5 is 2020.4.</p>
<p>So, let’s start. We will create a test.py file with the next lines:</p>
<pre><code>import numpy as np
import openvino.inference_engine.constants
from openvino.inference_engine import IENetwork, IECore
print(&quot;start&quot;)
e = IECore()
print(&quot;end&quot;)
</code></pre>
<p>Now let's install OpenVino, Pyinstaller, create an executable, and run it:</p>
<pre><code>sudo pip3 install pyinstaller
sudo mkdir -p /opt/intel/openvino
wget https://storage.openvinotoolkit.org/repositories/openvino/packages/2020.4/l_openvino_toolkit_runtime_raspbian_p_2020.4.287.tgz
sudo tar -xf l_openvino_toolkit_runtime_raspbian_p_2020.4.287.tgz --strip 1 -C /opt/intel/openvino
rm l_openvino_toolkit_runtime_raspbian_p_2020.4.287.tgz
sudo apt install cmake
source /opt/intel/openvino/bin/setupvars.sh
sh /opt/intel/openvino/install_dependencies/install_NCS_udev_rules.sh
pyinstaller --onefile --clean --add-data=&quot;/opt/intel/openvino/deployment_tools/inference_engine/lib/armv7l/plugins.xml:.&quot; test.py
dist/test
</code></pre>
<p>&quot;start&quot; and &quot;end&quot; messages should be shown.</p>
<p>The most important when you are using OpenVino 2020.4 is the next line:</p>
<pre><code>import openvino.inference_engine.constants
</code></pre>
<p>In previous versions of OpenVino everything will work normally without this line. But in my specific case I’ve got the next error on OpenVino versions earlier 2020.4:
openvino.inference_engine.ie_api.IECore' object has no attribute 'read_network
That’s why I specified import for <strong>openvino.inference_engine.constants</strong>. Without this nothing works in OpenVino 2020.4.</p>
<p>Thanks.</p>
","217","1","2","<python><pyinstaller><raspberry-pi3><raspbian><openvino>"
"55166337","Openvino: Problem when trying to load CPU plugin in Qt","2019-03-14 15:27:36","<p>After installing and successfully run the OpenVino demos in my PC I start implementing a basic application in Qt with this library. I made the linking as Intel's <a href=""https://software.intel.com/en-us/articles/OpenVINO-InferEngine"" rel=""nofollow noreferrer"">documentation</a> describes and the application successfully compiled.</p>
<pre><code>auto plugin = PluginDispatcher({L&quot;&quot;}).getPluginByDevice(&quot;CPU&quot;);

auto netBuilder = new CNNNetReader();
netBuilder-&gt;ReadNetwork(&quot;../TestModel/squeezenet1.1.xml&quot;);
netBuilder-&gt;ReadWeights(&quot;../TestModel/squeezenet1.1.bin&quot;);

auto network = netBuilder-&gt;getNetwork();
netBuilder-&gt;getNetwork().setBatchSize(1);
</code></pre>
<p>The application pops an exception when debugger reach getPluginByDevice call (getSuitablePlugin method from ie_plugin_dispacher.hpp (line 73)).</p>
<p><a href=""https://i.stack.imgur.com/smSGA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/smSGA.png"" alt=""enter image description here"" /></a></p>
<p>I am using MSVC2017 64bit with Qt 5.11.1 in a Windows 10 machine. The .pro file library call is:</p>
<pre><code>#OpenVino
INCLUDEPATH += $$PWD/inference_engine/include
LIBS += -L$$PWD/inference_engine/lib/intel64/Release
LIBS += -linference_engine -llibiomp5md
</code></pre>
<p>Is anyone experienced the same or has an idea what's going on?</p>
<p>Thanks in advance,</p>
<p>Thanasis</p>
","<p>The release libraries were causing the problem. When I switched to the debug ones (inference_engined.lib insted of inference_engine.lib) the application run successfully.</p>

<p><strong>EDIT</strong></p>

<p>I paste the code from .pro file in case someone face the same problem.</p>

<pre><code>#OpenVino
INCLUDEPATH += $$PWD/inference_engine/include

CONFIG(release, debug|release):BuildVar=release
CONFIG(debug, debug|release):BuildVar=debug

equals(BuildVar,debug) {
    message(Debug Mode)
    LIBS += -L$$PWD/inference_engine/lib/intel64/Debug
    LIBS += -linference_engined
}

equals(BuildVar,release) {
    message(Release Mode)
    LIBS += -L$$PWD/inference_engine/lib/intel64/Release
    LIBS += -linference_engine
}
</code></pre>
","213","0","1","<c++><qt5><intel><openvino>"
"54423407","OpenVino for Intel HD Graphic","2019-01-29 14:37:36","<p>I have processor Intel® Core™ i7-7700 Processor. I am trying to run <a href=""https://software.intel.com/en-us/openvino-toolkit"" rel=""nofollow noreferrer"">OpenVino</a> on Intel GPU.</p>

<p>According to specs <a href=""https://ark.intel.com/products/series/95544/7th-Generation-Intel-Core-i7-Processors"" rel=""nofollow noreferrer"">here</a>, the CPU has Intel® HD Graphics 630 built in.</p>

<p>So that means, I have Intel GPU inside and just need to have right driver for GPU according to discussion <a href=""https://software.intel.com/en-us/articles/OpenVINO-Install-Windows#Install%20GPU"" rel=""nofollow noreferrer"">here</a>.</p>

<p>I just want to make sure I don't need external GPU like Nvidia if I want to use Intel HD Graphics.</p>
","<p>That's right, GPU support in OpenVino means Intel GEN or Intel HD Graphics. You don't need external GPU to try OV.</p>
","210","0","1","<intel><openvino>"
"62712870","openvino on macOS build issue","2020-07-03 09:51:40","<p>I followed the instruction on thier github <a href=""https://github.com/openvinotoolkit/openvino"" rel=""nofollow noreferrer"">https://github.com/openvinotoolkit/openvino</a> to build it on macOS, done it and when i wrote a small code to check was all clear or not</p>
<pre><code>#include &lt;string&gt;
#include &lt;iostream&gt;
#include &lt;dlfcn.h&gt;
#include &lt;fstream&gt;

#define USE_STATIC_IE

#define _CRT_SECURE_NO_WARNINGS

#include &lt;ie_core.hpp&gt;

using namespace :: std;

int main(){
    dlopen(&quot;/Users/nk/Documents/HZ/openvino/inference-engine/temp/tbb/lib/libtbb.dylib&quot;, RTLD_LAZY);
    dlopen(&quot;/Users/nk/Documents/HZ/openvino/bin/intel64/Release/lib/libngraph.dylib&quot;, RTLD_LAZY);
    dlopen(&quot;/Users/nk/Documents/HZ/openvino/bin/intel64/Release/lib/libinference_engine_transformations.dylib&quot;, RTLD_LAZY);
    dlopen(&quot;/Users/nk/Documents/HZ/openvino/bin/intel64/Release/lib/libinference_engine_legacy.dylib&quot;, RTLD_LAZY);
    dlopen(&quot;/Users/nk/Documents/HZ/openvino/bin/intel64/Release/lib/libinference_engine.dylib&quot;, RTLD_LAZY);
    dlopen(&quot;/Users/nk/Documents/HZ/openvino/bin/intel64/Release/lib/libinference_engine_lp_transformations.dylib&quot;, RTLD_LAZY);
    dlopen(&quot;/Users/nk/Documents/HZ/openvino/bin/intel64/Release/lib/libMKLDNNPlugin.dylib&quot;, RTLD_LAZY);

    const std::string pluginsFilePath = &quot;/Users/nk/Documents/HZ/openvino/bin/intel64/Release/lib/plugins.xml&quot;;
    InferenceEngine::Core ie(pluginsFilePath);
    const auto info = ie.GetVersions(&quot;CPU&quot;);

    if(!info.empty())
        cout &lt;&lt; &quot;Yes&quot;;

    return 0;
}
</code></pre>
<p>when i compile it with command</p>
<pre><code>g++ -std=c++11 -I/Users/nk/Documents/HZ/openvino/inference-engine/include testdll.cpp -o testdll &amp;&amp; &quot;/Users/nk/Documents/FelenaSoft/&quot;testdll
</code></pre>
<p>it gave me the error</p>
<pre><code>Undefined symbols for architecture x86_64:
  &quot;InferenceEngine::Core::Core(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&quot;, referenced from:
      _main in testdll-788073.o
  &quot;InferenceEngine::Core::GetVersions(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) const&quot;, referenced from:
      _main in testdll-788073.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
</code></pre>
<p>then i added .a libs and ran this command</p>
<pre><code>g++ -std=c++11 -L/Users/nk/Documents/HZ/openvino/bin/intel64/Release/lib -lmkldnn -lpugixml -linference_engine_s -I/Users/nk/Documents/HZ/openvino/inference-engine/include testdll.cpp -o testdll &amp;&amp; &quot;/Users/nk/Documents/FelenaSoft/&quot;testdll 
</code></pre>
<p>and i got this</p>
<pre><code>Undefined symbols for architecture x86_64:
  &quot;ngraph::as_output_vector(std::__1::vector&lt;std::__1::shared_ptr&lt;ngraph::Node&gt;, std::__1::allocator&lt;std::__1::shared_ptr&lt;ngraph::Node&gt; &gt; &gt; const&amp;)&quot;, referenced from:
      ngraph::Node::generate_adjoints(ngraph::autodiff::Adjoints&amp;, std::__1::vector&lt;std::__1::shared_ptr&lt;ngraph::Node&gt;, std::__1::allocator&lt;std::__1::shared_ptr&lt;ngraph::Node&gt; &gt; &gt; const&amp;) in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::match_node(ngraph::pattern::Matcher*, ngraph::Output&lt;ngraph::Node&gt; const&amp;)&quot;, referenced from:
      vtable for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::match_value(ngraph::pattern::Matcher*, ngraph::Output&lt;ngraph::Node&gt; const&amp;, ngraph::Output&lt;ngraph::Node&gt; const&amp;)&quot;, referenced from:
      vtable for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::constant_fold(std::__1::vector&lt;ngraph::Output&lt;ngraph::Node&gt;, std::__1::allocator&lt;ngraph::Output&lt;ngraph::Node&gt; &gt; &gt;&amp;, std::__1::vector&lt;ngraph::Output&lt;ngraph::Node&gt;, std::__1::allocator&lt;ngraph::Output&lt;ngraph::Node&gt; &gt; &gt; const&amp;)&quot;, referenced from:
      vtable for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::set_arguments(std::__1::vector&lt;ngraph::Output&lt;ngraph::Node&gt;, std::__1::allocator&lt;ngraph::Output&lt;ngraph::Node&gt; &gt; &gt; const&amp;)&quot;, referenced from:
      ExecGraphInfoSerialization::ExecutionNode::clone_with_new_inputs(std::__1::vector&lt;ngraph::Output&lt;ngraph::Node&gt;, std::__1::allocator&lt;ngraph::Output&lt;ngraph::Node&gt; &gt; &gt; const&amp;) const in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::set_output_type(unsigned long, ngraph::element::Type const&amp;, ngraph::PartialShape const&amp;)&quot;, referenced from:
      ExecGraphInfoSerialization::ExecutionNode::clone_with_new_inputs(std::__1::vector&lt;ngraph::Output&lt;ngraph::Node&gt;, std::__1::allocator&lt;ngraph::Output&lt;ngraph::Node&gt; &gt; &gt; const&amp;) const in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::m_next_instance_id&quot;, referenced from:
      std::__1::shared_ptr&lt;ExecGraphInfoSerialization::ExecutionNode&gt; std::__1::shared_ptr&lt;ExecGraphInfoSerialization::ExecutionNode&gt;::make_shared&lt;&gt;() in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::validate_and_infer_types()&quot;, referenced from:
      vtable for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::evaluate(std::__1::vector&lt;std::__1::shared_ptr&lt;ngraph::runtime::HostTensor&gt;, std::__1::allocator&lt;std::__1::shared_ptr&lt;ngraph::runtime::HostTensor&gt; &gt; &gt; const&amp;, std::__1::vector&lt;std::__1::shared_ptr&lt;ngraph::runtime::HostTensor&gt;, std::__1::allocator&lt;std::__1::shared_ptr&lt;ngraph::runtime::HostTensor&gt; &gt; &gt; const&amp;)&quot;, referenced from:
      vtable for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::~Node()&quot;, referenced from:
      ExecGraphInfoSerialization::ExecutionNode::~ExecutionNode() in libinference_engine_s.a(ie_rtti.cpp.o)
      ExecGraphInfoSerialization::ExecutionNode::~ExecutionNode() in libinference_engine_s.a(ie_rtti.cpp.o)
      std::__1::__shared_ptr_emplace&lt;ExecGraphInfoSerialization::ExecutionNode, std::__1::allocator&lt;ExecGraphInfoSerialization::ExecutionNode&gt; &gt;::~__shared_ptr_emplace() in libinference_engine_s.a(ie_rtti.cpp.o)
      std::__1::__shared_ptr_emplace&lt;ExecGraphInfoSerialization::ExecutionNode, std::__1::allocator&lt;ExecGraphInfoSerialization::ExecutionNode&gt; &gt;::~__shared_ptr_emplace() in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::is_dynamic() const&quot;, referenced from:
      vtable for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::description() const&quot;, referenced from:
      vtable for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::is_constant() const&quot;, referenced from:
      vtable for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::get_arguments() const&quot;, referenced from:
      vtable for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::get_output_size() const&quot;, referenced from:
      ExecGraphInfoSerialization::ExecutionNode::clone_with_new_inputs(std::__1::vector&lt;ngraph::Output&lt;ngraph::Node&gt;, std::__1::allocator&lt;ngraph::Output&lt;ngraph::Node&gt; &gt; &gt; const&amp;) const in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::write_description(std::__1::basic_ostream&lt;char, std::__1::char_traits&lt;char&gt; &gt;&amp;, unsigned int) const&quot;, referenced from:
      vtable for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::copy_with_new_args(std::__1::vector&lt;std::__1::shared_ptr&lt;ngraph::Node&gt;, std::__1::allocator&lt;std::__1::shared_ptr&lt;ngraph::Node&gt; &gt; &gt; const&amp;) const&quot;, referenced from:
      vtable for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::get_output_element_type(unsigned long) const&quot;, referenced from:
      ExecGraphInfoSerialization::ExecutionNode::clone_with_new_inputs(std::__1::vector&lt;ngraph::Output&lt;ngraph::Node&gt;, std::__1::allocator&lt;ngraph::Output&lt;ngraph::Node&gt; &gt; &gt; const&amp;) const in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::get_default_output_index() const&quot;, referenced from:
      vtable for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::get_output_partial_shape(unsigned long) const&quot;, referenced from:
      ExecGraphInfoSerialization::ExecutionNode::clone_with_new_inputs(std::__1::vector&lt;ngraph::Output&lt;ngraph::Node&gt;, std::__1::allocator&lt;ngraph::Output&lt;ngraph::Node&gt; &gt; &gt; const&amp;) const in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::get_autob() const&quot;, referenced from:
      vtable for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;ngraph::Node::is_output() const&quot;, referenced from:
      vtable for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;typeinfo for ngraph::Node&quot;, referenced from:
      typeinfo for ExecGraphInfoSerialization::ExecutionNode in libinference_engine_s.a(ie_rtti.cpp.o)
  &quot;vtable for ngraph::Node&quot;, referenced from:
      std::__1::shared_ptr&lt;ExecGraphInfoSerialization::ExecutionNode&gt; std::__1::shared_ptr&lt;ExecGraphInfoSerialization::ExecutionNode&gt;::make_shared&lt;&gt;() in libinference_engine_s.a(ie_rtti.cpp.o)
  NOTE: a missing vtable usually means the first non-inline virtual member function has no definition.
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
</code></pre>
<p>Did anyone had the same problem as me, and did anyone know how to fix it?</p>
","<p>First and foremost just need to cross check with you on your toolkit installation. Since you are trying to run a model already, I assume you already set it up properly as in (fyi latest version is 2020.4): <a href=""https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_macos.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_macos.html</a></p>
<p>The error that you are getting, the Undefined symbols for architecture x86_64 is an Xcode build failure indicating you missed out something in your inference engine or the way you link the model with certain resources are incorrect or some other reasons. As long as this error persist you won't be able to compile your code. That is why you got the linker command failed with exit code 1.</p>
<p>Most error occurs because the users did not Set the OpenVINO environment variables or Configure the Model Optimizer correctly. This is why it's really important to Run verification scripts to verify installation and compile samples.</p>
<p>Before running any <strong>custom model</strong>, it is good to have a solid working Openvino environment, hence  please help to check whether you could run the default models as in example here <a href=""https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_macos.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_macos.html</a></p>
<p>By default, <strong>you should already have pre-trained models</strong> including open zoo model, Model Optimizer and Inference Engine which are <strong>working example</strong> to work with <strong>if you configured the toolkit correctly</strong>. These should already located in your installed toolkit's directory which you don't have to install separately from other resources.</p>
<p>Please take a note that you need:
<em><em>CMake 3.4 or higher,
Python 3.5 or higher,
Apple Xcode</em> Command Line Tools</em>*,
(Optional) Apple Xcode* IDE (not required for OpenVINO, but useful for development),</p>
<p>And supported MAC OS is <em><em>macOS</em> 10.14.4</em>*</p>
<p>Again, please help to cross check what you have in your Openvino environment and steps you had used to configure with the official documentation <a href=""https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_macos.html"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_macos.html</a></p>
<p>Thanks!</p>
","210","0","1","<c++><macos><g++><openvino><inference-engine>"
"57075492","cv::dnn::Layer::forward does not work on specific layer (python)","2019-07-17 12:09:33","<p>I'm using the openvino toolkit in python for head position estimation.
I load the network as follows:</p>

<pre><code>weights_headpose = 'head-pose-estimation-adas-0001-2018-FP32.bin'
config_headpose = 'head-pose-estimation-adas-0001-2018-FP32.xml'
model_headpose = cv.dnn.readNet(weights_headpose, config_headpose)
</code></pre>

<p>The following</p>

<pre><code>print(model_headpose.getLayerNames())
</code></pre>

<p>gives:</p>

<pre><code>['angle_p_fc', 'angle_r_fc', 'angle_y_fc']
</code></pre>

<p>When I run:</p>

<pre><code>&gt;print(model_headpose.forward('angle_y_fc'))
</code></pre>

<p>I get a float, as expected; <br>
BUT when i run</p>

<pre><code>print(model_headpose.forward('angle_p_fc'))
</code></pre>

<p>or</p>

<pre><code>print(model_headpose.forward('angle_r_fc'))
</code></pre>

<p>I get the following error:</p>

<pre><code>cv2.error: OpenCV(4.1.0-openvino) C:\jenkins\workspace\OpenCV\OpenVINO\build\opencv\modules\dnn\src\op_inf_engine.cpp:688: error: (-215:Assertion failed) !isInitialized() in function 'cv::dnn::InfEngineBackendNet::initPlugin'
</code></pre>

<p>Are these layers not initialized? Can someone please help me? Thanks in advance!</p>
","<p>My question was solved by using <code>model_headpose.forward(['angle_p_fc', 'angle_r_fc', 'angle_y_fc'])</code></p>
","206","1","1","<opencv><neural-network><openvino>"
"65594023","OpenVINO tensorflow model optimizer error","2021-01-06 10:26:33","<p>I'm getting the following error when i try to convert my trained tensorflow model to IR</p>
<blockquote>
<p>Model Optimizer arguments:
Common parameters:
- Path to the Input Model:  None
- Path for generated IR:    /home/ec2-user/Notebooks/.
- IR output name:   saved_model
- Log level:    ERROR
- Batch:    Not specified, inherited from the model
- Input layers:     Not specified, inherited from the model
- Output layers:    Not specified, inherited from the model
- Input shapes:     [1,512,512,3]
- Mean values:  Not specified
- Scale values:     Not specified
- Scale factor:     Not specified
- Precision of IR:  FP32
- Enable fusing:    True
- Enable grouped convolutions fusing:   True
- Move mean values to preprocess section:   None
- Reverse input channels:   False
TensorFlow specific parameters:
- Input model in text protobuf format:  False
- Path to model dump for TensorBoard:   None
- List of shared libraries with TensorFlow custom layers implementation:    None
- Update the configuration file with input/output node names:   None
- Use configuration file used to generate the model with Object Detection API:  None
- Use the config file:  None
Model Optimizer version:    2021.1.0-1237-bece22ac675-releases/2021/1
2021-01-06 09:55:53.886652: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/openvino_2021/data_processing/dl_streamer/lib:/opt/intel/openvino_2021/data_processing/gstreamer/lib:/opt/intel/openvino_2021/opencv/lib:/opt/intel/openvino_2021/deployment_tools/ngraph/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/hddl_unite/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/lib/intel64
2021-01-06 09:55:53.886685: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-01-06 09:55:57.571164: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/openvino_2021/data_processing/dl_streamer/lib:/opt/intel/openvino_2021/data_processing/gstreamer/lib:/opt/intel/openvino_2021/opencv/lib:/opt/intel/openvino_2021/deployment_tools/ngraph/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/hddl_unite/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/lib/intel64
2021-01-06 09:55:57.571198: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2021-01-06 09:55:57.571216: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-65-233.ec2.internal): /proc/driver/nvidia/version does not exist
2021-01-06 09:55:57.571389: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-01-06 09:55:57.607790: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2999980000 Hz
2021-01-06 09:55:57.608150: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3d4d800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-01-06 09:55:57.608169: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-01-06 09:56:15.612182: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0
2021-01-06 09:56:15.613446: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2021-01-06 09:56:16.010348: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2021-01-06 09:56:16.010388: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 5850 nodes (5149), 13416 edges (12708), time = 182.03ms.
2021-01-06 09:56:16.010397: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 5850 nodes (0), 13416 edges (0), time = 88.009ms.
2021-01-06 09:56:16.010404: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: __inference_Preprocessor_ResizeToRange_cond_false_12695_58123
2021-01-06 09:56:16.010409: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2021-01-06 09:56:16.010416: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2021-01-06 09:56:16.010422: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: __inference_Preprocessor_ResizeToRange_cond_true_12694_56958
2021-01-06 09:56:16.010428: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2021-01-06 09:56:16.010433: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
[ ERROR ]  Cannot infer shapes or values for node &quot;StatefulPartitionedCall/Preprocessor/ResizeToRange/cond&quot;.
[ ERROR ]  Function __inference_Preprocessor_ResizeToRange_cond_true_12694_56958 is not defined.
[ ERROR ]<br />
[ ERROR ]  It can happen due to bug in custom shape infer function &lt;function tf_native_tf_node_infer at 0x7f1fbc2c8050&gt;.
[ ERROR ]  Or because the node inputs have incorrect values/shapes.
[ ERROR ]  Or because input shapes are incorrect (embedded to the model or passed via --input_shape).
[ ERROR ]  Run Model Optimizer with --log_level=DEBUG for more information.
[ ERROR ]  Exception occurred during running replacer &quot;REPLACEMENT_ID&quot; (&lt;class 'extensions.middle.PartialInfer.PartialInfer'&gt;): Stopped shape/value propagation at &quot;StatefulPartitionedCall/Preprocessor/ResizeToRange/cond&quot; node.
For more information please refer to Model Optimizer FAQ, question #38. (<a href=""https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html?question=38#question-38"" rel=""nofollow noreferrer"">https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html?question=38#question-38</a>)</p>
</blockquote>
<p>Debug log file: [Debug_Log][1]</p>
<p>Model Folder: [Model][2]
[1]: https://drive.google.com/file/d/1VFlAW7C0RhmL-T1HrKwBMrvQzJNvCql7/view?usp=sharing
[2]: https://drive.google.com/drive/folders/1kkbp9fAXXsiDeq583Z0tV95zIfbI9_-N?usp=sharing</p>
","<p>First and foremost, please take note that <strong>only these OS are supported by OpenVINO</strong>:</p>
<p>Ubuntu 18.04.x long-term support (LTS), 64-bit</p>
<p>CentOS 7.6, 64-bit (for target only)</p>
<p>Yocto Project v3.0, 64-bit (for target only and requires    modifications)</p>
<p>Windows 10</p>
<p>Raspbian* Buster, 32-bit</p>
<p>Raspbian* Stretch, 32-bit</p>
<p>MacOS</p>
<p>Based on your OS, make sure that you had setup OpenVINO correctly and you are able to run the sample application as in <a href=""https://docs.openvinotoolkit.org/latest/installation_guides.html"" rel=""nofollow noreferrer"">here</a></p>
<p>Once you got that ready,
Cross-check your Tensorflow topology and see whether it's listed <a href=""https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html"" rel=""nofollow noreferrer"">here</a></p>
<p>Models and topologies listed there are supported by OpenVINO.</p>
<p>You may proceed to the next step according to the guide above once you are sure of yours.</p>
<p>NOTE: Please ensure that you had run the setupvars and able to see the initialization message each time you open a new terminal.</p>
","204","0","1","<tensorflow><openvino>"